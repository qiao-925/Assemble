#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ€æœ¯ä¿¡æ¯å·¥ä½œæµå®ˆæŠ¤è¿›ç¨‹
å¯ä»¥è‡ªä¸»è¿è¡Œï¼Œæ”¯æŒå®šæ—¶ä»»åŠ¡å’Œå¤–éƒ¨è§¦å‘
"""

import os
import sys
import time
import json
import schedule
import logging
import threading
import signal
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from tech_workflow import TechWorkflow

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/workflow_daemon.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TechWorkflowDaemon:
    """æŠ€æœ¯ä¿¡æ¯å·¥ä½œæµå®ˆæŠ¤è¿›ç¨‹"""
    
    def __init__(self, config_path: str = "config/workflow_config.yaml"):
        self.workflow = TechWorkflow(config_path)
        self.running = True
        self.config_path = config_path
        
        # è§¦å‘æ§åˆ¶æ–‡ä»¶è·¯å¾„
        self.trigger_dir = Path("triggers")
        self.trigger_dir.mkdir(exist_ok=True)
        
        # çŠ¶æ€æ–‡ä»¶
        self.status_file = "daemon_status.json"
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        Path("logs").mkdir(exist_ok=True)
        
        # æ³¨å†Œä¿¡å·å¤„ç†
        signal.signal(signal.SIGTERM, self._signal_handler)
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """å¤„ç†ç³»ç»Ÿä¿¡å·"""
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡ä¼˜é›…é€€å‡º...")
        self.running = False
    
    def start_daemon(self):
        """å¯åŠ¨å®ˆæŠ¤è¿›ç¨‹"""
        logger.info("ğŸš€ æŠ€æœ¯ä¿¡æ¯å·¥ä½œæµå®ˆæŠ¤è¿›ç¨‹å¯åŠ¨")
        
        # æ›´æ–°çŠ¶æ€
        self._update_status("running", "å®ˆæŠ¤è¿›ç¨‹å·²å¯åŠ¨")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        self._setup_scheduled_tasks()
        
        # å¯åŠ¨è§¦å‘æ–‡ä»¶ç›‘å¬çº¿ç¨‹
        trigger_thread = threading.Thread(target=self._monitor_triggers, daemon=True)
        trigger_thread.start()
        
        # ä¸»å¾ªç¯
        try:
            while self.running:
                # æ‰§è¡Œå®šæ—¶ä»»åŠ¡
                schedule.run_pending()
                
                # çŸ­æš‚ä¼‘çœ ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(10)
                
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œå‡†å¤‡é€€å‡º...")
        except Exception as e:
            logger.error(f"å®ˆæŠ¤è¿›ç¨‹å‡ºé”™: {e}")
        finally:
            self._cleanup()
    
    def _setup_scheduled_tasks(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        logger.info("ğŸ“… è®¾ç½®å®šæ—¶ä»»åŠ¡")
        
        # æ¯æ—¥æŠ€æœ¯æ‘˜è¦ - æ¯å¤©8:00æ‰§è¡Œ
        schedule.every().day.at("08:00").do(self._run_daily_digest)
        
        # å‘¨åº¦æ·±åº¦åˆ†æ - æ¯å‘¨ä¸€9:00æ‰§è¡Œ
        schedule.every().monday.at("09:00").do(self._run_weekly_analysis)
        
        # æ•°æ®æ¸…ç† - æ¯å¤©å‡Œæ™¨2:00æ‰§è¡Œ
        schedule.every().day.at("02:00").do(self._cleanup_old_data)
        
        logger.info("âœ… å®šæ—¶ä»»åŠ¡è®¾ç½®å®Œæˆ")
    
    def _run_daily_digest(self):
        """æ‰§è¡Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡"""
        try:
            logger.info("ğŸŒ… å¼€å§‹æ¯æ—¥æŠ€æœ¯æ‘˜è¦ç”Ÿæˆ")
            output_file = self.workflow.run_daily_digest()
            
            # æ›´æ–°çŠ¶æ€
            self._update_status("daily_completed", f"æ¯æ—¥æ‘˜è¦å·²ç”Ÿæˆ: {output_file}")
            
            # åˆ›å»ºæˆåŠŸæ ‡è®°æ–‡ä»¶ï¼Œä¾›å¤–éƒ¨æ£€æŸ¥
            self._create_completion_marker("daily_digest", output_file)
            
            logger.info(f"âœ… æ¯æ—¥æ‘˜è¦å®Œæˆ: {output_file}")
            
        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            self._update_status("error", f"æ¯æ—¥æ‘˜è¦å¤±è´¥: {e}")
    
    def _run_weekly_analysis(self):
        """æ‰§è¡Œå‘¨åº¦æ·±åº¦åˆ†æ"""
        try:
            logger.info("ğŸ“Š å¼€å§‹å‘¨åº¦æŠ€æœ¯è¶‹åŠ¿åˆ†æ")
            
            # ç”Ÿæˆå¤šä¸ªä¸»é¢˜çš„è¶‹åŠ¿æŠ¥å‘Š
            topics = ["ai", "cloud-native", "database", "performance"]
            output_files = []
            
            for topic in topics:
                output_file = self.workflow.run_trending_analysis(topic)
                if output_file:
                    output_files.append(output_file)
            
            # æ›´æ–°çŠ¶æ€
            self._update_status("weekly_completed", f"å‘¨åº¦åˆ†æå·²å®Œæˆ: {len(output_files)} ä¸ªæŠ¥å‘Š")
            
            logger.info(f"âœ… å‘¨åº¦åˆ†æå®Œæˆ: {len(output_files)} ä¸ªæŠ¥å‘Š")
            
        except Exception as e:
            logger.error(f"âŒ å‘¨åº¦åˆ†æå¤±è´¥: {e}")
            self._update_status("error", f"å‘¨åº¦åˆ†æå¤±è´¥: {e}")
    
    def _monitor_triggers(self):
        """ç›‘å¬è§¦å‘æ–‡ä»¶"""
        logger.info("ğŸ‘ï¸ å¼€å§‹ç›‘å¬è§¦å‘æ–‡ä»¶")
        
        while self.running:
            try:
                # æ£€æŸ¥è§¦å‘æ–‡ä»¶
                trigger_files = list(self.trigger_dir.glob("*.trigger"))
                
                for trigger_file in trigger_files:
                    self._process_trigger_file(trigger_file)
                
                time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"ç›‘å¬è§¦å‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                time.sleep(10)
    
    def _process_trigger_file(self, trigger_file: Path):
        """å¤„ç†è§¦å‘æ–‡ä»¶"""
        try:
            # è¯»å–è§¦å‘æŒ‡ä»¤
            with open(trigger_file, 'r', encoding='utf-8') as f:
                trigger_data = json.load(f)
            
            command = trigger_data.get('command')
            params = trigger_data.get('params', {})
            
            logger.info(f"ğŸ”” æ”¶åˆ°è§¦å‘æŒ‡ä»¤: {command}")
            
            # æ‰§è¡Œç›¸åº”å‘½ä»¤
            output_file = None
            if command == 'daily_digest':
                output_file = self.workflow.run_daily_digest()
                
            elif command == 'analyze_project':
                repo = params.get('repo')
                if repo:
                    output_file = self.workflow.analyze_project(f"https://github.com/{repo}")
                    
            elif command == 'trend_analysis':
                topic = params.get('topic')
                output_file = self.workflow.run_trending_analysis(topic)
                
                         elif command == 'compare_projects':
                 projects = params.get('projects', [])
                 if len(projects) >= 2:
                     project_urls = [f"https://github.com/{p}" for p in projects]
                     output_file = self.workflow.compare_projects(project_urls)
             
             elif command == 'stop_daemon':
                 logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå‡†å¤‡é€€å‡º")
                 self.running = False
                 trigger_file.unlink()
                 return
             
             # åˆ›å»ºå®Œæˆæ ‡è®°
             if output_file:
                 self._create_completion_marker(command, output_file)
                 logger.info(f"âœ… è§¦å‘ä»»åŠ¡å®Œæˆ: {output_file}")
             
             # åˆ é™¤è§¦å‘æ–‡ä»¶
             trigger_file.unlink()
            
        except Exception as e:
            logger.error(f"å¤„ç†è§¦å‘æ–‡ä»¶å¤±è´¥: {e}")
            # ç§»åŠ¨åˆ°é”™è¯¯ç›®å½•
            error_dir = self.trigger_dir / "errors"
            error_dir.mkdir(exist_ok=True)
            trigger_file.rename(error_dir / trigger_file.name)
    
    def _create_completion_marker(self, task_type: str, output_file: str):
        """åˆ›å»ºä»»åŠ¡å®Œæˆæ ‡è®°"""
        marker_dir = Path("completed")
        marker_dir.mkdir(exist_ok=True)
        
        marker_data = {
            'task_type': task_type,
            'output_file': output_file,
            'completed_at': datetime.now().isoformat(),
            'status': 'success'
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        marker_file = marker_dir / f"{task_type}_{timestamp}.completed"
        
        with open(marker_file, 'w', encoding='utf-8') as f:
            json.dump(marker_data, f, ensure_ascii=False, indent=2)
    
    def _update_status(self, status: str, message: str):
        """æ›´æ–°å®ˆæŠ¤è¿›ç¨‹çŠ¶æ€"""
        status_data = {
            'status': status,
            'message': message,
            'timestamp': datetime.now().isoformat(),
            'uptime_seconds': (datetime.now() - self.start_time).total_seconds() if hasattr(self, 'start_time') else 0
        }
        
        with open(self.status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)
    
    def _cleanup_old_data(self):
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æ—§æ•°æ®")
            
            # æ¸…ç†30å¤©å‰çš„åŸå§‹æ•°æ®
            data_dir = Path("data")
            if data_dir.exists():
                cutoff_date = datetime.now() - timedelta(days=30)
                for file_path in data_dir.glob("tech_info_*.json"):
                    if file_path.stat().st_mtime < cutoff_date.timestamp():
                        file_path.unlink()
                        logger.info(f"åˆ é™¤æ—§æ•°æ®æ–‡ä»¶: {file_path}")
            
            # æ¸…ç†7å¤©å‰çš„å®Œæˆæ ‡è®°
            completed_dir = Path("completed")
            if completed_dir.exists():
                cutoff_date = datetime.now() - timedelta(days=7)
                for file_path in completed_dir.glob("*.completed"):
                    if file_path.stat().st_mtime < cutoff_date.timestamp():
                        file_path.unlink()
            
            logger.info("âœ… æ•°æ®æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
    
    def _cleanup(self):
        """å®ˆæŠ¤è¿›ç¨‹æ¸…ç†"""
        logger.info("ğŸ§¹ å®ˆæŠ¤è¿›ç¨‹æ¸…ç†ä¸­...")
        self._update_status("stopped", "å®ˆæŠ¤è¿›ç¨‹å·²åœæ­¢")

def create_systemd_service():
    """åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶"""
    service_content = f"""[Unit]
Description=Tech Info Workflow Daemon
After=network.target

[Service]
Type=simple
User={os.getenv('USER', 'root')}
WorkingDirectory={os.getcwd()}
Environment=PATH={os.environ.get('PATH')}
ExecStart={sys.executable} {os.path.abspath(__file__)} --daemon
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
"""
    
    service_file = f"/etc/systemd/system/tech-workflow.service"
    print(f"ğŸ“ SystemdæœåŠ¡æ–‡ä»¶å†…å®¹:")
    print(service_content)
    print(f"\nğŸ’¡ è¦å®‰è£…ä¸ºç³»ç»ŸæœåŠ¡ï¼Œè¯·è¿è¡Œ:")
    print(f"sudo tee {service_file} << 'EOF'")
    print(service_content)
    print("EOF")
    print("sudo systemctl enable tech-workflow")
    print("sudo systemctl start tech-workflow")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æŠ€æœ¯ä¿¡æ¯å·¥ä½œæµå®ˆæŠ¤è¿›ç¨‹')
    parser.add_argument('--daemon', action='store_true', help='ä»¥å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œ')
    parser.add_argument('--create-service', action='store_true', help='åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶')
    parser.add_argument('--status', action='store_true', help='æŸ¥çœ‹å®ˆæŠ¤è¿›ç¨‹çŠ¶æ€')
    parser.add_argument('--stop', action='store_true', help='åœæ­¢å®ˆæŠ¤è¿›ç¨‹')
    
    args = parser.parse_args()
    
    if args.create_service:
        create_systemd_service()
        return
    
    if args.status:
        # æŸ¥çœ‹çŠ¶æ€
        status_file = "daemon_status.json"
        if os.path.exists(status_file):
            with open(status_file, 'r', encoding='utf-8') as f:
                status = json.load(f)
            print(f"ğŸ“Š å®ˆæŠ¤è¿›ç¨‹çŠ¶æ€: {status['status']}")
            print(f"ğŸ“ æœ€åæ¶ˆæ¯: {status['message']}")  
            print(f"â° æ›´æ–°æ—¶é—´: {status['timestamp']}")
            if 'uptime_seconds' in status:
                uptime_hours = status['uptime_seconds'] / 3600
                print(f"â±ï¸ è¿è¡Œæ—¶é•¿: {uptime_hours:.1f} å°æ—¶")
        else:
            print("âŒ å®ˆæŠ¤è¿›ç¨‹æœªè¿è¡Œæˆ–çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    if args.stop:
        # åˆ›å»ºåœæ­¢ä¿¡å·æ–‡ä»¶
        stop_file = Path("triggers/stop_daemon.trigger")
        stop_data = {
            'command': 'stop_daemon',
            'timestamp': datetime.now().isoformat()
        }
        with open(stop_file, 'w', encoding='utf-8') as f:
            json.dump(stop_data, f, ensure_ascii=False, indent=2)
        print("ğŸ›‘ åœæ­¢ä¿¡å·å·²å‘é€")
        return
    
    if args.daemon:
        # å¯åŠ¨å®ˆæŠ¤è¿›ç¨‹
        daemon = TechWorkflowDaemon()
        daemon.start_time = datetime.now()
        daemon.start_daemon()
    else:
        # æ˜¾ç¤ºå¸®åŠ©
        print("ğŸ¤– æŠ€æœ¯ä¿¡æ¯å·¥ä½œæµå®ˆæŠ¤è¿›ç¨‹")
        print("\nä½¿ç”¨æ–¹å¼:")
        print("  python tech_workflow_daemon.py --daemon          # å¯åŠ¨å®ˆæŠ¤è¿›ç¨‹")
        print("  python tech_workflow_daemon.py --status          # æŸ¥çœ‹çŠ¶æ€")  
        print("  python tech_workflow_daemon.py --stop            # åœæ­¢å®ˆæŠ¤è¿›ç¨‹")
        print("  python tech_workflow_daemon.py --create-service  # åˆ›å»ºç³»ç»ŸæœåŠ¡")

if __name__ == "__main__":
    main()