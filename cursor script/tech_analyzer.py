#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ€æœ¯ä¿¡æ¯æ™ºèƒ½åˆ†æå™¨
åŸºäºæ”¶é›†çš„æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æï¼Œç”Ÿæˆæ´å¯Ÿæ€§å†…å®¹
"""

import json
import requests
import re
from datetime import datetime
from typing import List, Dict, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

@dataclass
class TechAnalysis:
    """æŠ€æœ¯åˆ†æç»“æœ"""
    title: str
    analysis_type: str  # project_analysis, trend_report, competitor_comparison
    content: str
    insights: List[str]
    timeline: List[Dict]
    references: List[str]
    generated_at: datetime

class TechAnalyzer:
    """æŠ€æœ¯ä¿¡æ¯åˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_templates = self._load_analysis_templates()
    
    def _load_analysis_templates(self) -> Dict:
        """åŠ è½½åˆ†ææ¨¡æ¿"""
        return {
            'project_analysis': {
                'structure': [
                    'ğŸ§  ã€æ€ç»´è·¯çº¿å¯¼èˆªã€‘',
                    'ğŸ“š é¡¹ç›®å‘å±•å†ç¨‹', 
                    'ğŸ¯ è®¾è®¡å“²å­¦æ·±åº¦è§£æ',
                    'ğŸ”§ æ ¸å¿ƒæŠ€æœ¯å®ç°',
                    'âš–ï¸ ç«å“å¯¹æ¯”åˆ†æ',
                    'ğŸ”® è¶‹åŠ¿æ´å¯Ÿä¸æœªæ¥å±•æœ›',
                    'ğŸ”— å‚è€ƒèµ„æ–™'
                ]
            },
            'trend_report': {
                'structure': [
                    'ğŸ¯ æ ¸å¿ƒå‘ç°',
                    'ğŸ“… æŠ€æœ¯å‘å±•æ—¶é—´çº¿',
                    'ğŸ”¥ çƒ­é—¨é¡¹ç›®è§£æ',
                    'ğŸ“Š æ•°æ®æ´å¯Ÿ',
                    'ğŸ’¡ ä¸ªäººè§‚ç‚¹',
                    'ğŸ”— æ·±åº¦é˜…è¯»'
                ]
            },
            'competitor_comparison': {
                'structure': [
                    'ğŸ¯ å¯¹æ¯”æ¦‚è§ˆ',
                    'ğŸ“Š æŠ€æœ¯æ¶æ„å¯¹æ¯”',
                    'âš¡ æ€§èƒ½è¡¨ç°åˆ†æ',
                    'ğŸŒ ç”Ÿæ€ç³»ç»Ÿå¯¹æ¯”',
                    'ğŸ’¼ å•†ä¸šæ¨¡å¼åˆ†æ',
                    'ğŸ† ç»¼åˆè¯„ä»·',
                    'ğŸ”— å‚è€ƒèµ„æ–™'
                ]
            }
        }
    
    def analyze_github_project(self, repo_url: str, collected_data: Dict = None) -> TechAnalysis:
        """æ·±åº¦åˆ†æGitHubé¡¹ç›®"""
        logger.info(f"å¼€å§‹åˆ†æé¡¹ç›®: {repo_url}")
        
        # æå–ä»“åº“ä¿¡æ¯
        repo_info = self._extract_repo_info(repo_url)
        if not repo_info:
            return None
        
        # æ”¶é›†é¡¹ç›®ç›¸å…³ä¿¡æ¯
        project_data = self._collect_project_data(repo_info)
        
        # ç”Ÿæˆåˆ†æå†…å®¹
        analysis_content = self._generate_project_analysis(repo_info, project_data)
        
        # ç”Ÿæˆæ´å¯Ÿ
        insights = self._generate_project_insights(repo_info, project_data)
        
        # æ„å»ºæ—¶é—´çº¿
        timeline = self._build_project_timeline(repo_info, project_data)
        
        # æ”¶é›†å‚è€ƒèµ„æ–™
        references = self._collect_project_references(repo_info, project_data)
        
        return TechAnalysis(
            title=f"{repo_info['name']} æŠ€æœ¯æ·±åº¦è§£æ",
            analysis_type="project_analysis",
            content=analysis_content,
            insights=insights,
            timeline=timeline,
            references=references,
            generated_at=datetime.now()
        )
    
    def _extract_repo_info(self, repo_url: str) -> Dict:
        """ä»GitHub URLæå–ä»“åº“ä¿¡æ¯"""
        try:
            # ä»URLæå–ownerå’Œrepo
            pattern = r'github\.com/([^/]+)/([^/]+)'
            match = re.search(pattern, repo_url)
            if not match:
                return None
            
            owner, repo = match.groups()
            
            # è°ƒç”¨GitHub APIè·å–è¯¦ç»†ä¿¡æ¯
            api_url = f"https://api.github.com/repos/{owner}/{repo}"
            response = requests.get(api_url)
            response.raise_for_status()
            
            repo_data = response.json()
            
            return {
                'owner': owner,
                'name': repo,
                'full_name': repo_data['full_name'],
                'description': repo_data.get('description', ''),
                'language': repo_data.get('language', ''),
                'stars': repo_data['stargazers_count'],
                'forks': repo_data['forks_count'],
                'created_at': repo_data['created_at'],
                'updated_at': repo_data['updated_at'],
                'topics': repo_data.get('topics', []),
                'license': repo_data.get('license', {}).get('name', 'Unknown') if repo_data.get('license') else 'Unknown'
            }
            
        except Exception as e:
            logger.error(f"è·å–ä»“åº“ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _collect_project_data(self, repo_info: Dict) -> Dict:
        """æ”¶é›†é¡¹ç›®ç›¸å…³æ•°æ®"""
        data = {
            'basic_info': repo_info,
            'releases': self._get_project_releases(repo_info),
            'contributors': self._get_top_contributors(repo_info),
            'issues_stats': self._get_issues_statistics(repo_info)
        }
        return data
    
    def _get_project_releases(self, repo_info: Dict) -> List[Dict]:
        """è·å–é¡¹ç›®å‘å¸ƒå†å²"""
        try:
            url = f"https://api.github.com/repos/{repo_info['full_name']}/releases"
            response = requests.get(url, params={'per_page': 10})
            response.raise_for_status()
            
            releases = []
            for release in response.json():
                releases.append({
                    'tag_name': release['tag_name'],
                    'name': release['name'],
                    'published_at': release['published_at'],
                    'body': release.get('body', '')[:200]  # æˆªå–å‰200å­—ç¬¦
                })
            
            return releases
            
        except Exception as e:
            logger.error(f"è·å–å‘å¸ƒå†å²å¤±è´¥: {e}")
            return []
    
    def _get_top_contributors(self, repo_info: Dict) -> List[Dict]:
        """è·å–ä¸»è¦è´¡çŒ®è€…"""
        try:
            url = f"https://api.github.com/repos/{repo_info['full_name']}/contributors"
            response = requests.get(url, params={'per_page': 10})
            response.raise_for_status()
            
            contributors = []
            for contributor in response.json():
                contributors.append({
                    'login': contributor['login'],
                    'contributions': contributor['contributions'],
                    'avatar_url': contributor['avatar_url']
                })
            
            return contributors
            
        except Exception as e:
            logger.error(f"è·å–è´¡çŒ®è€…ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def _get_issues_statistics(self, repo_info: Dict) -> Dict:
        """è·å–Issuesç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–å¼€æ”¾Issuesæ•°é‡
            url = f"https://api.github.com/repos/{repo_info['full_name']}/issues"
            response = requests.get(url, params={'state': 'open', 'per_page': 1})
            
            stats = {
                'open_issues': repo_info.get('open_issues_count', 0),
                'recent_activity': response.headers.get('Link') is not None
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–Issuesç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def _generate_project_analysis(self, repo_info: Dict, project_data: Dict) -> str:
        """ç”Ÿæˆé¡¹ç›®æ·±åº¦åˆ†æå†…å®¹"""
        
        # åŸºäºæ¨¡æ¿ç”Ÿæˆåˆ†æå†…å®¹
        analysis_parts = []
        
        # æ€ç»´è·¯çº¿å¯¼èˆª
        navigation = f"""
## ğŸ§  ã€æ€ç»´è·¯çº¿å¯¼èˆªã€‘

åœ¨æ·±å…¥æ¢ç´¢ {repo_info['name']} ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå»ºç«‹æ€è€ƒæ¡†æ¶ï¼š

- **å†å²èƒŒæ™¯**ï¼š{repo_info['name']} ä¸ºä»€ä¹ˆä¼šè¯ç”Ÿï¼Ÿè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- **è®¾è®¡å“²å­¦**ï¼šæ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ä»€ä¹ˆï¼Ÿæœ‰å“ªäº›ç‹¬ç‰¹çš„æŠ€æœ¯é€‰æ‹©ï¼Ÿ
- **æŠ€æœ¯å®ç°**ï¼šå…³é”®æŠ€æœ¯æ˜¯å¦‚ä½•è½åœ°çš„ï¼Ÿ
- **ç«äº‰åˆ†æ**ï¼šä¸åŒç±»é¡¹ç›®ç›¸æ¯”æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
- **æœªæ¥è¶‹åŠ¿**ï¼šé¡¹ç›®çš„å‘å±•æ–¹å‘å¦‚ä½•ï¼Ÿ

---
"""
        analysis_parts.append(navigation)
        
        # é¡¹ç›®å‘å±•å†ç¨‹
        timeline_section = f"""
## ğŸ“š é¡¹ç›®å‘å±•å†ç¨‹

### ğŸ¯ åŸºæœ¬ä¿¡æ¯
- **é¡¹ç›®åç§°**ï¼š{repo_info['full_name']}
- **ä¸»è¦è¯­è¨€**ï¼š{repo_info['language']}
- **åˆ›å»ºæ—¶é—´**ï¼š{repo_info['created_at'][:10]}
- **å½“å‰çŠ¶æ€**ï¼šâ­ {repo_info['stars']:,} Starsï¼ŒğŸ´ {repo_info['forks']:,} Forks
- **å¼€æºåè®®**ï¼š{repo_info['license']}

### ğŸ“… é‡è¦é‡Œç¨‹ç¢‘
"""
        
        # æ·»åŠ å‘å¸ƒå†å²
        if project_data['releases']:
            timeline_section += "\n**ä¸»è¦ç‰ˆæœ¬å‘å¸ƒ**ï¼š\n"
            for release in project_data['releases'][:5]:
                timeline_section += f"- **{release['tag_name']}** ({release['published_at'][:10]}): {release['name']}\n"
        
        analysis_parts.append(timeline_section)
        
        # è®¾è®¡å“²å­¦åˆ†æï¼ˆè¿™é‡Œéœ€è¦åŸºäºé¡¹ç›®ç‰¹ç‚¹ç”Ÿæˆï¼‰
        philosophy_section = f"""
## ğŸ¯ è®¾è®¡å“²å­¦æ·±åº¦è§£æ

### æ ¸å¿ƒè®¾è®¡ç†å¿µ
{repo_info['description']}

### æŠ€æœ¯é€‰æ‹©åˆ†æ
- **ä¸»è¦æŠ€æœ¯æ ˆ**ï¼š{repo_info['language']}
- **é¡¹ç›®æ ‡ç­¾**ï¼š{', '.join(repo_info['topics'][:5]) if repo_info['topics'] else 'æš‚æ— æ ‡ç­¾'}

*ï¼ˆåŸºäºé¡¹ç›®ç»“æ„å’Œä»£ç åˆ†æçš„æ·±åº¦å“²å­¦è§£æéœ€è¦è¿›ä¸€æ­¥çš„ä»£ç å®¡æŸ¥å’Œæ¶æ„åˆ†æï¼‰*

---
"""
        analysis_parts.append(philosophy_section)
        
        return '\n'.join(analysis_parts)
    
    def _generate_project_insights(self, repo_info: Dict, project_data: Dict) -> List[str]:
        """ç”Ÿæˆé¡¹ç›®æ´å¯Ÿ"""
        insights = []
        
        # åŸºäºæ•°æ®ç”Ÿæˆæ´å¯Ÿ
        if repo_info['stars'] > 10000:
            insights.append(f"â­ é«˜å…³æ³¨åº¦é¡¹ç›®ï¼š{repo_info['stars']:,} starsè¡¨æ˜è¯¥é¡¹ç›®åœ¨ç¤¾åŒºä¸­å…·æœ‰é‡è¦å½±å“åŠ›")
        
        if repo_info['forks'] > repo_info['stars'] * 0.1:
            insights.append(f"ğŸ´ æ´»è·ƒå¼€å‘ï¼šForkæ¯”ä¾‹({repo_info['forks']/repo_info['stars']:.2%})è¡¨æ˜å¼€å‘è€…ç§¯æå‚ä¸è´¡çŒ®")
        
        # åˆ†æè´¡çŒ®è€…åˆ†å¸ƒ
        if project_data['contributors']:
            top_contributor = project_data['contributors'][0]
            contribution_ratio = top_contributor['contributions'] / sum(c['contributions'] for c in project_data['contributors'][:10])
            if contribution_ratio > 0.5:
                insights.append(f"ğŸ‘‘ æ ¸å¿ƒç»´æŠ¤è€…ï¼š{top_contributor['login']} è´¡çŒ®äº† {contribution_ratio:.1%} çš„ä»£ç ï¼Œé¡¹ç›®å…·æœ‰å¼ºä¸­å¿ƒåŒ–ç‰¹å¾")
            else:
                insights.append(f"ğŸ¤ åˆ†å¸ƒå¼å¼€å‘ï¼šè´¡çŒ®ç›¸å¯¹åˆ†æ•£ï¼Œä½“ç°äº†å¥åº·çš„å¼€æºåä½œæ¨¡å¼")
        
        return insights
    
    def _build_project_timeline(self, repo_info: Dict, project_data: Dict) -> List[Dict]:
        """æ„å»ºé¡¹ç›®æ—¶é—´çº¿"""
        timeline = []
        
        # åˆ›å»ºæ—¶é—´
        timeline.append({
            'date': repo_info['created_at'][:10],
            'event': f"é¡¹ç›®åˆ›å»ºï¼š{repo_info['full_name']} åœ¨GitHubä¸Šåˆ›å»º",
            'type': 'creation'
        })
        
        # é‡è¦ç‰ˆæœ¬å‘å¸ƒ
        for release in project_data['releases'][:3]:
            timeline.append({
                'date': release['published_at'][:10],
                'event': f"ç‰ˆæœ¬å‘å¸ƒï¼š{release['tag_name']} - {release['name']}",
                'type': 'release'
            })
        
        return sorted(timeline, key=lambda x: x['date'])
    
    def _collect_project_references(self, repo_info: Dict, project_data: Dict) -> List[str]:
        """æ”¶é›†é¡¹ç›®å‚è€ƒèµ„æ–™"""
        references = []
        
        # GitHubä»“åº“é“¾æ¥
        references.append(f"https://github.com/{repo_info['full_name']}")
        
        # å¦‚æœæœ‰å®˜æ–¹æ–‡æ¡£ï¼Œæ·»åŠ é“¾æ¥ï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•æ™ºèƒ½è¯†åˆ«ï¼‰
        if 'docs' in repo_info['topics'] or 'documentation' in repo_info['topics']:
            references.append(f"https://{repo_info['name']}.readthedocs.io")
        
        return references
    
    def generate_trend_report(self, collected_data: Dict) -> TechAnalysis:
        """ç”ŸæˆæŠ€æœ¯è¶‹åŠ¿æŠ¥å‘Š"""
        logger.info("ç”ŸæˆæŠ€æœ¯è¶‹åŠ¿æŠ¥å‘Š")
        
        # åˆ†æè¶‹åŠ¿æ•°æ®
        trends = self._analyze_trends(collected_data)
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        content = self._format_trend_report(trends)
        
        return TechAnalysis(
            title=f"æŠ€æœ¯è¶‹åŠ¿è°ƒç ”æŠ¥å‘Š - {datetime.now().strftime('%Yå¹´%mæœˆ')}",
            analysis_type="trend_report",
            content=content,
            insights=trends['insights'],
            timeline=trends['timeline'],
            references=trends['references'],
            generated_at=datetime.now()
        )
    
    def _analyze_trends(self, collected_data: Dict) -> Dict:
        """åˆ†ææŠ€æœ¯è¶‹åŠ¿"""
        trends = {
            'hot_topics': [],
            'emerging_technologies': [],
            'insights': [],
            'timeline': [],
            'references': []
        }
        
        # åˆ†æGitHubè¶‹åŠ¿é¡¹ç›®
        if 'github_trending' in collected_data:
            github_data = collected_data['github_trending']
            
            # ç»Ÿè®¡çƒ­é—¨æŠ€æœ¯æ ˆ
            languages = {}
            topics = {}
            
            for project in github_data:
                # ç»Ÿè®¡ç¼–ç¨‹è¯­è¨€
                lang = project.get('metrics', {}).get('language', 'Unknown')
                languages[lang] = languages.get(lang, 0) + 1
                
                # ç»Ÿè®¡è¯é¢˜æ ‡ç­¾
                project_tags = project.get('tags', [])
                for tag in project_tags:
                    topics[tag] = topics.get(tag, 0) + 1
            
            # ç”Ÿæˆæ´å¯Ÿ
            if languages:
                top_lang = max(languages.items(), key=lambda x: x[1])
                trends['insights'].append(f"ğŸ”¥ çƒ­é—¨è¯­è¨€ï¼š{top_lang[0]} åœ¨è¶‹åŠ¿é¡¹ç›®ä¸­å ä¸»å¯¼åœ°ä½ï¼ˆ{top_lang[1]}ä¸ªé¡¹ç›®ï¼‰")
            
            if topics:
                top_topics = sorted(topics.items(), key=lambda x: x[1], reverse=True)[:3]
                trends['insights'].append(f"ğŸ“ˆ çƒ­é—¨è¯é¢˜ï¼š{', '.join([t[0] for t in top_topics])} æˆä¸ºå¼€å‘è€…å…³æ³¨ç„¦ç‚¹")
        
        # åˆ†æHacker Newsè®¨è®º
        if 'hackernews' in collected_data:
            hn_data = collected_data['hackernews']
            
            # ç»Ÿè®¡è®¨è®ºçƒ­åº¦
            high_engagement = [post for post in hn_data if post.get('metrics', {}).get('comments', 0) > 50]
            if high_engagement:
                trends['insights'].append(f"ğŸ’¬ ç¤¾åŒºçƒ­è®®ï¼š{len(high_engagement)} ä¸ªæŠ€æœ¯è¯é¢˜å¼•å‘æ¿€çƒˆè®¨è®º")
        
        return trends
    
    def _format_trend_report(self, trends: Dict) -> str:
        """æ ¼å¼åŒ–è¶‹åŠ¿æŠ¥å‘Š"""
        content_parts = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        header = f"""
# ğŸ“ˆ æŠ€æœ¯è¶‹åŠ¿è°ƒç ”æŠ¥å‘Š - {datetime.now().strftime('%Yå¹´%mæœˆ')}

## ğŸ¯ æ ¸å¿ƒå‘ç°

ä»¥ä¸‹æ˜¯æœ¬æœŸæŠ€æœ¯ç”Ÿæ€çš„å…³é”®å‘ç°ï¼š
"""
        content_parts.append(header)
        
        # æ´å¯Ÿåˆ—è¡¨
        if trends['insights']:
            insights_section = "\n### ğŸ’¡ ä¸»è¦æ´å¯Ÿ\n\n"
            for i, insight in enumerate(trends['insights'], 1):
                insights_section += f"{i}. {insight}\n"
            content_parts.append(insights_section)
        
        return '\n'.join(content_parts)
    
    def save_analysis(self, analysis: TechAnalysis, output_dir: str = "output") -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = analysis.generated_at.strftime("%Y%m%d_%H%M%S")
        filename = f"{analysis.analysis_type}_{timestamp}.md"
        filepath = os.path.join(output_dir, filename)
        
        # ç”Ÿæˆå®Œæ•´çš„Markdownå†…å®¹
        full_content = f"""# {analysis.title}

{analysis.content}

## ğŸ’¡ å…³é”®æ´å¯Ÿ

{chr(10).join(f'- {insight}' for insight in analysis.insights)}

## ğŸ“… æ—¶é—´çº¿

{chr(10).join(f"- **{event['date']}**: {event['event']}" for event in analysis.timeline)}

## ğŸ”— å‚è€ƒèµ„æ–™

{chr(10).join(f'- {ref}' for ref in analysis.references)}

---
*ç”Ÿæˆæ—¶é—´ï¼š{analysis.generated_at.strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    analyzer = TechAnalyzer()
    
    # æ¨¡æ‹Ÿæ”¶é›†çš„æ•°æ®
    sample_data = {
        'github_trending': [
            {
                'title': 'microsoft/garnet',
                'metrics': {'language': 'C#', 'stars': 8500},
                'tags': ['redis', 'cache', 'performance']
            }
        ]
    }
    
    # ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
    trend_analysis = analyzer.generate_trend_report(sample_data)
    output_file = analyzer.save_analysis(trend_analysis)
    print(f"è¶‹åŠ¿æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")