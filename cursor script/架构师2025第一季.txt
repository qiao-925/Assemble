
 
 
CONTENTS / 目录 
特别专题 | Agentic 软件革命 
传统数据仓库正在被 Agentic AI 吞噬？Agentic Data Stack 初探 
Agentic AI 要终结数据库和 SaaS？大厂掌门人公开互撕，焦虑的 CEO 们押上了不同的技术路
线 
被骂惨的“现象级”Manus，今天我们来扒一扒它的真实水平！ 
GPT-4o“吉卜力”爆火，Prompt、SD 白学了？！大模型能力进化碾压一切 
资源有限，如何构建高效能的 AI Agent 
Agent 驱动的智能答疑产品构建：问答、诊断与修复实践 
Andrej Karpathy 爆火演讲刷屏技术圈：AI 开启软件 3.0，重写一切的时代来了！ 
 
访谈文章 | Interview 
吴恩达评 Agent 现状：MCP 还欠火候，单 Agent 跑通已是“奇迹”，A2A 协作堪称“双重
奇迹” 
“我已经过时了”！83 岁图灵奖大师、龙书作者在大模型时代的技术焦虑：越来越难以适应新
技术 
“不是 Cursor 不够强，是 Claude Code 太猛了”！创始人详解 Claude Code 如何改写编程
方式 
微软重磅开源 Copilot！64 岁 VS Code 创始人亲口承认：眼红 Cursor，但真正价值在后端，
它“抄”不过去！ 
年赚三亿美金、估值近百亿，Cursor 竟无护城河？ 
月烧 4 万元，两工程师用 Claude Code 跑出 15 人团队效率：值不值全网吵翻了！ 
 
II 
热门演讲实录 | 落地和进化 
AI 驱动的智能化单元测试生成：字节跳动的实践与创新 
去哪儿网前端代码自动生成技术实践 
AI 创新应用 C 端 B 端商业化实践，从中国走向全球 
大模型赋能电商 B 端，快手电商技术实践深度揭秘 
Databricks × Snowflake 纷纷下注，PostgreSQL 成 AI 时代数据库标准？ 
小红书鸿蒙 OS 下的性能优化探索与实践 
复杂场景下的 RAG 架构演进：跨模态知识联邦与统一语义推理实践 
揭秘千卡 GPU 集群如何高效训练多模态大模型：vivo AI 团队实战经验分享 
 
推荐文章 | Article 
Java 三十周年重磅发声：James Gosling 狠批 AI 是“一场骗局”，是科技高管“压榨程序员
的新利器” 
Redis 之父：哪怕被喷我也得说，AI 远远落后于人类程序员！ 
“前端已死”是危言耸听吗？ 
InfoQ 2025 年趋势报告：软件架构和设计 
AI 将如何颠覆传统软件开发团队 
API 网关十五年演进：从微服务核心到 AI 时代的神经网络 
“AI 六小虎”两年混战，新的较量开始 
 
 
架构师 2025 年第一季 
 
本期主编 Tina        反馈 feedback@geekbang.com 
流程编辑 丁晓昀      商务合作 hezuo@geekbang.org 
发行人 霍泰稳        内容合作 editors@geekbang.com 
 
1 
InfoQ 架构师2025年第一季 
卷首语 
十年前，我们惊叹于 AI 能“补全代码”“补个文档”。而今天，Agentic AI 已悄
然改变了软件开发的整个生态，它不只是一个工具，更像是一个新物种——能理解业务、
自动编写、持续演进的“数字工程师”。 
这一波变革，比过去任何一次技术跃迁都更彻底。它深刻重塑了程序员的角色：写
代码不再是核心能力，理解问题、设计解决方案、与智能体协同，成为新的竞争力。程
序员正从以“活动”（如编码）为导向，转变为以“结果”（如解决问题）为导向，其
核心职责是确保智能体伙伴交付的成果精准满足需求。 
它也颠覆了企业的技术策略：决策焦点从“该不该买 SaaS”转向了“能不能快速
生成自己的软件”。购买 SaaS 意味着放弃知识产权、受制于供应商的更新节奏，定制
需求常被搁置。而 Agentic AI 提供了新路径：自建软件不仅显著提速，更能灵活构建那
关键的 20%定制功能，精准契合业务需求。由于问题规模更小、开发周期更短、成本更
低，且核心 IP 和创新节奏完全自主掌控，Agentic AI 为曾受困于预算、资源、人才的中
小企业，开辟了一条前所未有的数字化捷径。 
这本关于“Agentic AI 十年”的电子书，很快会让你发现，我们已走得比想象中更
远：Agent 能读懂你的老旧代码，能自己给出改造建议，甚至能写出文档解释“我为什
么这么写”；它能遵守合规规则、复用已有代码、提醒你“这个方法在 repo 里早就
有”；它不是黑盒，而是“有逻辑、有记忆、有判断力的开发伙伴”。我们也许还没完
全适应，但变化早已不可逆转。 
这不是一本预测未来的书，而是一本记录当下变化的书。十年，不只是一个技术周
 
2 
卷首语 
期，更是一种承诺。Agentic AI 带来的，不只是开发方式的变化，更是在推动整个数字
世界加速进化。 
未来已经到来，智能正在成长。欢迎翻开这本书，和我们一起见证一个全新生态的
诞生。 
 

 
3 
InfoQ 架构师2025年第一季 
传统数据仓库正在被 Agentic AI 吞噬？
Agentic Data Stack 初探 
 
作者：郭炜 白鲸开源 CEO，Apache 基金会成员 
从技术架构的角度看，我认为这一次的 AI 浪潮将深刻影响整个软件生态。DSS 系
统的设计是以人作为最终消费者的决策支持逻辑为中心，然而，随着 Agentic AI 时代来
临，最终的“消费者”更可能是 Agent，对数据仓库和复杂 ETL 链路将被重新设计，甚
至消失。传统数据仓库偏重结构与查询模式，会被 Agentic Data Stack 架构强调语义与
响应模式取代。本文作者的原标题为《传统数据仓库正在被 Agentic AI 吞噬？Agentic 
Data Stack 初探》。 
 
作者 郭炜  策划 Tina 
 
4 
特别专题 | Agentic 软件革命 
引言：Snowflake 换 CEO 背后的信号 
2024 年春天，云数据仓库的明星公司 Snowflake 宣布换帅，前 Google 广告业务
负责人 Sridhar Ramaswamy 接替了曾带领 Snowflake 实现 600 亿美元估值的传奇 
CEO Frank Slootman。 
如果你只是把这当成一次高管轮换，理解就不够透彻，因为这背后真正的隐喻是，
数据仓库世界的范式，正在悄然巨变。 
“技术的演进，从来不是线性推进，而是技术的跃迁，从 OLTP 数据库到 MPP 数
据仓库，从 MPP 本地化计算到向量化云数仓引擎，都是一个技术跃迁到另一个技术，
从一个产品霸主到另一个产品霸主。” 
Slootman 是“数据仓库黄金时代”的代表。他押注云原生、押注多租户架构、押
注 Snowflake 成为新一代数据平台的中枢，直接在市场上干掉了我从业的第一家公司—
—当年的数据仓库霸主 Teradata（从 102 亿美金市值到现在 20 亿美金市值）。就在
他功成身退的这一刻，Snowflake 官方博客的关键词悄然切换：AI-first、Agent-driven、
语义导向的数据架构。 
这不是巧合，这是风向。 
同一时间，硅谷最具前瞻性的风投们正在押注“Agentic AI”这个新概念：AI 不再
只是一个模型，它是一个能感知、能行动、有目标、有协作能力的 Agent。 
那么问题来了： 
当 AI 不再只是“聊天工具”，而是能主动感知业务变化、理解意图并执行操作的
智能体时，传统数据仓库这样的为“人”建造的决策支持系统还可以满足 Agent 的需
要么？ 
数据仓库曾是企业的“重要的数据资产”，如今，却可能沦为 Agent 的“数据素
材库”。甚至连“素材”这个词都在贬值，因为 Agentic DataStack 可以直接访问原始数
据，并以语义 + 数据的形式直接供给给上层各类 Sales Agent，Risk Agent 直接使用；而
数据仓库里无语义、冗余的数据只能留给传统 BI 和数据开发人员来消费。 
 
5 
InfoQ 架构师2025年第一季 
真正危险的不是被淘汰，而是你还在运行上一代范式的规则，而世界已经换了剧本。 
这不是对数仓的轻视，而是历史的轮回。正如当年 Hadoop、Iceberg 的崛起重构了
数据湖，今天，Agentic AI 正在重写企业级的大数据架构。 
1970-2024：数据仓库架构是如何演进的 
1970：数据仓库之父：Bill Inmon 
数据仓库之父 Bill Inmon 首次提出“面向主题、集成、时变、不可更新的数据集合”
这一概念（EDW），奠定了后半个世纪企业数据架构的基石。 
我本人也有幸在 20 多年前在北京大学的时候，在唐世谓教授带领下，学习并参与
翻译《数据仓库》第一版，这本书里对主题域、数据分层架构和缓变维（历史拉链表）
的描述，从上个世纪一直沿用到今天，成为整体数据仓库的奠基之作。 
 
1983：Teradata 诞生，MPP 架构横空出世 
1983 年诞生了未来 30 年横扫所有企业数据仓库基础设施的公司 Teradata，这也
是我毕业后第一份工作所在的公司。首次将 MPP（大规模并行处理）架构引入数据处
理系统，Teradata 凭借软硬一体的基于 Bynet 的 MPP 架构，在超大量级数据处理和复
杂 SQL 的情况下，比 Oracle、DB2 效率高出数倍。第一次使用 Teradata 的时候我的惊
喜不亚于后来我首次测试使用 ClickHouse 做宽表查询时的惊诧。 
 
6 
特别专题 | Agentic 软件革命 
我加入 Teradata 的时候，他还是一个 NCR 旗下的部门，我名片 logo 是这样子的，
想了解 Teradata 的同学可以看我这一篇文章《再见，我的数仓黄埔军校，Teradata 正
式退出中国！》。 
 
1996：Kimball 提出“雪花模型”，OLAP 引擎出现 
继 Bill Immon 之后，Ralph Kimball 提出了“数据集市的概念”用星型模型和雪花模
型重新定义了数据建模思维。此后数十年间，先建立数据集市还是先建立统一的数据仓
库，变成数据仓库架构师不停争论的话题。“维度建模”和“雪花模型”成为数据工程
师的名片；而 BI 报表底层也出现了例如 Hyprion ESSbase，Cognos 等 MOLAP 引擎，
OLAP 技术也终于有了系统方法论支撑。 
在几十年后，新一代的数据仓库公司也用了 Snowflake（雪花模型）作为其公司名
称。 
2013：大数据概念爆发，Hadoop 风靡全球 
随着 2006 年 Apache Hadoop 的横空出世，低存储成本的大数据系统被企业广泛
引用。维克托·迈尔 - 舍恩伯格在《大数据时代》中给大数据下了定义：“Volume
（数据量）、Velocity（数据速度）、Variety（数据多样性）、Value（数据价值）。” 
从此轰轰烈烈的建立大数据平台的过程开始起步，10 年内，Apache Hadoop、Hive、
Spark、Kafka、DolphinScheduler、SeaTunnel、Iceberg……一批大数据技术涌现，大数据
平台开始动摇传统数据仓库的地位，以致于 2015 年后的中国，大多数中国企业存储 
Pb 数据量级的数据平台不会用 MPP 架构传统意义数据仓库，而一定是用 Hadoop 或
 
7 
InfoQ 架构师2025年第一季 
者 Iceberg 大数据平台 / 数据湖。 
2015：Snowflake 横空出世，New DataStack 兴起 
随着云的兴起，Marcin Zukowski “向量化”引擎论文的推出，Snowflake 横空出世
用云原生分离存算的架构，彻底颠覆了传统 DW 思维。BI 工程师第一次可以“随需随
用”、弹性扩缩容、不再焦虑集群调度和资源分配。Snowflake 把“数仓”变成了“数
云”。它带领下一众新一代数据仓库技术栈兴起，Fivetran、Daggster、Airbyte、DBT、
WhaleStudio 等一批新一代工具出现，在硅谷兴起了 New Data Stack （新数据技术栈）
的风潮。的确，上一代 ETL 工具和编程工具还是上个世纪 80 年代兴起的 Informatica、
Talend、DataStage 这些公司，新技术的兴起的确需要新生态的形成。 
整体上，这几十年数据仓库的发展，无论是数据仓库、大数据平台和云数仓和数据
湖，基本上整体架构都如下图所示： 
 
在 Inmon 时代，这个架构叫做 DSS 系统（决策支持系统），顾名思义，决策支持
的就是人。整个数据仓库技术栈都是为人而设计的。 
数据仓库的架构也是为数据开发工程师（Data Engineer）来设计的，所以会有 N 个
主题域、要分原子层、汇总层、指标层来帮助 ETL 工程师进行开发，BI 工具也需要建
 
8 
特别专题 | Agentic 软件革命 
立星型模型和雪花模型，拖拉拽可视化形成报表和 Dashboard。所有的消费者都是人。 
但是，这一切，在大模型 Agent 时代都会发生很大的变化。 
Agent 正在吞噬传统数据仓库？！ 
2022 年底，OpenAI 推出 ChatGPT，引爆大模型时代。 
2023 年后，Llama、Claude、Gemini、GPT-4o、DeepSeek……多模态模型加速演进，
AI 不再只是语言模型，而是具备复杂任务理解与决策能力的“通用智能引擎”。 
2024 年，RAG 技术走向主流，LlamaIndex、LangChain、Dify 等工具广泛应用，AI 
开始融合企业私域知识，成为真正“能查资料”的智能助手。 
2025 年，Agent 架构全面崛起，AutoGPT、Function Calling、MCP 协议等技术和协
议涌现，AI 不再只是聊天工具，而是具备感知、规划与执行能力的“数字员工”。 
在数据领域，大模型的到来也带来很大的冲击。你用过 ChatGPT 的 DataAnalyst 
么？如果用过，你一定惊异它的表现，它可以根据一份数据多个角度来辅助一个业务人
员做一份详细的数据分析报告。它几乎可以替代初级数据分析师。而在不同层次也出现
了很多“自动化”工具，例如 ChatBI、TXT2SQL，各个维度都开始利用大模型和 Agent 
自动化和半自动化地进行数据仓库开发过程。 
 
未来，会有越来越多的 Agent 出现，不仅仅是数据分析领域，更多的的广告投放 
 
9 
InfoQ 架构师2025年第一季 
Agent，客服 Agent，Risk Managment Agent，它们将逐步解放现有的业务人员，替代他
们与系统之间的交互。 
最终，AI 不再是“被动回答问题的工具”，而是“主动达成目标的智能体”。 
过去 20 多年，数据平台的“用户”通常是数据工程师、分析师和 BI 人员。 
而未来的 20 年，从分析师到供应链，每一个岗位的角色都可能被 Agent 所重构： 
• 营销人员配有 Campaign Agent，它可以自动整合多渠道数据、优化投放、生
成文案； 
• 客服坐席配有 Support Agent，它就不只是聊天机器人，而是具备知识图谱和
上下文记忆的专属助理； 
• 供应链部门配有 Procurement Agent，它就能解析订单、追踪货期、调用 ERP 
数据并自动补货； 
• 法务有  Compliance Agent，HR 有  Hiring Agent，董事会也有  Board 
Agent…… 
你过去每天写的 SQL、做的分析报告、开的运营会，正在变成一个个 Agent 的触
发动作、语义指令和自动响应。 
 
但一个现实问题随之而来： 
如果最终数据消费者都已经是 Agent，数据仓库开发也是 Agent，连具体使用数据
的决策者都是 Agent 而不是“用户”的时候，原先为人设计的“决策支持系统 DSS”
 
10 
特别专题 | Agentic 软件革命 
数据仓库整体架构还成立么？ 
学过软件工程的 IT 码农们都知道，设计一个系统首先要做的图就是“Use Case”
图，确定系统和用户的边界和操作场景与行为。 
当数据仓库的用户从人变成 Agent 的时候，原先 Bill Inmon 老爷子设计的整体 
DSS 架构还成立么？我个人认为，不成立了。 
软件用户变了，软件也必须变。 
Agent 的爆发，并不是大模型本身的胜利，而是“用户体验认知”被彻底颠覆： 
• 过去的数据系统，是“拉模式”：用户知道问题、查询数据、提取结论。 
• 未来的 Agent，是“推模式”：系统主动感知变化，理解意图，生成决策建议。 
这就像我们从传统地图升级到高德导航： 
你不再需要知道“路在哪儿”，而是告诉系统你要去哪，它带你过去。 
传统数据仓库偏重结构与查询，而 Agentic 架构强调语义与响应。 
简而言之，谁能理解业务语言，谁就能统治数据世界。 
Agentic Data Stack 和自带上文的数据 Contextual Data Unit 
对于 Agent 自动开发和使用来讲，当前数据仓库整体设计并不是为大模型和 Agent 
设计的，所以，里面存储的都是“裸”数据。只有具体的数值和字段名称，而这个数值、
这个字段名称是做什么用的，都存在另外一个叫做“数据资产”的项目里。想把每个数
值、字段搞明白，需要进行一个“数据治理”的项目才可以完成。这个设计，对于语义
才可以进行推理的大模型和 Agent 太不友好了。所以，如果为 Agent 和 大模型重新设
计大数据存储系统的话，一定需要把“数据”+“语义”放到一起存储，我管它叫： 
Contextual Data Unit(CDU)：语义 + 数据组合单元，每个数据自带语义和语义解释的
二元组合。 
把过去在数据目录（Data Catalog）里的信息，融合在每个数据条目当中，减少 
 
11 
InfoQ 架构师2025年第一季 
Agent 和大模型访问的时候重新从其它系统里检索的时间和错误概率。 
同时，CDU 里面的语义数据也是从业务系统里经过总结和推衍得来的，所以，这里
的数据本身，就是在 Data Flow Agent 从源头就组合成 CDU，ETL/Data Ingesstion 到 
Agentic Data Lake 里，而不是后期生成的。换句话说，数据治理和溯源的过程是融入在 
Agent 的自动开发过程当中，而不是现在的做法——在数据进入数据仓库之后，再开始
血缘分析、数据治理一系列的复杂操作，这样做的结果数据很容易具有争议。 
到这里，大家应该看懂我的思路了，Agentic AI 时代，从过去的数据仓库 ETL 到数
据存储，到数据应用分析，都会因为消费者是 Agent 和大模型而发生很大的变化。为
了服务这些智能体，传统数据平台必须演进出一套 Agent 可调用、语义感知、事件驱
动的数据架构——也就是我们所说的 Agentic Data Stack。 
Agentic Data Stack：在 Agent 时代，从底层数据获取“语义 + 数据”的工具，到支
持 CDU 格式计算和存储的计算平台，到最终供给各 Agent 使用数据的数据交互层新一
代的数据技术栈。 
我大胆猜测下，未来 Agentic Data Stack 可能有以下组件组成： 
 
• “数据交互层”（Semantic Orchestrator）：不再是传统意义上的 BI/ 查询界面，
而是变成 Agentic 数据架构中的“大脑”和“指挥中心”，它通过自然语言理解
 
12 
特别专题 | Agentic 软件革命 
和语义推理能力，作为 其它 Agent 与底层数据资源之间的桥梁，实现智能化、
多轮次的数据交互与服务生成。 
• “数据存储层”（Data Mesh）：不再是传统意义上的 Data Warehouse（数据仓
库） 或 Data Lake（数据湖），而变成了一种服务性的、计算友好的数据融合层。
这个层的本质是 “存储提供融合语义 + 数据，既可供给大模型进行复杂计算的
存储，也可以提供即时复杂计算能力” 
• “数据处理层”（Data Flow Agent）：不再是“搬数据”，而是“理解和编排数
据”；不再定时运行，而是 事件驱动 + 意图驱动；能主动发现数据变化、分析
表结构、理解业务语义、做出响应。 
在 Agentic AI 时代，数据仓库和大数据平台的建设周期将极致地缩短，新数据的获
取经过 Data Flow Agent 的自发发现，在 Data Mesh 中预存储，Semantic Orchestrator 
解析和实际业务场景的业务口径与对应，最终实现从业务需求到数据响应的“即时计
算”。 
大模型解决的是智慧的大脑，Agent 解决的是手和脚，Agentic DataStack 是让 
LLM 和 Agent 具有适合大模型时代快速的数据获取能力。 
Agentic AI 时代，随着建立新一代“数据仓库”成本显著降低，拥有可以自由对话
查询，拥有相关的数据不再是大企业的权利，更是小企业甚至个人的权利。你可以把你
的 Google Drive，家里的私有 NAS，电脑上的 PDF，手机里的 APP 订单通过 Data Flow 
Agent 捕获到个人的数据存储里，用交互层 APP 快速查询例如“上个月去 Disney Land 
游玩一共花了多少钱”这种过去问题，而这种问题过去需要从多个平台整理到 Excel 表
格里记录，甚至还可以解决“找到 5 年前保险订单及相关合同”这种复杂问题。 
而这些并不是天方夜谭，最近由白鲸开源主导的 Apache SeaTunnel 社区里发布了 
Apache SeaTunnel MCP Server，已经开始 了迈向 Data Flow Agent 的步伐。当然，中间还
有很多未解决的技术问题，例如 A2A 协议还不够完善，DataMesh 层的“语义 + 数据”
存储计算结构还没有突破；把过去数据治理的成果变为 Semantic Orchestrator 输入也是
需要时间来探索。 
但是，大模型和 Agent 时代的到来，对于整个数据分析行业来说，就像从过去没
有 SQL 语言到出现 SQL 语言之后的进展一样，都会发生深刻的变化。 
 
13 
InfoQ 架构师2025年第一季 
打败你的，永远不是你现在眼中看到的所谓的“竞争对手”。讲个故事，小时候，
我熟悉两个自行车品牌——永久和凤凰。它们曾在“加速轴”技术上竞争，看谁能跑得
更快。然而，真正颠覆自行车市场的，却是一家外卖公司推出的共享单车，彻底改变了
整个行业格局。随着 Agent 时代的到来，许多曾被视为核心的产品路线可能会失去意
义。在低头看路的时候，也要抬头看天。 
小结：活在当下，放眼未来 
我在 AICon/AWS Community Day 和其它几个技术峰会上分享这个认知的时候，台下
观众完全分成两派：一派是“降临派”，认为我估计 Agentic Data Stack 到来 5-10 年 
太保守，AI 发展日新月异，5 年内 Agentic Data Stack 就会成型。一派是“保守派”，
认为 AI Agent 影响整个数据仓库架构太扯了，不可能发生，当前数据仓库存储形式就
是最优 ROI 的数据存储方式，任何不是最优 ROI 的形式都无法普遍商业化，都只是空
中楼阁，不要听这些“AI 专家”乱讲。 
而我个人是“中间派”：在趋势上，我认为 Agentic Data Stack 形成是一个必然，
这轮 AI 对技术架构的影响和前几次是完全不同的。不能只从技术观点上看数据仓库存
储计算层 ROI 的产出，而要看当前企业数据仓库整体建设过程和维护过程的投入算总
账。当前来看，实时数据仓库的兴起，数据湖的扩大，现在的数据仓库设计的层数在明
显减少（我甚至认为我们这一批当年 Teradata 训练过的模型架构师退役之后，市场上
都没有专业的数据仓库模型架构师了，因为业务变化太快，传统数据仓库专业模型设计
跟不上变化）。所以在高速变更的业务情况下，传统数据仓库理论自己也在迭代，（现
在实时数据仓库模型变成 2 层了，而不是过去的 3 层、4 层），只不过我看到的是未
来 Agentic AI 时代一步到位的趋势而已。算总账，Agentic Data Stack 会明显比现在的全
套数据仓库 New Data Stack ROI 高很多。 
但是，这个趋势也不是马上能降临的，以我 2016 看中 ClickHouse 这个产品开始
在中国运营社区，到 2020 年几乎成就了一代“实时 OLAP”引擎标准的时间来看，有
现成产品到被大家接受也要 4-5 年时间，而 Agentic Data Stack 只有部分组件有一些创
业公司雏形，大部分组件和核心产品还没有出世，也不可能 5 年内就一统天下。如果
说节奏，我估计怎样也在实时数据仓库和数据湖被大面积企业接受之后，才可能到下一
步 Agentic Data Stack。 
 
14 
特别专题 | Agentic 软件革命 
“不是 AI 取代你，而是会用 AI 的人取代你；不是数据仓库被吞噬了，而是传统
数据仓库偏重结构与查询模式，被 Agentic Data Stack 架构强调语义与响应模式取代了。
就像用上高德地图导航的人，不会再去看传统地图了。” 
Agentic Data Stack 的门已经徐徐打开。 
你，准备好了吗？ 
 
 
15 
InfoQ 架构师2025年第一季 
Agentic AI 要终结数据库和 SaaS？大厂掌门
人公开互撕，焦虑的 CEO 们押上了不同的技
术路线 
 
Agent 正在成为 2025 年 AI 世界最炙手可热的关键词之一。 
无论是大模型厂商、AI 初创公司，还是企业级应用团队，几乎都在讨论“多智能体
协作”“自动化决策流程”以及“具备工具调用能力的 AI 系统”。 
谷歌、英伟达等科技巨头纷纷布局，上个月亚马逊还成立了一个专注于 Agentic AI 
的新部门，初创公司们也争相推出各类“Agent”产品。开源社区也不甘示弱，从 Lang-
Graph 到 Agent SDK、AutoGen、CrewAI，一波 Agent 框架竞相登场，掀起了继大模型
作者 Tina 
 
16 
特别专题 | Agentic 软件革命 
之后的第二轮工具潮。 
连大厂掌门人也开始在公开场合“互呛”。 
微软 CEO Satya Nadella 高调宣称：“我们所知的 SaaS 时代即将结束……Agent 将
成为核心驱动力”。而 Salesforce CEO Marc Beinoff 则直接嘲讽微软的 Copilot，称其为
“Clippy 2.0”：“根本不起作用，而且没有任何准确性”。Clippy（回形针）即 Office 
虚拟助手，是微软上世纪推出的基于规则的代理，为用户吐槽最多的失败设计之一。 
视频地址：https://www.infoq.cn/article/6O1HCovhE2gA9kpwdiCe 
言辞之锋利，背后其实是对 Agentic AI 两种截然不同落地路径的分歧。 
一条是微软 Nadella 倡议的“面向全平台的智能代理框架”路线。按照他们的设想，
未来将出现一个 AI 操作系统，能够调度多个智能体，并且这些智能体可以在整个企业
内无缝地传递任务、消息和知识。 
Nadella 认为，这是一场从“App Stack”到“Agent Stack”的根本性变革。过去，我
们依赖前端 UI 驱动的应用形态，每一个业务场景都被拆分为独立的 App，用户通过操
作完成任务。未来，主导者将是 Agent，它能感知用户意图，基于数据、模型和推理链
条，完成决策和自动执行。 
在这种架构转变下，当前的 SaaS 等应用因为其本质上是嵌入商业逻辑的数据库，
未来这些逻辑会被 Agent 接管，由 Agent 去做增删改查，在多个数据库之间工作，所
有的逻辑都会转移到 AI 层。而一旦 AI 层成为主导，背后的数据库最终也会开始被替
代。 
本质上，这是微软构建通用人工智能代理体系、通过底层架构向企业应用生态渗透
的战略布局。正如数势科技 AI 负责人李飞博士在分析中指出，微软此举旨在依托其既
有生态系统优势，打造覆盖全场景的 AI 接入平台，形成对垂直领域应用的聚合效应。
通过构建人工智能交互的核心枢纽，微软试图确立其在产业智能化转型中的顶层平台地
位，实现对各类专业化应用的系统性整合。 
大模型公司或云资源提供商大多支持这种“通用”入口性质的路线。比如 OpenAI 
 
17 
InfoQ 架构师2025年第一季 
就肯定倾向于 Nadella 的思路，因为在它看来，所有的 Agent 本质上都是对其大模型
能力的延伸和增强。对 OpenAI 来说，构建通用 Agent 能将所有应用集成在自己的能
力框架之下，使其成为一个统一入口。类似的例子还有 Manus，以及 AutoGLM 沉思等。
无论是微软、OpenAI，还是 Manus、沉思，这背后体现的，依然是一场关于“谁来掌握 
AI 入口权”的竞争。 
与之相对，另一条路线则由 Salesforce 所代表。 
Salesforce 的思路是从现有企业软件栈出发，强调以垂直领域（如 CRM）为根基，
推动业务逻辑的 AI 原生重构。他们不认为 AI 会“取代”现有 SaaS 应用，而是主张 
AI 和 SaaS 深度集成，将 Agent 机制嵌入业务流程中，通过业务数据和流程去驱动 
Agent 的运行和决策。Agent 不是一个外部工具或统一入口，而只是整个流程当中的一
个节点。 
相较于微软自底而上的通用化策略，Salesforce 的设计思路是自顶向下，从实际的
业务流程出发，反向构建 Agentic AI 能力，旨在复用并增强现有的各类 SaaS 应用。 
事实上，许多没有庞大通用平台基础、专注于 ToB 软件或垂直场景的企业，可能
会更倾向于 Salesforce 这种贴近业务、务实的路线。他们往往是先搭建好业务或工作流
程，然后将 Agent 融入这些流程中，使其专注于解决特定垂直场景的问题。 
哪种路径更具可行性？ 
贴补丁式 Agentic，能落地但改不了旧系统的命 
Salesforce 的 CEO 对微软 Copilot 的攻击颇为犀利，甚至直言这是“微软的一场巨
大灾难”。他公开表示，自己与多位微软客户交流后发现，“他们并没有发现自己因为
这项  Copilot 技术而发生了改变，很多人几乎没怎么使用这个功能”。相比之下，
Salesforce 则宣称其平台每周处理多达两万亿笔企业 AI 交易。 
有意思的是，Salesforce 自家的平台最初也叫 Einstein Copilot。更耐人寻味的是，
在弃用“Copilot”这个名称后，Benioff 便开始公开批评微软的  Copilot 产品。并且 
Salesforce 坚称，从 Einstein Copilot 更名为 Agentforce，绝不仅仅是换了一个名字，而
 
18 
特别专题 | Agentic 软件革命 
是其底层架构也经历了重要的调整。 
从 Salesforce 架构师的公开演讲来看，Einstein Copilot 在与用户进行交互时使用的
是“Chain of Thought”（思维链）模式，而 Agentforce 的一个关键进展，就是用 React 
prompting 替代了效果不好的传统的思维链。其次是引入 Topic 分类机制，解决 Copilot 
在对话过程中难以将任务限定在某个特定的范围内的问题。然后是改进了 LLM 的响应
方式，Copilot 直接返回未经处理的后端数据，导致用户难以理解，而 Agentforce 则会
利用 LLM 对原始数据进行自然语言包装，使其更易读懂。最后是引入了主动触发能力，
与 Copilot 依赖用户主动发起对话不同，Agentforce 希望实现基于特定数据库操作的自
动触发，无需用户干预即可启动 Agent 执行任务。 
可以说，早期的 Einstein Copilot 侧重于通过对话触发和调用预设的比如销量预测等
算法，实现人机协作。而 Agentforce 的目标似乎是转向更自主的任务执行模式，用户
设定目标后，系统能够自主规划并执行工作流程，最终呈现结果。 
因此，尽管 Salesforce 强调这是架构的演进，但其核心仍然是提升 AI 在任务执行
中的自主性和智能化水平。 
不仅如此，Beinoff 还评价微软是“快速追随者”，并称：“他们肯定会像往常一
样想抄袭我们，向我们靠拢。但我们现在已经有数千客户在实际使用我们的产品。” 
根据截至 3 月的最新数据显示，双方的 Agent 都有大规模的落地，Salesforce 宣
称拥有约 15 万名客户，微软则表示已有 16 万客户，基于 Copilot 打造出了 40 万个 
Agent。 
然而，白鲸开源 CEO 郭炜一针见血地指出，当前微软的 Agent 只是个人用户辅助
工具，和 Nadella 技术路线里要实现的最终理想，并不是同一个东西。 
微软对 Agentic AI 的设想是“OS”级别的变革。它构想中的 AI 操作系统不仅要能
够调度多个智能体，还要持续保持上下文状态、理解用户意图，并在多种数据源和系统
能力之间进行协调。在这一架构中，业务逻辑将由 Agent 全面接管，而“状态管理器”
（State Manager）被视为 AI 操作系统的核心组件——只有具备对用户状态的持续记忆，
Agent 才能真正理解用户是谁、在做什么、希望实现什么目标，类似于 AI 世界中的
 
19 
InfoQ 架构师2025年第一季 
“内存管理”。为此，微软正在打通自身所有产品线，并与 OpenAI 这样的企业合作，
构建一个开放的智能体生态平台。 
而 Salesforce 的 Agentforce，“它都不是行业智能体，只是 CRM 生态线的智能体。
目前技术手段都实现了个高级版本 RPA/按键精灵，只不过通过大语言模型交互可以自
动做一些分拆动作，做一些组合罢了。和当年移动互联网出来时候的塞班操作系统一样，
属于早期‘婴儿’版本的 Agentic AI。” 
当前 Salesforce 模式虽能被快速推广落地，但是这件事情的门槛也不高，很多创业
公司都可以做到类似的事情，大公司就更不用说了。 
这些做法本质上仍停留在对原有软件和交互流程的增强层面，就像当年微软试图将 
Windows 系统移植到手机上一样——看似容易实现，但并不是最优解。真正开启移动互
联网时代的，是 iPhone 和 iOS 的诞生，它们从底层重新定义了系统与用户之间的交互
逻辑。 
正因如此，Nadella 才会提出“所有软件和 SaaS 都将被重构”的观点，因为他预
见到了 Agentic AI 时代的来临。如果我们只是把 Agent 嵌入到现有软件中去打补丁，
就像是把 Windows 装在手机上——表面上可行，但远未触及这场变革的真正核心。 
真正的 Agentic AI 将倒逼整个技术体系重建 
短期来看，复用现有 SaaS API 或许是 Agentic AI 实现功能落地的捷径，对现有流
程体系的冲击也相对较小。然而，从长期视角出发，Nadella 所代表的观点显然更具颠
覆性：大模型和 Agent 的到来会彻底重塑整个软件和 SaaS 生态。 
郭炜认为，甚至 Nadella 所描述的“代理程序在多个数据库之间工作”的设想，还
不够 Agentic AI。因为在这一新范式下，整体的技术框架都会要被重新设计。传统数据
库的设计初衷，是为了服务于“人类决策”的时代。从数据库到数据仓库，所有的设计
都是上个世纪，为了辅助人类做决策、完成信息记录而构建的。于是我们看到了复杂的 
CRUD 操作、DAO 层、数据库层、数据仓库层以及 ETL 流程，它们共同支撑起了人与
数据之间的交互逻辑。 
然而，在 Agent 成为主要用户的背景下，这些为“人类使用”而生的复杂数据处
 
20 
特别专题 | Agentic 软件革命 
理体系是否仍然必要，正面临质疑。未来的存储不一定是数据库形式了，使用的方式也
必然不是 SQL 了。 
顺着 Satya 的逻辑推演，被重塑的不只是 SaaS，更是整个技术架构生态。数据库
技术，甚至是基础理论会被倒逼升级，传统的  No-SQL，New-SQL 都不适用于未来 
Agentic AI 的场景。就像从软件工程学角度来看，所有的软件设计都是从  Use Case
（UML）图开始设计的，而 User 都变了，凭什么底层的技术还是原来的技术体系呢？ 
AWS Agentic AI 主任科学家章毅也赞同整个技术架构生态体系会被重构，他进一步
指出，当前 Agent 利用 API 调用已有的数据库系统进行知识查询和整合（典型 RAG 
架构）能够覆盖一些基本的业务逻辑，但是知识、信息碎片化（以不同模态和格式存储
于多个数据库中）对 AI Agent 高效搜索、链接、归纳、提炼、使用与更新信息带来很
大的困难。这必然会带动对数据存储、管理系统进行重新思考和设计。 
而且，作为 AI 操作系统的核心的记忆层“能否实现”也是未知数。 
Salesforce 的 Agentforce 架构在预设的业务流程中，Agent 可以有效地访问和利用
先前操作产生的数据和状态信息，形成一种上下文记忆。但一旦进入微软所设想的跨系
统、跨复杂场景环境，Agent 就没有一个稳定的获取记忆的依托点，出错概率显著增加，
显然比 Salesforce 这种嵌入式设计的难度系数大。 
微软设想的“AI 操作系统”中，状态管理的关键在于让 AI 能够跨不同的应用和设
备记住信息，并需要有一种结合短期交互与长期语义记忆的机制。这实际上是将来 
Agentic AI 时代替代数据库地位的关键组件，然而在实际架构层面，挑战远超预期。也
有观点认为这并不是一个简单的“状态管理”，而是是在云端为每个用户构建一个“虚
拟人生”——系统需要持续记录用户的行为、偏好与选择，形成大模型的短期记忆和长
期语义。问题在于，这种能力无法用向量将它描述清楚，并且目前还不知道可以用什么
技术实现。 
可以说，微软的 Agentic AI 代表了一种技术理想主义。 
虽然很多厂商在尝试，但这个模式整体还太早，大模型和 Agent 本身都还没有进
化完全。当前阶段，更现实的路径是 Salesforce 式 Agent，其类 RPA 的特性使其更容
 
21 
InfoQ 架构师2025年第一季 
易被企业接受。毕竟，在当下，企业更愿意为确定性买单。 
但长期来看，传统企业软件终会被取代，因为软件只不过是规范流程，信息传递，
实现多人协同的一种方式，如果有更先进的更高效的管理和交流模式，旧有模式一定会
替换。今天的企业软件形态必然会被颠覆，这正如固定电话被移动设备取代、再被微信
等应用彻底重构通讯方式一样。Agentic AI 时代会是一个和移动互联网一样的全新的时
代。 
不过，这需要的是一次自底向上全套生态的改变，挑战不亚于当年的“云原生”转
型，甚至更甚。这种全模式的创新，或许只有在底层信息科学取得突破之后，才有真正
实现的可能。 
“MoE”是个折中选择 
微软式的通用 Agent 路线，听起来诱人，但在现实中目前还行不通。而在大模型
的最新演进中，已经出现越来越多转向 Mixture of experts（MoE）架构的趋势。 
MoE 背后的理念正是将任务拆分给更擅长的“专家”，而不是试图构建一个无所
不能的模型。 
如果在大模型里大家能接受 MoE 的思想，那么，同样的道理，在实现 Agent 的时
候，也一样可以采用这种方式：不是打造一个“超级大脑”，而是强调分工协作、各尽
其职。 
而且，当前现实来看，因为 Agent 能力太弱，还是多个专科 Agent 来做比较合适，
例如 Salesforce 的 Agent、SAP 的 Agent 等等。 
就算未来的 Agent 即使是基于一个“超级大脑”的路线，这个“大脑”中也必然
存在多个并行的、具有不同专属能力的组件，而不是靠一个模型包打天下。事实上，哪
怕使用少数甚至是同一个基础模型，构建多个专家 Agent 并通过高效协同完成复杂任
务，无疑是一个有效的设计范式，是同大模型能力提升演化协同共进、互补互助的方法。 
更重要的是，所谓“通用”与“垂直”的两条路线，从产业实践来看，并非互斥，
而是根据不同企业的战略优势进行差异化布局。微软聚焦平台型能力，将 Agent 集中
 
22 
特别专题 | Agentic 软件革命 
于 Copilot 品牌下统一推进，同时也不放弃在 Microsoft 365 等具体场景中的垂直落地。
Salesforce 以 SaaS 为核心，在 CRM 等场景深耕垂直 Agent，同时借助 Agentforce 等
平台工具，向多云生态拓展。平台化和垂直场景的结合，正成为行业主流趋势。 
打平台牌需要非常大量的固有成本。对于中小企业而言、迅速找到垂直 Agent 应
用并落地，会是更重要的当务之急。 
从“能不能做”到“做得成、做得起” 
回顾 Agentic AI 近两年多的发展，我们看到，行业正经历着从对大语言模型的初步
探索，到逐步赋予其感知、理解和行动能力的关键阶段。 
从最初直接依赖 LLM 进行信息处理，逐步演进到利用检索增强生成（RAG）技术
来扩展其知识覆盖范围。随后，多模态模型的出现使其能够处理更丰富的输入和输出形
式。 
紧接着，行业进入了“Agentic 对话”阶段，核心在于赋予模型执行动作的能力，
使其不再仅仅是内容生成工具。为模型添加可执行功能被视为一个重要的里程碑。 
关于  Agentic AI 最早的雏形，存在两种主要观点：一种认为是  OpenAI 推出的
“Function Call”功能，它使得大模型能够根据需求调用外部函数。另一种观点则认为，
真正的更早出现的 AutoGPT，因为 Function Call 并不是一个完整的链路。AutoGPT 是
在 OpenAI 的 ChatGPT (GPT-3.5) 发布后，由 Toran Bruce Richards（苏格兰爱丁堡人工智
能公司 Significant Gravitas Ltd. 的创始人兼首席开发者）在 2023 年 3 月份创建的演示
项目，展示了如何利用大型模型实现任务的自动分解和执行。 
此后，Agent 的完整生态闭环首次被较为系统地提出，这要归功于 OpenAI 研究员 
Lilian Weng 在 X（原 Twitter）上分享的 Agent 工作流程文章，清晰地描绘了包括“目
标规划、工具调用、执行、结果反思”等关键环节，被认为是 Agent 架构逐步演进的
重要起点。受此启发，业界围绕 AI Agent 展开了大量的探索与实践，并逐渐形成了更
为系统化的工程方法。 
 
23 
InfoQ 架构师2025年第一季 
 
 
数势科技的 Agent 系统演进，也反映了 Agent 技术自身的发展历程。其系统经历
了从 1.0 到 3.0 的迭代：初期的 1.0 阶段，主要实现了单个 Agent 对数据工具的调用。
进入 2.0 阶段，系统引入了与环境交互的反馈机制，并强化了自我反思与迭代能力。发
展到 3.0 阶段，则根据用户需求，从单 Agent 升级为多 Agent 协作机制，以覆盖更广
泛的场景和角色。 
Agentic AI 还有哪些挑战？ 
当前，Agentic AI 系统构建中，任务编排依然是极具挑战的技术难题。 
相较于 Salesforce 等平台采用的预定义工作流模式（其泛化性受限），微软提出了
通过 AI 实时理解规则并动态生成编排规则，进而连接现有 API 的设想。但真正落地时，
问题远比想象中复杂。 
 
24 
特别专题 | Agentic 软件革命 
当业务流程仅涉及少量节点时，编排复杂度尚在可控范围内。然而当节点数量增长
至 10 个、20 个甚至 100 个量级，且涉及多系统、API 和工具的协同工作时，确定正
确的线性执行顺序（如 ABCDEFG...或 ACBDEFG...）将面临组合爆炸问题——100 个节点
的全排列组合达 100!种。每新增一个节点，系统复杂度将以阶乘级数倍增，这种非线
性增长特性使得大规模编排系统极易产生级联错误。正如李飞博士指出的：“当编排网
络中出现单个节点顺序错位时，整个拓扑结构的执行逻辑都将发生系统性偏差。” 
基于实际构建 Agent 的经验，合理的工作流编排是核心难点。一旦工作流得以确
定，后续 API 的输入输出参数便相对固定。因此，能否实现准确高效的工作流编排，
直接关乎 Agentic AI 系统的可靠性。 
尽管如此，Agentic AI 根本性的进步仍然依赖于底层大模型能力的提升。“大模型
基座本身有错误率，Agent 和软件其实是在给大模型的错误率或者幻觉去做兜底的。”
因此无论上层应用多么创新，如果底层模型能力没有显著进步，系统所能解决的问题依
然有限。 
如今的大模型在各种基准测试中表现越来越好，但要真正实现具备自主性的 Agent 
系统，还存在不小的挑战。Google 首席科学家 Jeff Dean 刚好在 4 月底的一次演讲中
透露了目前大模型实际能力：当前模型大致能以 60% 到 70% 的准确率，完成三到五
步的小任务，能在有限范围内调用工具处理一些简单请求。但真正理想的智能体，应当
能够面对模糊而复杂的目标，自主拆解并完成上千个步骤，完成相当于一个月人类工作
量的任务，且准确率达到 95%。 
从现在的能力到这一目标之间，仍有巨大差距。这一演进过程可能会经历多个阶段
——从胜任 3～5 步的任务，提升到能够稳定完成 10 步以上的流程，最终迈向真正具
备规划、执行、反馈能力的智能体系统。 
章毅则进一步指出，目前的前沿大模型中能够全面达到打造具有高智能、高自主性 
Agent 要求的选择并不多、甚至可以说非常少。 
即便是在大模型服务平台如  AWS Bedrock 提供了多个主流模型（如  Claude、
LLaMA、Amazon Nova、Mistral Mixtral 等等）供开发者选择的背景下，真正能够满足构
造 Agent 能力要求的模型还是主要局限于 Anthropic Claude 的几个最新版本（Sonnet 
 
25 
InfoQ 架构师2025年第一季 
3.5/3.7 或者 Haiku 3.0/3.5）。 
从短期落地的可行性角度看，聚焦一个或少数几个前沿大模型会得到比较好的投资
回报率。 
“每次对话 2 美元” 
“节省数百万美元”“一半客服被 AI 替代”——这类故事，已经成为 AI 圈标配
的营销剧本。Salesforce 和微软正是靠这样的案例，推广他们的智能 Agent 系统。 
 
这些故事构建了一个极具吸引力的愿景：AI 代理系统不仅能显著提升效率，还能直
接压缩运营成本，甚至重塑组织结构。但真正落地到企业环境中，很多人才发现，这场
变革的代价远比想象中高。 
以 Salesforce 的 Agentforce 为例，其官网宣传“每次对话 2 美元起”。但在实际
合同中，超出 1000 次免费额度的部分，会按每次 2.50 美元收费。也就是说，虽然预
购有折扣，但一旦使用超额，就需要按每次 2.50 美元支付。相比微软 Copilot 那种更
强调“按有效消息计费、结果导向”的模式，Salesforce 更像是“按次数定额收钱”，
不管任务是否真正成功完成。 
 
26 
特别专题 | Agentic 软件革命 
 
如果按当前国内大模型的标准报价（如 0.0008 元 / 千 token）折算，这“每次 2 
美元”的对话，意味着系统可能要消耗接近 1800 万 token。这不仅意味着成本高昂，
也从侧面反映出任务流程可能执行起来极其复杂。一旦规模化使用，成本压力会骤然放
大：以一家中型企业为例，1000 名员工、每人每天调用 10 次，每天光软件费就超过 
2 万美元，月支出高达 60 万美元。有从业者直言：“比传统软件贵多了。” 
对于像 Dow 这样有能力重构系统流程的大企业而言，AI Agent 可能真能创造“几
百万美元的节省”；但对大多数公司来说，这种奇迹并不容易复刻。节省成本的路径，
远比那些营销故事复杂。 
这也引出了一个更现实的问题：企业到底该怎么上 Agent？ 
你的企业该怎么上 Agent？ 
对大多数企业来说，“要不要上 Agent”其实早就不是问题了。一旦基础系统和生
态完备，未来 Agentic AI 就像移动互联网一样蓬勃发展。技术领袖的共识也正在浮现：
AWS 等云厂商认为 AI Agent 将会重新定义云服务的未来，明确将 Agent 定位为“下一
代基础设施”，提供模型托管、推理算力以及致力开发针对 AI Agent 的可重用组件和
功能模块。 
但越是全行业热捧的趋势，越需要冷静评估其落地的现实条件。 
“不是十倍提效，就不值得上。”技术部署要看“性价比”。李飞直言：“如果不
是十倍时效，这个场景的意义其实不大”。也就是说，如果原来 20 小时能完成一个任
务，现在 Agent 缩短到 10 小时，效率虽提升了一倍，但不足以支撑 AI Agent 高昂的
 
27 
InfoQ 架构师2025年第一季 
部署成本，其实企业很可能不买账。特别是在企业端，Agent 本质上是对业务流程的自
动编排，必须依赖业务与技术的深度协同：业务人员提供流程知识，技术人员将这种 
Know-how 嵌入 Agent 开发中，才能让 Agent 真正融入企业工作流。 
“通用 Agent，不适合企业。”目前一些企业已经准备好了部署 Agent，但“通用 
Agent”更适合 C 端用户。因为 C 端的容错性更高，“这个任务不行就换下一个”，
但企业不能容忍任务中断或出错，一旦流程关键环节失败，就可能影响整个业务判断。
“在企业当中，大部分工作都是严谨的，你这个任务给我执行错了或者执行不出来，我
就认为你这个软件不行。” 
因此，企业更倾向从垂直场景做起，把 Agent 深度嵌入到已有业务流程中，通过
产品设计和流程兜底，把错误率降到最低，才能真正达成企业级可用。“垂直 Agent 
在今年其实已经准备好了采购”，但不同领域成熟度差异较大，最终落地效果也存在显
著差异。同时还应优先选择容错性相对较高的任务切入，比如在“研报分析”这类允许
人工校验和干预的内容生成任务，避免一上来就部署端到端决策系统，“Human-in-the-
loop”机制可以显著降低幻觉和错误的风险。 
“不要急着全部用 Agent。”“现在的 Agent 就像当年的 Windows Phone。”郭
炜给出了一个务实的策略：不要过早的迷信于现在的 agent，先采用 RPA+Agent 模式来
实现企业内部流程自动化，等待外部技术条件成熟（10 年+），在开始考虑公司全面 
Agent 化。 
“不执着于单一的架构范式。”在全球范围内，亚马逊是最早系统化推动 Agentic 
AI 的企业之一。从 Alexa 的智能化重构、“Buy for Me” 融入购物体验，到 Amazon Q 
面向开发者的代码助理，再到电商目录处理、合规审核，或者服务于公司员工的 HR 信
息系统，Agent 技术已在其多个业务线并行推进。从这些项目中，可以看到亚马逊“全
面拥抱 AI agent 技术的明显趋势”。正如章毅所说：“公司各个业务部门都非常重视 
AI Agent 可能带来的机遇和挑战，所以有各类并行的、针对不同应用场景的尝试。” 
值得一提的是，亚马逊对 Agent 架构持开放态度，并不执着于单一标准化框架。AI 
agent 作为一个新兴的领域，技术架构仍然有很大的不稳定、不确定性，新生框架鳞次
栉比。有的框架诞生不到六个月，时间长的也不过只有两年多。在它们成熟、演化、归
一的过程中，亚马逊会对这些框架灵活应用。 
 
28 
特别专题 | Agentic 软件革命 
“Agent 是个日新月异的领域，保持灵活性而不迷信某一种架构，将会是更务实的
做法。”对于正处于数字化转型关键阶段的企业，建议从具体场景出发，快速构建 
Agent 原型并验证可行性。他提醒，尽管开发门槛已降低，但有效的评测机制仍是选型
与演进的关键。 
相比之下，中国企业在 Agentic AI 落地上正面临不同的挑战与机会。 
从落地模式来看，郭炜认为目前大多数中国企业更倾向于 Salesforce 式的“融合”
路径，而非微软式的“重构”：“我个人希望中国可以像新能源车一样，在 Agentic AI 
上通过重构实现弯道超车，颠覆传统软件。但是当前仍缺少一些基础信息学理论的突破，
因此融合模式更现实，也更容易落地。” 
不过，中国软件市场定制化需求多，因此生态不如海外 SaaS 软件标准，“哪怕是
用 Salesforce 模式做的 Agent，也需要千人千面的定制开发。整体来看，在中国没有出
现大型全球性软件企业生态之前，中国本土 Agent 还是在沙滩上的楼阁。先做好软件，
再说软件上的自动化。”总的来说，对国内开发者和企业来说，从 App Stack 到 Agent 
Stack 的转变，是一次前所未有的“弯道超车”的机会。 
写在最后 
AI 技术的演进史，总在狂热与反思的交织中螺旋上升。 
两年前，微软投入数十亿美元以及大量人力打造 Microsoft Copilot，宣称它将成为
“新一代的开始菜单”；与此同时，向量数据库 Pinecone 一举拿下 1 亿美元融资，VC 
们争相下注，仿佛“新数据库时代”已经到来。 
但现实很快泼下了一盆冷水。预装在 15 亿台 Windows 设备中的 Copilot，其月活
用户量却始终徘徊在 ChatGPT 的 5%，最终迫使微软放弃 Copilot 键的唯一性，允许用
户将其恢复为传统菜单键。而向量数据库也在短短半年内从“下一代数据库”沦为通用
功能模块，初创公司估值迅速回落。 
时间到了 2025 年，Agentic AI 站上新的风口。回望 Copilot 和向量数据库的发展，
我们不禁要问：这一次，那些看似前景光明的技术，能否真正落地生根，摆脱昙花一现
的命运，成为推动下一轮 AI 革命的关键引擎？ 
 
29 
InfoQ 架构师2025年第一季 
被骂惨的“现象级”Manus，今天我们来扒一
扒它的真实水平！ 
 
昨天，一款由中国团队发布的 Agent 产品 Manus 在 AI 圈迅速走红，并登上热搜，
许多人称其为真“打工人救星”。一段长达 4 分 17 秒的演示 demo 里，官方介绍，
与传统 AI 助手不同，这款产品是一个真正自主的 AI Agent，不仅能提供各行业领域的
建议或答案，还能直接交付完整的任务成果，写周报、做 PPT、简历筛选、甚至炒股票
都不在话下。 
在 Manus 官网，还能看到其一口气放出的 60 多个场景案例。此外，在 GAIA 基
准测试（专门评估通用 AI 助手解决真实世界问题能力的权威测试）中，Manus 在所有
三个难度级别上都取得新的最先进 (SOTA) 表现，成绩超越了 OpenAI 的 Deep Search。 
作者 华卫 
 
30 
特别专题 | Agentic 软件革命 
爆火后，Manus 官网页面一度崩溃。由于 Manus 目前还没有公开上线，但对外开
放了免费申请体验链接，AI 圈里掀起一波“全网求邀请码”的风潮。就邀请码一事，
Manus 官方回应称，“服务器资源完全是按照行业里发一个 demo 的水平来准备的，
根本不成想到会引起如此大的波澜。目前采取邀请码机制，是因为此刻服务器容量确实
有限，不得已而为之，团队也熬夜搞了一整天了。” 
国内某二手物品出售平台上，Manus 的邀请码标价最高至数十万级别。据了解，
Manus 的邀请码不会绑定到单个账号，拿到邀请码后所生成的项目也不会与邀请码绑定，
但设置了每日使用上限。也就是说，在使用上限内，一个邀请码可以被多人同时使用、
异地使用、轮流使用。 
 
 
31 
InfoQ 架构师2025年第一季 
Manus 怎么就火了？！ 
3 月 6 日下午，Manus AI 合伙人张涛在社交平台澄清道，他们从未开设任何付费
获取邀请码的渠道，也从未投入任何市场推广预算。内测期间系统容量有限，将优先保
障现有用户的核心体验，并逐步有序释放邀请码。 
值得注意的是，与此前 DeepSeek 先在海外“出圈”的状况不同，目前在海外各社
交平台上，还较少看到 AI 行业从业者们对 Manus 发表的公开评价。面壁智能联合创
始人、首席科学家、清华大学计算机系副教授刘知远昨日在清华大学的大模型公开课上
表示，DeepSeek 的热度当时是酝酿了一周才扩散开，而他不太理解为什么 Manus 会如
此迅速地爆火，表示“让子弹飞一会再看”。 
对于 Manus 迅速攀升的热度，数势科技 AI 负责人李飞向 InfoQ 表示，这背后主
要由两层市场趋势推动。 
首先，Deepseek 在国内大模型市场“烧”起来的火，让大家对于国内去做大模型
及其应用更有信心了，目光也会关注到其衍生品上。所以在 Deepseek 之后的这一波，
只要是和大模型相关的应用、做得还不错的，其实都会能够获得大量的关注。 
其次，“天下苦 AI 应用久矣”，从去年开始到今年，大家一直在关注和期待大模
型的应用前景。AI Agent 将迎来大规模爆发，在落地场景方面将重点会在数据分析、智
能客服等企业办公、业务领域。 
近期，热门的 Agent 产品不止 Manus 这一个。前不久，号称能顶一整个开发团队
的多智能体开发平台 MGX (MetaGPT X)，也在程序员圈子里小火了一把。就在刚刚，该
团队又在 GitHub 上发布了一个开源版的 Manus，名为 OpenManus，支持网页浏览、
文件操作、写代码等任务。据称，这一项目是几个 00 后工程师在三个小时内手搓完成
的。紧随其后，CAMEL-AI 今天一早也发布了一个用于多智能体协作的开源框架 OWL。 
OpenAI 昨日也宣布了一项关于 Agent 的通知，表示将对达到博士水平的 AI Agent 
每月收费 2 万美元（约合 14.5 万元人民币），主要面向企业用户的高端需求，尤其是
在金融、医疗、制造等数据密集型行业。 
似乎现在几乎所有 AI 赛道的公司都在“盯着”Agent，那么这些智能体产品的效果
 
32 
特别专题 | Agentic 软件革命 
和应用真有那么“神”吗？ 
是架构上的新突破，还是常规工程范式？ 
与此前爆火的 ChatGPT 或 DeepSeek 不同，Manus 目前并未对外披露技术细节。
据 Manus 团队的 Hyan 在 Superlinear Academy 社区平台上发帖介绍，Manus 是全球
第一款通用 Agent 产品，可以解决各类复杂多变的任务。其奉行这样的技术理念：
“我们坚信并践行 less structure more intelligence 的哲学：当你的数据足够优质、模型
足够强大、架构足够灵活、工程足够扎实，那么 computer use、deep research、coding 
agent 等概念就从产品特性变为了自然涌现的能力。” 
从公开信息已知的是，Manus 采用多智能体（Multiple Agent）架构，运行方式与此
前 Anthropic 发布的 Computer Use 类似，完全运行在独立虚拟机中，同时可以在虚拟
环境中调用各类工具。在这个架构中，每个智能体基于独立的语言模型或强化学习模型，
但 Manus 本身并未自研大模型。 
李飞对 InfoQ 表示，Manus 跟 OpenAI 的 Operator 有异曲同工之处，但是它可以
在虚拟环境里执行代码。换言之，Manus 的任务覆盖范围更多了，不仅可以在浏览器里
执行任务，也可以去到云端虚拟机里去执行任务。 
在技术层面上，李飞指出，目前从演示视频来看，尽管 Manus 覆盖的领域较广，
可操作空间大了，任务的泛化性自然也较高，但整体的架构和理念并不算新。虽然工程
实现难度是有的，但可能不是特别大。 Agent 本身是一个工程化架构的范式，Manus 
团队做得更多可能是如何去保证任务之间的连通性，比如任务的连接、串联和回退等方
面，保证系统的容错性。 
“通用”Agent 现阶段不可能实现？ 
邀请码虽然“难得”，但也有一批业内人士先行体验了 Manus 的效果，我们也收
到了一些用户反馈。某大厂的 AI 负责人对我们透露，“体验后感觉并没有被惊艳到。” 
根据网上试用者反馈，Manus 目前能顺利执行的任务偏简单（表现与目前前沿大模
型没有明显差异），对于稍微复杂的任务就需要耗费较长时间，甚至最后崩溃而无法完
 
33 
InfoQ 架构师2025年第一季 
成，这也引发了部分人的算力焦虑。而且，由于各平台的登录制度，Manus 无法完成大
家期待的“点外卖”、“订机票”等任务。 
商汤科技高级 AI 产品经理王尚则在试用过 Manus 后给出了比较正向的反馈，对
于其技术局限和可行性，他对 InfoQ 表示，最大的限制除了模型本身的能力边界外，目
前还缺乏一套通用的 Agent 协议或接口，让 Agent 具备更强的自主实现能力。Manus 
依赖于类似虚拟化浏览器的环境来执行各类任务，在浏览器的环境中模拟人类的操作，
使用为人设计的用户界面。但短期内看不到 Agent 协议出现的可能性，毕竟我们对大
模型的能力挖掘程度可能还不到 10%。 
至于当前 Manus 是否做到了通用 Agent 的级别，李飞审慎地表示“应该还不能”。
具体来说，Agent 底层的工具池越丰富，规划的能力越丰富，越会往通用去走，就像人
一样懂得越多越容易成为一个通才。Agent 想要达到通用，一定是能够去完成用户所提
到的所有任务，但是现实事件当中任务又会分为很多种，这里面有两个难点：第一是怎
么去根据用户不同的个性化请求去找到任务执行路径，第二是 Agent 所具备的工具池
是否足够丰富。 
“任务路径节点越多，复杂度就越高，端到端完成的的成功率就会陡降。”在放出
来的场景案例里，Manus 不管是交互、性能还是准确性都打磨出了不错的效果，其目前
肯定是往通用 Agent 的路线去走的，但做到通用 Agent 的难度是比较大的，因为物理
世界的复杂度远超我们的认知。 
总的来看，Manus 在实际应用中或会遇到三方面的核心可行性问题：一是物理世界
的高复杂度，二是任务流的连通性，再就是当前缺少通用 Agent 协议或接口。 
Manus 团队也在最新公告中表示：大家目前看到的 Manus 还是一个襁褓中的小婴
儿，像模型幻觉、交付物友好度、运行速度等方面都还有很大的提升空间。 
不过，李飞认为 Manus 带给市场的反应是正向的。他认为，“Manus 的爆火是让
我感到兴奋的，因为它让其实更多的人进一步地去了解什么是 Agent、 Agent 可以帮助
我们做什么的以及怎么去做。”同时，李飞指出，目前 Manus 走 To C 是一个比较好
的路径，可以通过 C 端先把市场热起来，而且 C 端用户对于工具的宽容度比较高的，
但 B 端会更为严格，不确定它的能力上限能否满足企业应用。 
 
34 
特别专题 | Agentic 软件革命 
但值得注意的是，哪怕作为“通用 Agent”，Manus 在大众中的使用门槛也是不低
的。 
据李飞介绍，在使用层面可能出现两种情况：领域专家不用它，因为当前通用 
Agent 还没有达到能够解决领域难题的程度；一般使用者不知道该怎么去用，就像我们
在去用搜索的时候，提问是一件很难的一件事情。 
对此，李飞提出，当前很多 Agent 还是被动式的，需要用户以提问形式告诉它怎
么做。但未来 Agent 产品一定会走向主动式，无需用户提问而是会根据用户的行为习
惯以及历史消费记录或出行记录，主动推荐或者告知用户怎么做，这种形态对于使用者
更为友好。 
垂直 Agent 的“全能”困境 
相较而言，MGX 则是一个侧重于编程开发领域的多智能体产品，与刚刚发布的开
源版 OpenManus 出自同一团队之手。据称，其可以模拟人类软件开发流程，通过多个
专业 AI agent 的协作，同时干团队领导、产品经理、架构师、工程师和数据分析师等角
色的活儿。该团队是开发了一个多 Agent 系统来处理复杂的问题解决任务，包括问题
重现、高效的代码生成和验证以及强大的补丁选择。这些 Agent 能够利用高级存储库
级代码理解、搜索、编辑和调试功能，处理各种软件工程子任务。 
根据官方介绍，MGX 是以 DeepWisdom 团队的开源多智能体编程框架 MetaGPT 
为基础，由  GPT-4o 和  Claude 3.5-Sonnet 驱动。利用多  Agent 架构，  MetaGPT 在 
SWE-Bench Lite 上实现了  46.67% 的解决率。作为多  Agent 框架，MetaGPT 可以为 
GPT 分配不同角色，以形成执行复杂任务的协作软件实体。也就是说，MetaGPT 想提
供一支全能团队，包括老板、产品经理、架构师、项目经理、工程师、测试，完整复现
一家软件公司的工作流程和标准操作流程（SOP）。 
这比做代码补全的 GitHub Copilot 和任务自动化的 Devin 更为全面，因为 MGX 不
仅想要独立完成整个项目的全生命周期管理，还想将“自主创立一家员工 100% 由 AI 
组成的公司”变得可能。然而，这一愿景面临极高的技术复杂度，要让 AI 理解并执行
软件领域复杂的业务逻辑，挑战不容小觑。 
 
35 
InfoQ 架构师2025年第一季 
JetBrains 中国 AI 解决方案专家孙涛对 InfoQ 表示，“直白的说，MGX 代替初、
中级研发团队或设计、支持团队尚不现实，但是对于一个人参与实现和运营的独立项目
或者对于新需求、新概念的验证，与多智能体协作的人机交互模式肯定会提升这些场景
下的效率。” 
此外，孙涛表示，虽然没有尝试过在大型项目上使用多智能体框架，但在简单项目
上堆叠功能，多尝试几轮后，能明显体会到 token 消耗速度飞升，生成的内容质量不如
最初几轮交互。多智能体之间相互沟通也容易将错误信息逐步放大，让最终结果远远偏
离最初需求；上下文遗忘更是一个很明显的问题，受限于模型能力，智能体之间多轮互
动后容易出现早期信息遗忘、消失，影响整体一致性。 
在亲身体验 MGX 过后，孙涛还透露道，“我自己感觉还有一个问题，就是生成的
项目文档、设计资料缺乏解释性，往往有很多人类难以理解的内容，更像是‘给机器看
的文档’。或许这是 LLM 生成的一个限制。” 
总的来说，MGX 现有的产品形态在完成明确定义的小型任务上表现超出预期，但
面对大型、复杂、模糊定义或者需求动态变化的任务时，仍有诸多问题。智能体无法在
既定的提示词内，处理复杂的原子化操作，人类面对复杂业务时的应对和学习能力，目
前阶段的 LLM 还很难做到。 
李飞则指出，“MetaGPT 强调的是协作，但其实又回到那个问题，涵盖的角色越多，
复杂度就会越高。”在他看来， 当前 MetaGPT 有具体企业级应用或者商业化落地其实
很难，实际的业务项目开发不仅是编程一个项目或游戏这么简单，其实是逐渐在走向业
务上层的，智能体具备业务逻辑的理解挺难的。但他也表示，“这确实是一个应用方向，
我们不能因为它当前的落地难度大就否定它。” 
另值得一提的是，目前 MGX 在官网展示的案例项目成本几乎都不超 1 美元。对
此孙涛表示，在实际商业化项目开发中不可能做到这样的低成本。根据他本人的体验经
历，在多轮操作后，软件中添加一个细微的需求，如添加一个新列表、修改一些样式等，
多智能体框架消耗的 token 量会成倍提升。放下 token 用量的问题不谈，这些由机器
生成的内容，在真正投入生产使用前，也需要人工再次审阅确认。 
 
 
36 
特别专题 | Agentic 软件革命 
将来是否会被大模型内化？ 
无论走通用 Agent 路线的 Manus 还是 MGX 等这类领域 Agent，其前提都是依托
其他家的大模型作为核心引擎。那如果只是套壳大模型的产物，是否会被其依赖的核心
技术“内化”掉？这些 Agent 产品本身还有独立存在价值吗？通用 Agent 在 LLM 的
演进过程中有多大生存空间？ 
王尚对此表示，所有开放性的解决方案，最后都有可能被大模型内化，只是一个时
间节点的问题。另外，大模型和 Agent 的边界也在逐渐模糊，Anthropic 据传也在计划
从模型服务商发展为应用服务商或者方案服务商。随着大模型不断内化更多的能力，其
对于 Agent 的吞并趋势也将越发显著。 
“现阶段我们应该关注的，是想清楚我们希望 Agent 帮我们解决什么样的具体问
题，从问题出发，去找答案、找路径、找空间。” 
李飞则指出了一系列大模型内化不掉的 Agent 落地场景：比如数据访问，大模型
可以去连接、收集互联网的数据，但很难处理企业内部的数据，大模型不可能去适配到
每个企业内部数据结构和数据库；很多工具能力目前来看也是大模型内化不了的，大模
型是应用的下限，Agent 才是应用的上限，只有连接足够多的业务场景才能够去打造一
个可用、好用的产品应用。 
他表示，“未来，我们需要考虑的是通用 Agent 的形态问题，即通用 Agent 是一
个独立的产品吗？用户需要很多入口吗、是否会做统一？”对此他认为，未来电脑和现
实交互的入口一定会逐渐地收敛和整合，那么在这种情况下通用 Agent 就不会是一个
完全独立的产品，可能会以 MCP 的整合方式，融入到人机交互当中的某一个节点。 
 
 
37 
InfoQ 架构师2025年第一季 
GPT-4o“吉卜力”爆火，Prompt、SD 白学
了？！大模型能力进化碾压一切 
 
ChatGPT 的新 AI 图像生成功能上线仅两天，社交媒体上便已充斥着以日本动画工
作室吉卜力风格的 AI 生成梗图，埃隆·马斯克、《指环王》和美国总统唐纳德·特朗
普都没“逃过”，甚至 OpenAI 首席执行官萨姆·奥尔特曼也将他的新头像设置为吉卜
力风格的图片。（吉卜力工作室以制作《龙猫》和《千与千寻》等热门影片而闻名。） 
大量用户正在将现有的图像上传到 ChatGPT，并要求聊天机器人以新的风格重新创
作这些图像。今天，奥尔特曼在 X 上发文表示：“看到大家如此喜爱 ChatGPT 的图像
功能非常有趣，但我们的 GPU 快扛不住了。”虽未具体说明限制次数，但 Altman 称
该措施不会持续太长时间，因为他们正在尝试提升处理海量请求的效率，免费用户将
作者 华卫、褚杏娟  策划 Tina 
 
38 
特别专题 | Agentic 软件革命 
“很快”能每天最多生成三张图像。 
 
虽然后续 OpenAI 又宣布了对 GPT-4o 进行了更新，但显然人们的注意力还在“玩
图”上。 
“我认为，这个功能是过去半年里 OpenAI 发布的 GPT-4o 中最有价值的一个，它
确实非常炸裂。相比之下，正式上线的 Sora 以及后来连续 12 天的直播所展示的内容，
大多都没有超出人们的预期。”原快手可图大模型负责人李岩说道。 
与 SD 等模型比，GPT-4o 赢在了哪里？ 
“昨天还在看 SD 教程，今天发现白看了……”知名开发者 Jimmy Cheung 发帖说
道，“今天情绪非常低落，压力非常大，我不清楚我现在做什么，是从现在开始到将来
都还有价值的。” 
 
 
39 
InfoQ 架构师2025年第一季 
李岩表示，这次 GPT-4o 火爆的关键在于实现了对话式图像生成。 
实际上，基于自然语言指令的图像编辑能力之前已经有了，比如字节 SeedEdit 和 
Google Gemini 2.0 都具备相似能力。但在实际生成过程中，指令响应能力没有那么强，
效果做得没有那么好。 
例如在一致性保持方面，当要求去除背景中的某个物体时，模型可能还去掉了其他
的东西；或者在对人物进行特定修改时，最终效果可能是不像原来的人了。此外，还存
在指令不响应的问题，比如要求添加某些元素时未能执行。 
但这次 GPT-4o 的交互方式所达到的文本跟图像的响应是非常精准的，大大超出了
大家的预期。 
李岩分析，虽然 OpenAI 没有发布详细的技术报告，但有一点非常明确：他们一定
采用了自回归框架（Autoregressive Model, AR），只有自回归框架才能实现如此自然的
图文交互效果。后续大概率也接入了 decode 模块后再做图像生成，但其整体框架已经
完全统一到了自回归框架之下。 
具体说来，Flux、Stable Diffusion 等模型，现在的做法都是将文本表征和图像生成
过程进行解耦，然后扩散模型出图。这种方式通常要先对文本进行完整表征，例如通过 
CLIP 或大语言模型提取特征，然后将该特征直接输入扩散模型，并要求扩散模型在生成
图像的整个过程中持续参考这个固定的文本特征。这个文本特征的来源是用户输入的 
prompt，某种编码器的方式会对 prompt 进行特征提取。 
然而问题在于：特征提取完成后，信息量就被固定了。在文本到图像的生成过程中，
100% 的原始信息都存在于用户输入的文本 prompt 中，但经过文本编码或表征提取后
可能只剩下 70% 的信息，这意味着后续最多就只能基于这 70% 的信息量进行图像生
成。 
当前几乎所有图像生成模型都采用了上述模式。但可以看出，这些模型在生成文本
表征时都会不可避免地造成信息损失，而这种损失一旦形成固定的 embedding 或表征
就无法挽回，这一阶段出现的信息缺失，后续扩散模型在生成图像时是无法回溯弥补的。 
当前，扩散模型的扩充方式是 prompt engineering（提示词工程）。但是，提示词
 
40 
特别专题 | Agentic 软件革命 
工程只能扩展成显式描述，比如输入“一个漂亮的小女孩”，系统会将其扩展为非常详
细的描述，包括小女孩戴着什么样的帽子、出现在什么样的背景下等等。但这种方式在
后续建模中仍然需要提取文本特征，依然会造成信息损失。只要是采用二阶段的方式，
即先建模文本再以文本为条件输入扩散模型，就必然会因为文本建模过程中的信息损失
导致最终生成的图像无法与文本描述 100% 对齐。 
GPT-4o 之所以强大，关键在于它能有效处理用户提供的简洁信息。例如，用户通
常只会简单地输入：“帮我画一只小猫或小狗”，但不会给出具体是什么样的猫或狗。
现在，GPT-4o 统一到大语言模型的自回归框架下，所以天然具备了语义泛化能力。这
种能力本质上源于模型本身的知识储备，使其能够准确理解用户简单文字背后代表的真
正的、稠密的信息量是什么。 
正是由于 GPT-4o 拥有强大的大语言模型作为知识基础，它才能在完整的端到端框
架中实现如此精准的理解和生成能力，这一点至关重要。模型输入的就是用户的原始 
prompt，然后直接出图，中间过程中没有二阶段损失，都是一阶段做的，可以充分利用
大语言模型所带来的隐式知识，包括扩充 prompt 等。 
另外一点是，原来的方法仅支持单轮操作，即输入文字生成提示词，再通过特征提
取生成图像，但无法支持多轮条件控制。 
GPT-4o 可以直接将图片按照上传图片的风格生成新图像，其中关键在于需要理解
上下文中的具体指向，如“刚才提到的狗的照片是哪一张”，这需要大语言模型具备跨
模态理解能力。在自回归框架下，上下文从纯文本扩展到了文本 + 图像，因此模型能
轻松 get 上下文，甚至远程的上下文。 
值得注意的是，从出图质量来看，目前基于自回归框架的生成效果并没有碾压式地
超过扩散模型，甚至可能还不如扩散模型的表现。现阶段，两者的生成质量水平其实相
差不大。 
李岩指出，这仅仅是就出图效果而言，我们更应该关注的是交互方式的差异。未来
在交互体验方面，自回归框架显然具有更大的理论优势，它能够更好地兼容完全开放的
自由度，实现更接近自然语言对话式的交互方式。 
 
41 
InfoQ 架构师2025年第一季 
“这种 Interleaved 的图文交错技术才是真正原生的多模态大模型。”李岩认为，
在当前行业中，真正意义上的原生全模态的大模型领域里，OpenAI 还是走在最靠前的。 
此外，李岩表示，“文生图架构没有什么可以争议的了，在 2025 年这个话题就不
是话题了。” 
自回归框架对于多模态里面的文本模态、音频模态，自不用多说，基本上已经证明
了是可行的，难点在于视觉模态。现在行业内最好的模型，包括开源的 Flux、闭源的可
灵、Sora 等，还在用 DIT 的架构，真正做到高精度的视觉生成现在还离不开扩散模型，
但图像生成领域，单靠自回归框架实际上是有可能达到一个新的高度的，这件事情 
GPT-4o 已经给出了答案。 
李岩还大胆设想，如果 GPT-4o 接入联网功能并整合 RAG 技术，其在图像生成方
面的潜力将更加巨大。通过 RAG 技术，模型可以直接检索到用户所指的网络流行梗或
热点，用户就不需要再上传参考图片了。例如，当用户想生成网络流行表情包时，GPT-
4o 可能无需参考图片，仅凭对网络流行文化的理解就能准确捕捉到用户想要的梗，这
将进一步提升文生图应用的便捷性和准确性。 
是否会吞噬所有产品？ 
OpenAI 发布 GPT-4o 文生图功能后，Jimmy Cheung 的评价是：GPT-4o 的图像能力，
直接干翻了之前很多创业公司的产品，他们花了那么多时间、人力、投资人的钱去调优
的算法、工作流、模型，直接被一次大模型的更新就取代了。 
除了 Jimmy Cheung 吐槽“SD 白学了”，还有网友感叹，学了两年的作图工具流 
comfyUI 也白学了。一部分人直接大呼：工作流已死。事实上，对于像 comfyUI 这样的
工作流产品而言，情况可能没有那么悲观。 
“GPT-4o 目前为止的结果确实挺颠覆，但在真正的商业化可用的能力上，现在不
太行，相当长一段时间还是要依赖 comfyUI。”李岩说道。 
比如，当前 GPT-4o 的出图大小并不能满足实际商拍场景里的需求，分辨率的提高
会需要一些外接能力。另外，OpenAI 在照片改换风格时是做全图的重绘，细化到了图
像的每一个像素点，但在实际情况中，用户可能只需要改某一块地方，其他地方，甚至
 
42 
特别专题 | Agentic 软件革命 
一个像素值都不能动，这样的需求就需要 comfyUI 这类非常细粒度的工作流方式去精
细化处理。 
comfyUI 里面有后处理、抠图、调整亮度等很多链路，支持使用基于图形、节点和
流程图的界面来设计和执行高级的稳定扩散流水线。 
 
“对于轻娱乐场景或者要求没有那么高的批量生产场景，GPT-4o 现在已经可以发
挥价值了。但对于容忍度比较低、项目要求非常高的场景，未来相当长一段时间里还是
要依赖 comfyUI。”李岩总结道。 
但是，GPT-4o 对于 Prompt 工程可能会是致命打击。 
“Prompt 工程这件事有可能以后变得也没那么重要了。”李岩解释称，现在 
Prompt 对文生图、文生视频模型很重要，是因为整个文本侧和图像侧还没有办法做到
那么强的 alignment 效果，所以需要尽可能把文本侧的内容写明确、减少信息损失，因
此诞生了 prompt engineer。但实际上未来这部分工作如果如果能统一到 GPT-4o 的框架
里，这份工作大家慢慢就不需要了。就算 Prompt 写的不好也没关系，还可以再改，只
需把不满意的改进点用自然语言描述给模型，模型就会理解到底应该怎么改。 
在李岩看来，GPT-4o 这次更加证明了工具型产品会更容易被大模型能力吞噬。比
如美颜类工具，对于不懂美颜的男生来说，语言交互就可以得到理想的效果。 
 
43 
InfoQ 架构师2025年第一季 
但显然，作为正在遭受“冲击”的 Midjourney 并不这样想。Midjourney CEO David 
Holz 犀利指出：GPT-4o 的图像生成速度慢、效果又差，OpenAI 只是为了筹集资金，而
且在以一种不良竞争的方式行事。这不过是一时的噱头，并非创作工具，不出一周就没
人会再谈论它了。 
 
据称，Midjourney 准备在下周推出最新的 V7 版。值得注意的是，领导 Midjour-
ney V2 至 V7 模型开发的核心人物 theseriousadult 在 3 月 21 日宣布离职，之后将加
入 Cursor 转做 AI 编程 Agent。 
而早在 GPT-4o 掀起此次关于“大模型是否会吞噬所有产品”的热议之前，AI 科技
公司 Pleias 联合创始人 Alexander Doria 就提出了“模型就是产品”的观点。他明确指
出：所有投资者一直都在押注应用层，但在人工智能进化的下一阶段，应用层很可能是
最先被自动化和颠覆的。同时，Doria 还认为，OpenAI 的 DeepResearch 和 Claude Son-
net 3.7，以及“不仅把模型当作产品，而且将其视为通用基础设施层”的 DeepSeek，都
是“模型作为产品”的典型示例。 
不过，就目前的大模型能力来看，大模型暂不能覆盖到所有的应用产品。但这种低
门槛的使用形式，似乎正一步步瓦解许多现有的各类产品逻辑和形态。 
结束语 
大模型更多是在做技术平权这件事，就是让很多不懂技术的人逐渐都可以公平地使
 
44 
特别专题 | Agentic 软件革命 
用大模型。在技术迅速变化的当下，每个人，甚至企业都很容易被迫进行战略调整，甚
至转向。 
李岩的建议是，首先，要明确自己在这个行业中的具体业务需求。其次，在实际工
作中，每个人都应该采取两种策略：一是“低头走路”，确保自己对所用工具的理解和
运用熟练，从而稳步前进；二是“抬头看路”，关注行业的发展和变化。这两者不是相
互排斥，而是需要同时进行，以便我们在专注工作的同时，及时调整方向。 
李岩认为，未来大模型的发展将深刻影响各行业的组织形态和人员能力结构。以传
统的人才金字塔为例，其结构通常分为底层、中腰部和顶层。目前看来，底层能力画像
的人会被大面积“吞噬”，接着是腰部能力的人群，而最头部的那部分人永远不会被大
模型吞噬，因为大模型本身也需要他们的 feedback 和教化。 
“所以，每一个人应该尽量避免做技术含量低的工作，而是慢慢往上去走。”李岩
说道。 
 
 
45 
InfoQ 架构师2025年第一季 
资源有限，如何构建高效能的 AI Agent 
 
在人工智能的璀璨星河中，AI Agent 无疑是一颗耀眼的明星。自诞生之日起，它便
承载着人类对自主决策和持续进化能力的追求，历经数次技术浪潮的洗礼，而今随着大
模型技术的突破再次站上了风口浪尖，成为业界瞩目的焦点。甚至不少专家认为，未来 
AaaS（Agent as a Service）模式或将颠覆现有的 MaaS（Model as a Service），成为主导 
AI 产业的新趋势。 
日前，在 AICon 全球人工智能开发与应用大会 2024 北京站 【AI Agent 技术突破
与应用】 专题圆桌交流中，小米大模型负责人栾剑担任主持人，与数势科技 AI 负责人
李飞、彩讯股份 AI 产研部总经理邹盼湘、钉钉智能化平台架构师柯杰，共同探讨 AI 
Agent 领域的最新进展和发展方向。 
作者 AICon 全球人工智能开发与应用大会 
策划 李忠良 
 
46 
特别专题 | Agentic 软件革命 
部分精彩观点如下： 
• 大模型的性能将会急剧提升。 
• 大模型 API 可能会促进国内 SaaS 模式的进一步发展。 
• 有效利用私域数据并精准描述场景任务，可以在小模型下实现低成本、高效推理。 
• 大模型技术并非万能，但通过合理拆解问题，就能在可行的范围内解决问题。 
• 改进 AI 意图识别是提升人机交互体验的重要方面。 
以下内容基于现场速记整理，经 InfoQ 删减。 
栾剑：如何挑选和判断适合使用 AI Agent 赋能的场景？ 
邹盼湘：在选择场景时，我们主要从两个方面考虑。首先，业务流程必须要清晰，
因为大模型的落地应用需要明确的业务流程。如果业务流程不清晰，模型的效果就难以
达到预期。其次，我们的场景中需要有一定的数据积累，无论是业务数据还是用户行为
数据。只有在这种数据积累的基础上，进行 AI 探索或初步落地，才是一个较为合适的
选择。 
李飞：在落地过程中，我们发现 Agent 主要用于工作流编排。简单场景不适合用 
Agent，因为任务本身简单，Agent 反而可能增加复杂性，客户等待时间过长。但对于复
杂场景，涉及多环节且环节顺序灵活，Agent 也许能通过大模型规划实现编排。因此，
没有一个固定答案，需根据场景找到合适的平衡点。复杂任务用预编排工作流，中等复
杂度任务可以用大模型规划。 
柯杰：我们可以从三个场景来讨论。首先是“AI+”，即将 AI 与现有业务流程结合。
这种方式的核心是连接当前大家已经熟悉的业务流程，让业务习惯得以保留。但实际上，
很多人对于这种转变的接受度仍然较低。 
其次，由于目前大模型技术还不够成熟，我们可以创建一些通用模板，在模板中替
换不同的参数来生成新的工作流。例如，某个工作流可能最终会将数据收集到一个多表
中，而不同的工作流只是替换了不同的多表参数。这种方法可以在一定程度上复用现有
工作流，提高效率。 
最后，我们的功能编排目前还是基于传统的工作流系统，这对开发框架来说仍然是
 
47 
InfoQ 架构师2025年第一季 
一个挑战，因此目前对开发人员的要求较高。在这种情况下，我们每个人都需要理解
“模型与产品匹配度”。我们需要清楚了解模型的能力和产品的需求，找到二者之间的
平衡点，明确哪些任务适合模型来做，哪些需要人工介入。 
栾剑：我个人在选择场景时，首先会考虑这个场景的商业价值。我们需要判断使用 
AI 后，是否能在降本增效等方面带来显著的商业价值。如果人工完成该任务已经非常高
效，而引入 AI 反而增加了成本，那么可能不值得替代。 
其次，要考虑技术能力。随着大模型的发展，它在自然语言理解和生成，以及视觉
理解和生成方面的能力有了显著提升。如果一个任务或场景涉及这些领域，大模型可能
会带来很大的收益，能够完成得更好。但对于一些大模型目前尚不擅长的任务，如复杂
推理或规划能力，我们需要更加谨慎地判断是否可行。 
第三，数据积累也是一个关键因素。通用大模型仅通过 Prompt 方式进行任务时，
效果会受到一定限制。我们通常希望有更多场景相关的真实数据来优化模型，因此，如
果场景内的数据积累较多，优化效果会更好。相反，如果数据积累不足，效果可能就会
受到限制。 
最后，还要做风险评估。需要考虑场景对可信赖度和准确度的要求，并评估用户使
用过程中是否会感到不适。在很多场景中，用户希望与人类进行交互，特别是客服场景
中，用户可能不愿意与 AI 客服对话，主要是因为之前的智能体验不好，或者他们更倾
向于与人互动。此外，还需要考虑法律和隐私风险。 
栾剑：虽然大模型的算力要求在不断降低，但与传统 AI、模板驱动的系统或小模
型相比，其服务成本仍然较高。这使得一些公司和行业在引入 AI Agent 时，面临着算
力、内存等资源上的挑战。在这种情况下，如何利用有限的资源来实现更高的应用价值，
并突破普通 Agent 的能力瓶颈？ 
柯杰：我之前看到面壁智能提到一个“面壁定律”，这与早期的摩尔定律相似。摩
尔定律讲的是 CPU 的计算能力每 18 个月翻一倍，而面壁定律则指出，大模型的知识
密度也会在短时间内提升，甚至不需要 18 个月。实际上，现在很多小模型已经能够在
手机上取得非常好的效果，我认为这个问题会很快得到解决。 
 
48 
特别专题 | Agentic 软件革命 
目前，很多大模型的潜力尚未完全挖掘出来。虽然大模型存在缺陷和短板，但从应
用开发的角度来看，大模型的能力已经足够应对大多数场景。就像电力一样，虽然电力
紧张，但对于大模型的应用来说，其所需的电力是足够的。我对未来很乐观，认为大模
型的性能将会急剧提升，并且未来许多小模型将能够在端侧解决更多问题。因此，我并
不感到焦虑，问题并不像看起来那么严重。 
李飞：突然想到或许还有一个“价格定律”，随着基础设施价格的降低，ToB 客户
越来越关注设备成本，特别是做私有化部署的客户。如果某个场景的 ROI 无法覆盖高
昂的设备成本，落地就变得困难。 
在国内，客户在采购软件时通常比较保守，偏向于私有化部署。这与国外市场不同，
国外最初也做私有化，但由于成本过高才转向 SaaS 模式。国内客户接受 SaaS 的速度
较慢，因为他们没有经历过私有化部署的转变。 
此外，很多客户坚持私有化是因为数据安全的考虑，但并非所有数据都需要完全私
有化，部分非敏感数据可以出库。关键在于评估哪些数据需要私有化，哪些可以外部处
理。 
随着大模型的发展，客户的观念也在转变，不再单纯要求私有化，而是考虑采用 
SaaS 模式，尤其是面对高算力成本时。大模型的 API 也可视为 SaaS 的一种形式，未
来可能会促进国内 SaaS 模式的进一步发展。 
邹盼湘：在中国，SaaS 的推广存在文化障碍。中国人倾向于购买能“看得见、摸
得着”的东西，而 SaaS 服务是虚拟的，可能在没有续费的情况下消失。因此，许多企
业在做立项时更愿意选择私有化部署，而非 SaaS。尤其在向集团汇报时，他们更看重
“实体化”资产，而非过程中能力的沉淀。 
此外，大模型在落地时仍面临挑战，尽管算力不断提升、价格下降，当前的大模型
效果还未达到预期。随着技术的不断发展，许多问题会逐步解决。短期内，我们需要补
充大模型的不足，特别是在性能、可控性和“幻觉”问题上。 
为了应对这些问题，我们常常减少大模型的处理量，使用小模型或传统方法来控制
成本并提高性能。即使大模型变得智能，它仍然无法解决私域数据和业务流程的问题。
 
49 
InfoQ 架构师2025年第一季 
私域数据随着时间变化不断积累，大模型无法实时获取并处理这些数据。同时，业务流
程和系统因公司不同而各异，模型无法完全取代这些差异化的系统。 
从长远来看，我们的目标是如何高效地将私域数据和业务流程链接起来。我们正在
开发一个名为 Aibox 的大模型应用平台，旨在解决大模型的不可控性和性能问题，并实
现多模态数据与业务系统的高效连接。 
栾剑：总结一下，资源有限的情况下，我们不必过于担心 AI 的应用可能出现问题。
首先，模型的能力在不断增强。两年前推出的 ChatGPT 是一个千亿参数的大模型，而
现在即便是一个 7B 模型，也能超越当时的效果。这表明模型的参数可以大幅压缩，依
然保持良好的性能。 
我们小米在端侧大模型的研究中，已验证这一趋势：在效果基本保持不变的情况下，
模型规模越来越小，服务成本越来越低。同时，随着更多优秀的工程师关注这一领域，
从硬件到软件都在加速推理的优化，我们看到大模型服务的价格最近一年显著下降，甚
至国内大模型服务的价格下调还带动了海外。 
最后，很多时候我们不必依赖庞大的模型才能取得良好效果。通过有效利用私域数
据并精准描述场景任务，许多应用可以在小模型下实现低成本、高效推理。 
观众：我是一名 TCL 初级 Agent 开发者，企业中很多人认为 AI Agent 无所不能，
可以一键控制很多东西，但真正落地完发现它其实碌碌无为。应该怎么应对这种大模型
落地之后的差距问题？ 
邹盼湘：就像在与客户沟通时，我们发现客户对 AI 的理解往往存在误区，以为 AI 
无所不能。例如，在某个客户开发的营销助手项目中，客户最初只希望推荐饮品，但很
快提出了更多需求，包括根据天气推荐饮品、提供天气查询、推荐周边餐厅等。 
虽然这些问题超出了我们最初的预期，但客户认为这些都属于“助手”应答的范畴，
因此我们逐步为系统增加了天气插件、定位功能和商户信息。随着需求不断升级，客户
还提出了关于运营活动和折扣券的推荐，这些问题更复杂，但客户依然认为它们是知识
问答的一部分。 
为避免类似问题重复出现，我们在后续项目中将流程进行了详细拆解，明确哪些问
 
50 
特别专题 | Agentic 软件革命 
题由模型解决，哪些需要提示工程、模型微调或知识库对接。我们要求客户在提需求时
明确功能边界，并清楚了解每项需求的预期效果，避免模糊需求导致项目实施困难。同
时，我们也提前向客户说明可能遇到的挑战，如第三方 API 对接问题，并提供应对措
施。通过这种方式，我们有效地管理了客户期望，确保项目顺利进行。 
李飞：正确地管理预期，尤其是在交付过程中，是至关重要的。对于老板的预期，
也需要提前框定合理的范围并逐步满足。有时候，老板的要求可能是愿景，虽然无法完
全实现，但我们需要理解并努力朝这个方向前进。 
柯杰：当老板愿意一步一步分析问题时，复杂的挑战也能被逐步解决。例如，提升
人效看似难以回答，但如果将问题具体化，分析员工时间消耗，找出可以优化的部分，
这就变得可行。同理，虽然当前的大模型技术并非万能，但通过合理拆解问题，我们能
在可行的范围内解决问题。 
其次，我们不可能直接拒绝老板的需求。我们需要告诉老板哪些问题是可以解决的，
哪些是目前无法实现的。一个简单的判断标准是，哪些是人类能做到的，哪些是人类做
不到的。如果人类做不到的事情，大模型也很难做到；反之，如果人类能做到的，我们
就可以努力去实现。 
例如，在阿里园区，有个功能可以通过语音控制开关灯，这看似简单，但实际上需
要先进行数字化建设，将工位和灯具建立关联。这个过程虽然需要时间，但通过数字化
积累后，就能实现这种控制。 
栾剑：在引入大模型之前，我们需要进行一场“启蒙运动”，将员工和老板的热情
调动起来，让大家理解并拥抱 AI 技术。这样才能为未来的技术变革奠定基础。然而，
一旦大模型开始应用，则需要进行“反启蒙运动”，告知大家 AI 目前的能力范围，设
定合理预期。 
此外，大众对 AI 能力的理解有很大误区。普通人觉得人类很容易做到的事情 AI 
就应该能做，人类很难做到的事情对 AI 应该也很难。但事实上，真不一定。AI 能在一
些人类难以完成的任务上表现出色。举个例子，打乱扑克牌后，人类很难记住牌的顺序，
但 AI 可以轻松记住甚至多副扑克牌的顺序。类似地，AI 能够处理极大数量的上下文信
息，这在人类中是做不到的。但另一方面，一些对人类很简单的推理对 AI 可能很难。 
 
51 
InfoQ 架构师2025年第一季 
因此，我们需要通过好的类比向公众，尤其是老板，解释 AI 的能力及其局限性。
这样，他们才能更好地理解 AI 能做什么，不能做什么，背后的原因是什么。 
栾剑：在未来，最理想或者说最终极的情况下，人和 AI Agent 会以什么样的形态
进行互动？人和 AI 会是什么样的协同方式？ 
邹盼湘：一个智能体应该像是我们的助理或伙伴。人类交流可以分为两类问题：事
实类问题（如“现在是什么时候？”）和认知类问题（如“你怎么看这件事？”）。在
与 AI 的交互中，我们也会遇到类似的两类意图：明确意图和模糊意图。 
模糊意图是指用户提出的问题不够明确，例如“帮我做个事”，这个时候 AI 需要
通过提问逐步明确用户的需求。例如，用户要求“帮我定个出差申请”，AI 会进一步询
问目的地、出差日期等信息，从而将模糊的问题转化为具体任务。明确意图则可以分为
单一意图和多任务意图。 
例如，单一意图可能是“捡起地上的水瓶”，而多任务意图可能是“从某人那里取
某样东西并快递给别人”。这些任务之间有依赖关系，AI 需要正确识别并处理这些任务。 
对于明确的任务，我们把任务定义为“语义事件”，每个事件有相应的参数。比如
在工作流中，某些节点可能会需要特定的变量来执行任务。与人类交互不同，AI 不会一
开始就问所有问题，而是逐步获取所需信息。例如，在订出差机票时，AI 首先询问出发
地和目的地，之后根据具体情况再询问其他信息。 
另一个挑战是在任务切换时的处理。假设用户先要求订票，然后突然收到消息要取
消行程并订餐。这时 AI 应该能理解并切换任务，而不是重新询问用户已提供过的信息，
如身份证号或电话等。当前，大模型在切换任务时常常需要重复获取用户信息，这使得
体验不够流畅。 
我们正在聚焦于如何改进 AI 的意图识别，尤其是在单一意图、模糊意图、多任务
意图的识别过程中，以及如何解决任务切换、恢复和跳转的问题，这些都是提升人机交
互体验的重要方面。 
李飞：这个问题很有趣，我觉得可以从两种形态来讨论：交互形态和系统形态。 
 
52 
特别专题 | Agentic 软件革命 
从交互形态来看，未来的 AI 不会像现在的聊天机器人那样简单。例如，在电影
《钢铁侠》和《蜘蛛侠》中的贾维斯和伊迪斯，它们虽然是工具，但具备高度智能，可
以直观地为我服务。另一种形态像《机器纪元》中展现的具身智能机器人，它们是明确
的机器人，但能有效地帮助我完成任务。还有一种可能是像《终结者》中的机器人，外
观和人类一样，完全不显得是机器。 
从系统形态来看，之前《思考，快与慢》这本书引起了讨论，DeepMind 也发表过
相关论文，探讨了系统架构的“快与慢”。它提出将系统分为前端的“talker”和后端的
“reasoner”，这可能是未来系统架构的发展方向。 
柯杰：我讲一下我理解的三种形态。第一种是尊重现有用户习惯的 AI 整合形态。
我们可以在现有产品中融入 AI 技术，通过 AI Agent 来优化和构建流程，这种方式尊重
用户的使用习惯，能够帮助用户快速适应并提升体验。 
第二种形态是打破信息孤岛。在过去的移动互联网时代，信息被分割成了多个孤岛，
每个 APP 都有自己的闭环。但随着 AI 多模态技术和大模型的发展，我们有了打破这
些孤岛的机会。平台如钉钉、微信等正在推动信息互通，这种开放性使得 AI 不再局限
于单一场景，而是可以在更多地方为用户提供服务。 
第三种形态则关注智能化设备的进化，它不仅仅是具身智能或拟人化。未来的 AI 
可能不再依赖传统的人类形态，比如机器人不一定像人一样使用手机做事，可能会有更
适合的工具。例如，通过 WiFi 路由器分析信号波动来感知家中老人的摔倒行为，而不
需要依赖摄像头来避免隐私问题。这种智能化技术的进步，将会为我们的生活带来更多
便捷和安全，使 AI 技术的发展朝着更加实际和无缝集成的方向前进。 
 
53 
InfoQ 架构师2025年第一季 
Agent 驱动的智能答疑产品构建：问答、诊断
与修复实践 
 
答疑类产品是提升用户体验的重要手段，是平台问题的“清道夫”。然而现有的智
能化答疑产品，只能解决少量的静态文本匹配类问题，无法处理用户真实遇到的平台问
题。因此才会有著名的“ Gartner：64% 受访者不希望客服系统部署 AI ”事件发生。
在 InfoQ 举办的 QCon 全球软件开发大会（上海站）上，阿里巴巴技术专家黄建磊为
我们带来了精彩演讲“Agent 驱动的智能答疑产品构建：问答、诊断与修复实践”，
重点阐述在小喵智能答疑产品的研发实践中，如何通过主动问题定位、根因分析、问题
修复构建的群体智能体，动态化解决用户问题，提升用户满意度。 
 
分享嘉宾 黄建磊  审校 Kitty 
策划 QCon 全球软件开发大会 
 
 
54 
特别专题 | Agentic 软件革命 
内容亮点 
• 当前国内可用模型和 GPT 的代差，会造成工程层面大量的补丁工作； 
• 最真实的 RAG 和 Agent 企业级落地。 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。 
我们团队大概在去年 10 月 1 日的时候，开始启动智能答疑系统的研发，现在正
好过去一年时间，借这次 QCon 的机会，我们把过去一年在实践过程中遇到的问题以及
我们如何解决这些问题分享给大家，希望对大家后续在应用层的落地有所帮助。 
答疑产品的重要性 
答疑产品在我们的生活和工作中无处不在。它是一种用于满足用户在使用特定平台
时遇到问题后寻求解决方案的工具。这个特定平台可以被称为“宿主平台”。例如，常
见的云产品如阿里云、腾讯云和 AWS 等都具备答疑产品的形态。 
答疑产品的重要性体现在以下几个方面：如果设计得当，它可以显著提升用户体验、
服务质量，并增强用户粘性。这些结论可以从一些公开资料和数据中得到佐证。然而，
如果答疑产品设计不佳，反而可能产生负面影响。根据 Gartner 发布的最新报告，一个
令人担忧的指标显示，约 60% 到 70% 的用户不希望他们使用的产品背后的答疑功能
是由 AI 驱动的。甚至有超过 50% 的用户表示，如果他们所使用的产品背后的智能答
疑是由 AI 驱动的，他们可能会转向其他平台。 
答疑产品是依托于宿主平台的，用户在宿主平台上遇到问题后才会转向答疑平台。
接下来，我们介绍一下我们团队所负责的宿主平台。我们平台是阿里巴巴研发运维的基
础设施，面向产品研发人员提供软件交付全链路的自动化、平台化和智能化保障。简单
来说，这就是一个 DevOps 平台，说得更专业一点，就是 SRE（Site Reliability Engineer-
ing，即网站可靠性工程）平台。 
答疑产品的主要问题 
我们先了解一个完整答疑的全链路过程。用户在宿主平台上进行操作时，可能会遇
到红色感叹号提示，表明无法继续操作，这意味着用户遇到了问题。此时，用户会转向
 
55 
InfoQ 架构师2025年第一季 
智能答疑的 AI 机器人，描述自己遇到的问题，例如“我遇到了什么样的问题，该如何
解决？” 
在智能答疑的背后，首先是一个 RAG（Retrieval-Augmented Generation）系统。RAG 
系统包含底层的离线和在线知识库。问题进入系统的第一个环节是问题改写与泛化，这
一环节也被称为问题的前置处理。处理完成后，问题会经过文本与向量的两路召回，从
知识库中找到与问题最相关的问题 - 答案对。然后，将这些相关的内容传递给模型，模
型会总结出一个答案，并通过答疑机器人告知用户，例如“你可以通过什么操作来解决
这个问题”。 
我想特别强调的是，在接下来的分享中，我不会过多强调 RAG 体系，但这并不意
味着它不重要。实际上，RAG 是整个答疑系统中非常关键的环节。目前，无论是学术界
还是工业界，对 RAG 的研究已经相当丰富。例如，LangChain、Llama Index 以及许多公
开的论文都对这一领域进行了深入探讨。如果大家在 RAG 方面遇到问题，开源社区中
已经有大量资源可以提供帮助。 
对于答疑机器人给出的答案，用户可能不满意，或者不确定答案的正确性，这时他
们会转向人工答疑。人工答疑分为两层：第一层是一线答疑人员，他们的主要职责是处
理答疑工作。他们通常是非领域专家，主要解决相对简单的问题。一线答疑人员在解决
问题时，往往依赖自己的隐性知识——即他们头脑中已有的知识。这种隐性知识与显性
知识相对，显性知识是指明确记录在文档或知识库中的知识，而隐性知识则是个人在日
常工作中积累的经验和直觉。 
如果一线答疑人员无法解决问题，他们会将问题转给特定领域的专家。这些领域专
家可能是功能的开发人员，他们对自己的功能模块和工作流程非常熟悉。他们会利用自
己的隐性知识来解决问题。这就是一个完整的答疑系统。 
在智能答疑系统的开发与落地过程中，我们遇到了三个核心问题。 
首先，用户的问题描述往往不准确。智能答疑系统依赖于用户的原始问题，但用户
在输入问题时，往往只是希望快速绕过智能答疑环节，转而寻求人工帮助。因此，用户
输入的问题通常非常简略，甚至不准确。数据显示，约 40% 的用户问题长度小于 8 个
字符。这种简略且不准确的问题描述，即使后续有最先进的 RAG 体系或领先的模型支
 
56 
特别专题 | Agentic 软件革命 
持，也很难为用户提供精确的答案。 
第二是缺乏领域知识。当问题从通用领域转向特定私域时，基座模型往往不具备私
域领域的专业知识。例如，企业内部的特定知识是基座模型所没有的，这就导致 RAG 
体系中的知识库无法提供有效的解决方案。在答疑过程中，无论是人工一线答疑还是功
能开发人员，他们解决问题时依赖的往往是自己脑海中的隐性知识，而这些知识并未被
纳入 RAG 知识库中。因此，整体私域问题的解决率非常低。 
第三是用户对 AI 的不信任。在实际应用中，用户对 AI 机器人给出的解决方案
（例如分步骤的操作建议）往往持怀疑态度，甚至在转向人工答疑时，用户会要求一线
答疑人员确认机器人给出的答案是否正确。本质上，当前的智能体系给出的是一套面向
过程的解决方案，用户需要自行辨别这些解决方案的正确性，而这一过程需要用户承担
一定的成本，这是用户不愿意接受的。相比之下，人工答疑给出的答案默认是被信任的，
用户会直接执行。 
目前市场上许多所谓的“ AI 产品”其实只是在名称上有 AI，本质上可能只有简单
的提示（Prompt），而缺乏真正的智能化能力。在实际落地智能化应用时，我们遇到的
这些问题，正是由于智能化本身的基座编程范式改变所引发的连锁反应。这些问题才是
我们需要更多精力去解决的。相比之下，一些通用的、常见的组件在开源社区或工业界
中都有相对成熟的参考案例。而我所提到的这些问题则更为具体、更具挑战性，这也是
我今天分享的核心内容。 
LLM 如何解决上述问题 
接下来我们探讨如何在现有的 AI 基座之上，利用 AI 的能力解决上述提到的三个
核心问题。首先，我们着重解决第二个问题——缺乏领域知识。在答疑过程中，人工答
疑人员通常依赖自己脑海中的隐性知识来解决问题。那么，如何将员工脑海中的隐性知
识转化为显性知识，并让这些显性知识能够被上层智能化系统（如 RAG 或模型对齐、
SFT 过程中的语料）所利用呢？ 
在解决方案的迭代过程中，我们最初的想法是“人工智能，先有人工，再有智能”。
因此，我们让一线人员手动编写知识，类似于编写代码的过程。然而，这种方式对开发
人员的压力很大，因为它是一个旁路分支，不在他们的日常工作链路中。同时，显性知
 
57 
InfoQ 架构师2025年第一季 
识的审核成本非常高，因此这条人工编写的链路难以持续。 
随着对业界研究的深入，例如清华大学实验室关于智能体与环境对齐的工作，我们
意识到可以利用专家知识来训练智能体。这启发我们转变思路：每个员工在日常工作中
实际上已经在利用隐性知识解决问题，那么我们是否可以基于这些日常行为来提取显性
知识呢？这就是隐性知识显性化解决方案的核心思路。 
隐性知识显性化 
隐性知识显性化是一个相对泛化的命题，不同的场景对知识的需求和知识的表示形
式都不相同，因此落地形式也各有差异。举一个具体的案例：我们有一个智能修复 
Agent，其中的 Planning 过程至关重要。Agent 在解决问题时需要规划如何解决，而这
个规划过程需要私域知识的增强。我们发现，一线员工在构建平台上发起构建时，如果
出现错误和错误日志，他们会利用隐性知识通过代码编写来解决问题。此时，错误日志
以及解决错误的代码 Diff 可以构成一个生产数据集。我们可以通过这个数据集提取显
性知识。 
第一个阶段是知识提取。底层依然使用基础语言模型，核心是利用其推理能力。我
们向语言模型提供错误日志和用户代码的 Diff ，请求模型提取用户解决问题时的思路。 
然而，仅靠知识提取还不够，因为模型底层可能存在幻觉，我们需要保证提取内容
的准确性。业界常见的准确性解决方案有三种：第一种是找领域专家进行判断，这是最
准确的方法；第二种是使用更先进的模型作为“教师”进行判断，但在我们的私域场景
中显然不适用，因为私域知识无法让 GPT 或 O1 这样的通用模型进行判断；第三种是
我们实践中采用的方法——自我一致性。简单来说，就是让模型多次判断同一问题。如
果多次判断结果一致，那么在当前场景下就可以认为是正确的。我们将这些经过验证的
知识流转到显性知识库中。 
显性知识的提取核心在于通过精心设计的 Prompt 引导模型完成知识提取。具体来
说，Prompt 采用三段论的结构化方式：首先，明确模型的身份和角色；其次，说明模
型需要完成的任务；最后，告知模型输入的内容，例如错误日志和用户提交的代码 Diff。
我们希望模型能够从这些输入中提取出显性知识，包括识别问题的类型、定位问题的方
法、针对此类问题的通用解决路径，以及用户实际采用的解决路径。 
 
58 
特别专题 | Agentic 软件革命 
第二个阶段是正确性的判断。由于模型底层存在不稳定性，例如幻觉等问题，导致
其输出可能具有概率性。因此，在正确性判断阶段，我们采用基于自我一致性的方法，
让模型对同一问题进行多次判断。这一阶段的 Prompt 设计与第一阶段类似：首先明确
模型的角色和任务，然后解释输入内容的含义，最后告知模型需要返回的结果形式。在
这一阶段，模型需要从多个维度对提取的知识进行评估，包括：识别的问题是否正确、
定位问题的理由是否合理、通用解决路径是否正确，以及用户的解决路径是否正确。最
终，模型需要给出一个综合结论。这一阶段的目标是确保提取的知识在准确性上达到可
接受的标准。 
问题感知 Agent 
在智能答疑系统中，用户对 AI 结果的信任度较低，尤其是在非编码领域。用户可
能不会接受 AI 给出的解决方案，而是选择转人工处理。为了解决这一问题，我们提出
了将面向过程的解决方案转变为面向结果的解决方案。传统的面向过程解决方案是指导
用户按步骤操作，例如在构建失败的场景中，告诉用户第一步、第二步、第三步应该怎
么做。而面向结果的解决方案则是直接为用户提供一个已经验证成功的代码 Diff 。这
种思路的转变能够降低用户接受结果的成本，从而提高智能答疑系统的拦截率。 
基于这一思路，我们引入了智能修复 Agent 的概念。我们意识到 Agent 将成为未
来智能化应用的核心基座。如果将大语言模型（LLM）比作操作系统，那么 Agent 就是
运行在该操作系统上的应用程序。因此，我们构建了一套完整的框架，涵盖 Agent 的
定义、运行时管理、数据管理和知识提取。 
在 Agent 的定义上，我们遵循了社区中广泛认可的 LangGraph 架构。这种架构在
底层设计和生态扩展性方面都表现出色。在智能修复 Agent 的具体实现中，我们首先
确认构建错误的发生，然后执行修复动作。修复节点中会进行规划（Planning）和排序
（Sorting）操作。我们发现，在代码修复场景中，文件修改工具的成功率较低且受多种
因素影响，因此我们将文件修改单独作为一个节点处理，并在修复过程中进行数据上报。
这些数据对后续优化和效果评估具有重要意义。 
Agent 运行时的设计至关重要。Agent 本质上是一个典型的“木桶应用”，由多个
节点组成，任何一个节点的效果劣化都会对后续环节产生放大效应。为了避免这一问题，
 
59 
InfoQ 架构师2025年第一季 
我们将 Agent 的运行时设计为一个链表，链表由多个步骤组成，每个步骤对应一个节
点的运行时。每个步骤包含两个设计要素：Action 和 Checkpoint。Action 是模型的结构
化输出，包括工具选择、行为生成和推理过程；Checkpoint 用于回退，当运行中出现效
果劣化时，可以基于某些节点进行回退。Checkpoint 中包含状态（state）和环境
（environment），例如当前的 Git 环境或工作区等。此外，我们还引入了 Reward 机
制，类似于机器学习中的损失函数，使 Agent 能够自适应地决定是前进还是回退。 
在数据到知识的维度上，我们继续推进隐性知识显性化的工作。最终，我们直接向
用户提供经过验证的代码 Diff，用户可以直接接受而无需再手动执行复杂的步骤。 
在智能答疑产品的开发过程中，我们整个思路是通过遇到一个个问题并基于这些问
题实现一个个 Agent 。我们预测智能答疑未来将朝着群体智能的方向演进。尽管过去
群体智能可能看起来较为虚幻，但在某些垂直领域和产品中，群体智能的实现或许会更
快到来。我们团队将继续面向更多问题，补充更多 Agent ，并探索 Agent 之间的自主
协作和沟通。我们认为，无论是答疑还是其他智能化应用，未来的底层架构一定是面向
多 Agent 协作的方式演进。 
心得感想 
在 AI 产品的开发和应用中，我们有以下几点心得。 
• 关注业务价值：当前许多 AI 产品只是名义上有 AI（AI in name only），实际上
我们需要真正关注智能化应用的业务价值，避免仅仅为了蹭热度而开发产品。AI 
产品的核心价值在于解决实际问题，提升效率或优化用户体验，而非单纯的技术
堆砌。 
• 进行可行性验证：与传统软件工程不同，AI 的基础是概率模型，这意味着其输
出具有一定的不确定性。在某些需要严格容错的产品场景中，例如胰岛素注射剂
量的计算等不允许出错的场景，我们需要更加慎重地验证 AI 的可行性，确保其
可靠性和安全性。 
• 优化用户体验：用户体验是 AI 产品成功的关键之一，主要体现在以下两个方面。 
○ 成本与效率的权衡：在 Agent 领域，用户体验和准确率之间往往需要进行权
衡。Agent 的运行需要消耗大量资源，因此在设计时需要平衡效率和成本，
 
60 
特别专题 | Agentic 软件革命 
确保用户在使用过程中能够感受到高效和便捷。 
○ 提供可解释性：为了增强用户对 AI 系统的信任，需要提供更多的可解释性。
用户需要了解 AI 为什么做出某个决策或执行某个操作，这有助于减少用户
的疑虑，提升他们对系统的信任感。 
嘉宾介绍 
• 黄建磊，阿里巴巴技术专家。2017 年担任钉钉桌面前端负责人，是可交互卡片（现
酷应用的重要底层能力）的发起人。2021 年开始做 O2 Space 研发平台，通过建设 
O2 Pai 的开放能力，打造端到端的开放能力。2023 年至今，担任阿里巴巴爱橙科技
的技术专家。研发过程智能化创新小组负责人，探索智能答疑、智能诊断、智能修
复等研发过程中的智能化改造，提升研发效率和质量。 
 
61 
InfoQ 架构师2025年第一季 
Andrej Karpathy 爆火演讲刷屏技术圈：AI 
开启软件 3.0，重写一切的时代来了！ 
 
编者按： 
近日，在旧金山 AI 创业学校的讲台上，曾任职斯坦福大学、OpenAI 和特斯拉
的 AI 领袖 Andrej Karpathy，以一种横跨学术与产业的独特视角，揭示了一场正
在重塑技术世界的范式转移。 
Andrej 看到了一场“编程革命”正在发生。随着 AI 技术的发展，软件编程已经
进入了“3.0 时代”，自然语言取代传统代码成为核心编程接口，大模型则承担
起过去需要人工编写的复杂逻辑。 
作者 Andrej Karpathy 
编译 冬梅  策划 Tina 
 
62 
特别专题 | Agentic 软件革命 
Andrej 指出，这一转变远非简单的工具迭代。当开发者通过日常语言指令即可驱
动系统，当用户的需求能直接转化为机器可执行的意图时，我们实际上是在构建
一种“新型计算机”。这种计算机不再依赖精确的语法规则，而是以概率化、语
义化的方式理解世界——就像人类一样。 
这种进化对开发者来说是一件好事，这意味着编程门槛的消弭。对用户来讲更是
好事，因为能让交互方式彻底解放，人机协作再也没有语言层面的障碍。正如 
Andrej 所强调的：我们正站在人机关系的历史转折点上，未来的软件将不再是冷
冰冰的工具，而是能理解、推理甚至主动协作的智能伙伴。这场变革的深度，或
许不亚于当年从命令行到图形界面的跨越。 
以下内容为 InfoQ 基于 Andrej Karpathy 在现场分享的视频整理而来，在保持
原意的基础上进行了编译。 
AI 颠覆了传统的软件构件 
很高兴今天在这里和大家聊一聊“AI 时代的软件”。我听说在座很多人是本科、硕
士、博士等在读学生，正准备进入这个行业。现在正是进入这个行业的一个非常独特、
非常有趣的时间节点。 
为什么这么说？因为我认为如今的软件正在经历又一次深刻的变革。注意咯，我这
里用到的词是“又一次”，是因为我其实之前就做过类似的演讲，那为啥还要再做这个
话题的演讲？因为软件一直在变，所以我也总能找到新材料来做新的演讲。而这一次的
变化，我觉得是非常根本性的。 
大致来说，软件在过去 70 年里本质上并没有太大改变，但在最近这几年里，它已
经经历了两次巨变。因此，我们有大量的工作要做——大量的软件需要被重写或重新设
计。 
我们把视角转向软件的疆域。如果我们把它（Map of GitHub）视为软件地图的话，
它展示了我们所写的所有软件代码，也就是让计算机在数字空间中执行任务的各种指令。 
放大之后可以看到，每一个小点其实都是不同的代码仓库，都是已经编写的代码。 
 
63 
InfoQ 架构师2025年第一季 
 
几年前我开始意识到，软件正在发生变化，开始出现一种“新的软件类型”。当时
我将其称为“软件 2.0”。 
软件 1.0，就是我们传统意义上写给计算机的代码。而软件 2.0，本质上是神经网
络的权重。你不再直接写代码，而是通过调整数据集、运行优化器，来训练出神经网络
的参数。当时神经网络还被很多人当成是像决策树之类的分类器，没什么特别。但我当
时的观点是，这其实是一个新的软件范式。 
 
现在在软件 2.0 时代，也开始出现类似“GitHub”的东西。比如 Hugging Face，基
本上就是软件 2.0 时代的 GitHub。还有像 Model Atlas 这样的可视化工具，可以看到
 
64 
特别专题 | Agentic 软件革命 
各种模型的参数——举个例子，中间那个大圆就是图像生成模型 FLUX 的参数点。每次
有人基于 FLUX 微调一个新模型，其实就是在这张图上“提交了一次 git commit”，本
质上是生成了一个新版本的图像生成器。 
 
所以我们可以理解为： 
• 软件 1.0 是写给计算机的代码 
• 软件 2.0 是写给神经网络的权重参数 
比如 AlexNet，就是一个图像识别神经网络。 
过去我们所熟悉的神经网络，大多是“定值函数”型的计算机——比如输入一张图
像，输出一个类别标签。但最近发生了一个根本性变化，那就是：神经网络变得可以
“被编程”了，这要归功于大语言模型（LLMs）。所以我认为这是一个全新的计算机世
界了。 
 
65 
InfoQ 架构师2025年第一季 
 
所以这个新时代值得称之为软件 3.0：你不再写代码、不再训练神经网络参数，而
是直接通过“Prompt”（提示词）来“编程” LLM。更妙的是，这种程序语言就是我们
日常说的“英语”。 
这实在是太有趣了。我们再来用一个例子说明下软件 3.0 时代和其他编程方式的区
别： 
假设你要做情感分类，在软件 1.0 时代，你需要写一段 Python 代码，到了 2.0 
时代需要训练一个神经网络，而现在，很多 GitHub 代码不只是代码了，你可以直接用
英语写一个 Prompt 来让大语言模型输出分类结果。 
 
 
66 
特别专题 | Agentic 软件革命 
这其实是一种全新的编程方式，而且它是用自然语言完成的。 
 
几年前我意识到这一点时发了一条推文，很多人因此关注到了这一转变。这条推文
现在依然是我置顶的内容：我们现在可以用英语来编程计算机了。 
 
我在特斯拉时曾参与自动驾驶系统的开发。我们试图让汽车自动驾驶，并在当时展
示过一个架构图。 
 
67 
InfoQ 架构师2025年第一季 
 
图中显示，输入来自各种传感器（如摄像头），经过一系列软件处理，最终输出方
向盘角度和加速度。 
当时我指出：系统中有“一吨”左右的 C++ 代码，也就是软件 1.0，同时也开始出
现一些神经网络用于图像识别。这种变化非常有趣——随着自动驾驶系统性能提升，神
经网络的规模和能力越来越强，同时，我们开始逐步删除大量原本由 C++ 编写的逻辑
代码。 
原来那些“把多摄像头图像拼接起来”之类的操作，现在交给神经网络来做。结果
是：我们删掉了大量的 1.0 代码。可以说，软件 2.0 堆栈“吞噬”了软件 1.0 堆栈，
变成了系统的主干部分。 
现在，我们正在经历同样的事情。新的软件范式（软件 3.0）正在快速向整个技术
栈渗透。我们现在面前有三种完全不同的编程范式：1.0、2.0、3.0。 
如果你正要进入这个行业，我建议你最好对三者都非常熟悉。它们各有优缺点：有
的功能可能适合直接写代码（1.0），有的适合训练神经网络（2.0），而有些则只需要
写一个 Prompt（3.0）。我们会不断面临这样的抉择：这个功能要用哪种方式实现？要
不要训练模型？是不是直接用 LLM 生成答案就可以？ 
 
68 
特别专题 | Agentic 软件革命 
 
AI 正成为新电力 
而我们也必须具备在这三种范式之间灵活切换的能力。 
接下来，我想进入这场分享的第一部分…… 
 
大语言模型（LLM）具备公共基础设施、晶圆厂和操作系统的多重特性——它们正
在成为一种新的“操作系统”，由各大实验室打造，并像公用事业一样进行分发（目前
是这样）。很多历史类比都适用——在我看来，我们现在的计算水平大概相当于上世纪 
60 年代。 
关于 LLM，以及如何理解这个新范式和生态系统，以及它的样貌，我想引用一段安
 
69 
InfoQ 架构师2025年第一季 
德鲁（Andrew）多年前说过的话，他应该接着我后面发言。他当时说：“AI 是新电
力。” 
 
我觉得这句话很有启发性，它确实很好地捕捉到了一个关键点：LLM 显然具备类似
“公用事业”的属性。 
现在的 LLM 实验室，比如 OpenAI、Gemini、Anthropic 等，会投入大量资本支出
（CapEx）去训练 LLM，这就像在建设电力网络。而随后，它们又需要通过 API 把智能
能力“供电”给我们所有人，这是运营支出（OpEx）。我们通过按百万 token 计价的
方式进行付费接入。这种模式本质上就像对公用事业的需求：我们要求低延迟、高可用、
服务质量稳定。 
 
在电力系统中，有“切换开关”，可以在市电、太阳能、电池或发电机之间切换。
在 LLM 世界里，我们也有“开放路由器”类的机制，可以轻松在多个 LLM 提供方之
间切换。因为 LLM 是软件，它们不会在物理空间上产生直接竞争，你可以同时使用多
个“供电方”。这点很有趣。 
前几天我们看到多个 LLM 出现故障，人们突然无法正常工作了。我们逐渐依赖它
 
70 
特别专题 | Agentic 软件革命 
们到这样一个程度：一旦最先进的 LLM “宕机”，就像发生了“智能停电”，就像电
网电压不稳定时整个社会变“笨”了。我们越依赖这些模型，这种影响只会越发明显。 
 
但 LLM 不只是具有“公用事业”的属性，它们也有点像“晶圆厂”。训练 LLM 
需要的资本支出是巨大的，这不仅仅是造个发电站那么简单。这是一项高强度的研发投
资。LLM 技术栈复杂而深，技术秘密正在快速集中在少数几个实验室中。 
不过类比也开始模糊了，因为毕竟 LLM 是软件，而软件的可变性很强、防御性较
弱。不过还是可以找到一些对应关系。例如，可以将“4nm 工艺制程”类比为一个具有
固定 FLOPS 上限的 GPU 集群。当你仅使用 Nvidia GPU 做模型训练、而不涉足芯片制
造时，这是类似“无晶圆厂”的模式；而如果你像 Google 一样自己设计 TPU 并训练
模型，那就是“英特尔模式”——你拥有自己的“晶圆厂”。 
但对我来说，最贴切的类比其实是：LLM 像操作系统。这不仅仅是电力或水这种从
水龙头流出的商品，而是越来越复杂的软件生态系统。 
 
71 
InfoQ 架构师2025年第一季 
 
这个生态系统也开始呈现类似的结构：有封闭源的主流平台，比如 Windows、ma-
cOS，也有开源替代品，比如 Linux。对 LLM 来说，我们也有几个闭源的主导者，然后
像 LLaMA 这样的开源生态，可能会慢慢成长为 LLM 时代的“Linux”。当然现在还太
早，这些 LLM 还相对简单，但它们会快速复杂化，因为这不仅仅是模型本身的问题，
还涉及工具使用、多模态融合等等。 
我曾经尝试画出一张草图，来捕捉这个新范式。LLM 就像一种新型的“计算机”，
它类似于 CPU，而上下文窗口就像内存。LLM 负责调度“内存 + 算力”来解决问题，
并调用各种能力。从这个角度看，它非常像一个操作系统。 
 
再举个例子，比如你想下载一个应用程序，比如 VS Code。你可以在 Windows、
 
72 
特别专题 | Agentic 软件革命 
Linux 或 macOS 上运行它。同样地，现在你可以拿一个 LLM 应用，比如 Cursor，在 
GPT、Claude 或 Gemini 上运行——只需要一个下拉选择而已，这非常类似。 
 
还有更多类比浮现在我脑海中。我们现在所处的阶段，就像是 1960 年代初期——
LLM 所需的计算资源还非常昂贵，这使得模型必须集中部署在云端，我们只能作为“瘦
客户端”通过网络接入这些计算机。每个人都没有完整控制权，因此我们只能“时间共
享”地使用它们——就像当年云端计算机的批处理模式。 
 
那时候的操作系统也是集中部署的，一切都需要网络传输，大家排队等待自己的
“批次”被执行。个人计算机革命还未发生——因为经济上还不合理。但现在已经有人
 
73 
InfoQ 架构师2025年第一季 
在尝试了，比如 Mac Mini 实际上很适合跑某些 LLM 模型。因为很多 LLM 推理是
“batch=1”的，非常依赖内存，而 Mac Mini 恰好内存大，这可能是个人计算机化的早
期迹象。 
 
不过现在还没人真正发明“LLM 的 GUI”。我每次跟 ChatGPT 或其他模型交流时，
都感觉像是在用终端和操作系统交互——纯文本接口，直接对话。我们现在看到一些 
app 拥有 GUI，但还没有一个“跨任务通用 GUI”出现。这是一个还没被发明出来的机
会。 
 
LLM 与传统操作系统还有一个重大区别，这点让我特别在意：LLM 颠覆了科技扩
散的路径。 
 
74 
特别专题 | Agentic 软件革命 
历史上每一项革命性技术——电力、密码学、计算、飞行、互联网、GPS ——最早
的使用者总是政府或大型企业，因为新技术贵又复杂。然后才逐渐扩散到消费者。 
 
但 LLM 正好相反。最早的应用者是我们这些普通用户。例如，过去早期计算机用
于军用弹道学，但现在 LLM 却在教我怎么煮鸡蛋！我每天的大部分使用就是如此。新
的“魔法计算机”现在首先服务的是普通人，而不是国家或企业。 
政府和企业反而在落后地采用这些技术。这完全颠倒了传统路径，也可能启示我们：
真正的 killer app 会从个人用户端长出来。 
总结一下：LLM 实验室和 LLM 这两个术语现在非常准确。但我们需要意识到，
LLM 本质上是复杂的软件操作系统，我们正在“重新发明计算”，就像 1960 年代那样。
而且它们现在以“时间共享”的方式提供服务，像公用事业一样被分发。 
 
75 
InfoQ 架构师2025年第一季 
 
真正不同的是，它们不是掌握在政府或少数企业手里，而是属于我们每一个人。我
们每个人都有电脑，而 LLM 只是软件，它可以在一夜之间传遍整个星球，进入数十亿
人的设备。 
这是疯狂的。它已经发生了。现在，轮到我们进入这个行业，去编程这个“新计算
机”。 
太不可思议了。 
大模型有超能力，也有认知缺陷 
要编程 LLM，我们必须先花点时间思考这些东西到底是什么。我尤其喜欢谈谈它们
的“心理学”。 
 
我喜欢把 LLM 想象成“人类精神”（people spirits）。它们是对人类的随机模拟
（stochastic simulation），而这个模拟器恰好是一个自回归 Transformer。Transformer 
 
76 
特别专题 | Agentic 软件革命 
是一种神经网络，它在 token 层面上工作，逐块处理，就像“咔哒、咔哒、咔哒”一样，
每一块的计算量几乎是一样的。 
 
这个模拟器的本质是某些权重参数，它被拟合到了我们从互联网上获得的所有文本
上。于是我们得到了这样一个模拟器。由于它是用人类的数据训练出来的，所以它具备
一种“涌现”的类似人类的心理学特征。 
你首先会注意到的一点是，LLM 具有百科全书式的知识和记忆力。它们可以记住非
常多的信息，远远超过任何一个人能记住的量。因为它们读过的东西太多了。这让我想
起一部电影《雨人》（Rainman），我真的很推荐大家去看一下，非常棒的一部电影。
我非常喜欢这部电影。达斯汀·霍夫曼在里面饰演一个自闭症学者型天才，他拥有近乎
完美的记忆力。他能看一遍电话簿，然后就记住所有的名字和电话号码。 
 
77 
InfoQ 架构师2025年第一季 
 
我觉得 LLM 在某种意义上就很像这样，它们可以轻松记住 SHA 哈希值、各种信
息等。 
所以在某些方面，它们无疑有“超能力”。但它们同样也存在很多“认知缺陷”。
例如，它们经常“幻觉”（hallucinate），也就是说，它们会凭空编造内容。它们对于
自身知识的内部模型也不够好，至少还不够完善。虽然这方面已经有所进步，但仍然不
够完美。 
 
它们展现出的是一种“锯齿状的智能”：在某些领域，它们可以展现出超人的问题
解决能力，但在其他方面却会犯下人类绝不会犯的错误。例如，它们可能坚持说 9.11 
比 9.9 大，或者说“strawberry”里有两个 “R”。这些都是比较有名的例子。这些
 
78 
特别专题 | Agentic 软件革命 
“棱角”是你很容易会被绊倒的。 
 
它们还会“顺行性遗忘”（Anterograde Amnesia），我这里指的是：如果你有一个
新同事加入团队，他会随着时间推移慢慢了解组织、获取上下文、增长知识，他会回家
睡觉、巩固知识、发展专长。但 LLM 不会自动做到这些。这其实是目前 LLM 的研发
还没有解决的问题。 
所以，LLM 的上下文窗口其实更像是“工作记忆”。你必须非常明确地对其进行编
程，因为它不会像人类一样“自然而然”变得更聪明。我觉得很多人会在这个类比上误
入歧途。 
 
 
79 
InfoQ 架构师2025年第一季 
我推荐大家看两部电影：《记忆碎片》（Memento）和《初恋 50 次》（50 First 
Dates）。在这两部电影中，主角的大脑权重是“固定的”，而上下文窗口每天早上都会
被清空。这样一来，要去上班、要维持人际关系就非常困难。而这正是我们每天与 LLM 
打交道时经常遇到的情形。 
 
还有一点值得一提，是跟安全相关的 LLM 使用限制。比如说，LLM 非常容易“轻
信”外界信息，它们容易受到提示注入（prompt injection）攻击的影响，可能会泄露你
的数据等等。当然，还有很多其他安全方面的考量。 
 
总之，说到底，我们必须一边思考这个具有“超能力”的系统，一边应对它存在的
 
80 
特别专题 | Agentic 软件革命 
各种认知缺陷和问题。 
 
大模型带来的机遇 
那么现在的问题在于：我们该如何编程这些系统？我们该如何在避开它们局限的同
时，发挥它们的超能力？ 
接下来我想讲的，是如何实际使用这些模型，以及其中存在的一些最大机会…… 
 
大型语言模型（LLMs）就像“人的灵魂”，这意味着我们可以用它们构建具备部分
 
81 
InfoQ 架构师2025年第一季 
自主能力的产品。 
 
我整理了一些在这次分享中觉得有趣的点，首先我最感兴趣的是所谓的“部分自主
应用”。 
比如，以编程为例，你当然可以直接去用 ChatGPT，复制粘贴代码，提交 bug 报
告，获取回复，再继续复制粘贴。但为什么要这样做？为什么要直接跟操作系统交互？
其实更合理的方式，是为这个场景构建一个专门的应用。 
 
我想在座很多人可能都在用 Cursor，我自己也在用。Cursor 就是一个很好的例子，
它比直接去用 ChatGPT 更合适。它是早期的 LLM 应用中非常典型的一种，具备一些我
认为在所有 LLM 应用中都非常有用的共性： 
 
82 
特别专题 | Agentic 软件革命 
 
首先，它保留了传统的交互界面，人类依然可以手动完成所有工作，但在此基础上，
它集成了 LLM，可以处理更大块的任务。 
一些共性的关键点包括： 
• 上下文管理：LLM 在应用中自动处理大量上下文管理任务； 
• 多轮调用编排：比如在 Cursor 中，底层有用于代码文件的嵌入模型、聊天模型、
diff 应用模型等，它们都经过编排协调； 
• 专用 GUI 的重要性：我们并不总是希望用纯文本交互，文本不易读、不易理解，
而且很多操作也不适合用文本完成。你更希望看到一个 diff，用绿色表示新增、
红色表示删除，然后只需 Command + Y 接受、Command + N 拒绝，而不是用文
本输入这些命令。GUI 让我们可以快速审查这些不完美的系统，效率更高。 
我后面会再讲到这一点。再提一个关键特性，就是我所谓的“自主滑块”
（autonomy slider）。以 Cursor 为例，你可以选择轻量的补全（你主导），或者选中一
段代码 Command + K 修改它，再比如 Command + L 改整个文件，甚至 Command + I 任
意改整个 repo。这就是“自主滑块”：你可以根据任务复杂度，选择愿意放权给 LLM 
的程度。 
再看一个成功的 LLM 应用例子——Perplexity。 
 
83 
InfoQ 架构师2025年第一季 
 
它跟 Cursor 类似，具备如下特性： 
• 集成了大量信息处理； 
• 编排了多个 LLM 调用； 
• 提供 GUI 来让你审阅生成结果，比如引用来源你可以点进去看； 
• 也有“自主滑块”：你可以只做一次快速搜索，或者进行深度研究，甚至查完 
10 分钟后再回来。这些都是你放权的不同层级。 
所以问题来了：我认为未来大量软件都会变成“部分自主”的，那么这些软件会是
什么样子？对于你们这些维护产品和服务的人来说，怎么把自己的产品做成“部分自主”
的？LLM 是否能看到人能看到的所有内容？是否能执行人类能执行的所有操作？人是否
能始终保持在环中进行监督？因为这些系统仍然容易出错，尚未完美。 
 
84 
特别专题 | Agentic 软件革命 
 
我们还得思考：在像 Photoshop 这样的图形软件里，diff 应该如何呈现？传统软件
界面里，很多开关、控件都是为人设计的，现在这些也必须要能被 LLM 理解和访问。 
关于 LLM 应用，还有一点我认为没有被充分重视：我们和 AI 的协作形式正在发
生变化。通常 AI 负责生成，我们负责验证。而我们要让这个生成 - 验证循环尽可能快，
这样我们才能高效地完成工作。 
 
让这个循环更快的两个核心方向： 
• 加快验证速度：GUI 非常重要。GUI 利用了我们大脑里的视觉 GPU——读文本
 
85 
InfoQ 架构师2025年第一季 
很费劲，看图很快。可视化能极大提高系统审阅效率； 
• 控制 AI 行为的范围（“牵好绳子”）：现在很多人对 AI Agent 太激动了。但
你看，比如一次性给我生成一个包含 1 万行代码的 diff 并不实用。我作为人类
仍然是瓶颈，我必须确认没有引入 bug、逻辑正确、安全无虞等等。所以必须控
制 AI 的主动程度。 
 
我现在做 AI 辅助编程的时候，就是这个感觉：小的字节级补全很顺，但如果让我
用一个过于激进的 Agent 来完成任务，那体验就不太好了。 
我自己还在探索怎么把这些 Agent 融入我的编程流程，实践 AI 辅助编程。我倾向
于以小步快跑的方式前进，确保每次修改都是安全的，这样可以让验证 - 生成循环非常
快。 
 
 
86 
特别专题 | Agentic 软件革命 
我也看到很多人分享了关于如何和 LLM 配合的最佳实践。举个例子：如果你的 
prompt 很模糊，AI 就容易跑偏，验证失败后你还得重新 prompt，这就拖慢节奏。更
好的方式是，在 prompt 上多花点时间，明确具体目标，提高验证成功率，让流程更顺。 
 
所以我想，我们都需要摸索出一套自己的 LLM 工作方法。我现在也在思考 AI 与
教育结合的方式。你不能指望直接去 ChatGPT 说一句“教我物理”，然后它就能教你。
AI 很容易“迷路”。 
 
对我来说，这应该是两个不同的应用：一个是给教师设计课程的工具，一个是面向
学生的授课工具。它们之间会产生一个“课程”这个中间产物，是可审阅的，可以确保
结构合理、内容一致。这种方式把 AI 控制在教学大纲、项目节奏范围内，更容易成功。 
 
87 
InfoQ 架构师2025年第一季 
我还有一个类比想提一下：其实我对“部分自主”不陌生，我在 Tesla 做了五年，
这就是个“部分自主产品”。例如，Autopilot 的 GUI 就直接嵌在仪表盘里，展示神经
网络看到的内容；我们也有“自主滑块”：随着时间推进，Autopilot 能执行的任务越来
越多。 
 
我想讲一个小故事：我第一次体验自动驾驶是在 2013 年，一位在 Waymo 的朋友
带我在 Palo Alto 兜了一圈，我还用 Google Glass 拍了照片。我们开了 30 分钟，从城
市到高速一路畅通无阻，没有任何人为干预。 
 
当时我心想，“自动驾驶马上就来了！”但如今 12 年过去了，我们仍然在研究这
 
88 
特别专题 | Agentic 软件革命 
个问题，仍然需要人为参与。你看到路上的 Waymo 车辆，虽然看起来无人驾驶，其实
很多时候还有远程操控和人类参与。所以我们仍然没有完全解决自动驾驶问题，虽然现
在确实比过去更接近成功了，但它就是很难。 
 
“2025 是 Agents 元年”，这种认知让人担忧 
我认为软件的复杂性就像自动驾驶一样。所以每次我看到有人说“2025 是 Agents 
的元年”，我都会担忧。我觉得这更像是“Agents 的十年”，我们要慢慢推进，让人始
终在环内，不能浮躁，这毕竟是软件，要认真对待。 
 
我一直脑海里还有一个比喻：钢铁侠的战衣。这个比喻我非常喜欢，因为它非常贴
切地展示了技术的本质：它既可以作为增强工具，由 Tony Stark 驾驶，也可以在某些电
影里自己飞来救人、完成任务。 
 
89 
InfoQ 架构师2025年第一季 
 
所以我们要构建的是“战衣”，不是“机器人”。我们不是要一开始就炫技做全自
动 agent，而是要先构建具有自主滑块的“部分自主产品”，配备自定义的 GUI 和 
UI/UX，让人类的生成 - 验证循环变得飞快。 
 
但我们也不能忘记，长期来看，这些任务是可以被完全自动化的。你的产品中应该
包含一个“自主滑块”，并思考如何逐步推进自主程度。 
我想说的是：现在是构建这类产品的大好机会。 
氛围编码：是计算机，也具备人的特质 
接下来我想换一个话题，谈谈另一个特别的维度——不仅是新的编程方式出现了，
而且这个“新语言”是用英语来编程。 
 
90 
特别专题 | Agentic 软件革命 
也就是说，所有人都可以编程了，因为大家都讲自然语言。这让我极度看好这个方
向，因为这在历史上是前所未有的。 
 
过去你得花 5 到 10 年学习编程，现在不再需要了。 
你有没有听说过 “vibe coding”？这个概念最早是从我发的一个推文火起来的。 
 
当时我没觉得这个推文会火，它只是我一个随手的想法，结果却成了大 meme，后
来还有了 Wikipedia 页面之类的。 
 
91 
InfoQ 架构师2025年第一季 
 
Tom Wolf（来自 Hugging Face）分享了一个我非常喜欢的视频，是一群小朋友在 
Vibe Coding。这是个非常温暖的视频。你看完根本不会对未来感到悲观——未来真的挺
棒的。 
 
我觉得这将成为一代人接触软件开发的“网关药物”。我不是“末世论者”，我很
看好下一代人。我也尝试过 Vibe Coding，因为它真的太有趣了。 
Vibe Coding 特别适合那种“周六随便搞点啥”的场景，比如我之前就用它构建了一
个 iOS 应用…… 
 
92 
特别专题 | Agentic 软件革命 
 
大型语言模型（LLM）如今已经成为数字信息的主要消费者和操作者之一（在图形
用户界面 / 人类和 API/ 程序之外），所以我们要为 Agent 而构建！ 
我们能否只构建 Agent？ 
我真心希望 Agent 能完成这些工作，我自己可不想干了，谢谢。 
 
从宏观上来说，我认为我们迎来了一个全新的数字信息“消费者”与“操作者”类
别。过去，只有人类通过图形界面与计算机互动，或者程序通过 API 交互。而现在我
们拥有了一个完全不同的新事物：Agent。它们是计算机没错，但也具备某种拟人特质，
可以说是“互联网上的人类精神”。它们需要与我们的软件基础设施进行交互，那我们
是不是可以为它们专门构建系统呢？这是一个全新的方向。 
 
93 
InfoQ 架构师2025年第一季 
 
举个例子，就像你可以在自己的网站上放一个 robots.txt 文件，用来指引或建
议爬虫行为，未来你也许可以添加一个 llm.txt 文件。它可以是一个简单的 mark-
down 文件，向 LLM 说明你这个域名是做什么的。对于 LLM 来说，这种格式非常友好。
相比之下，如果它必须解析网页的 HTML 内容，这是非常容易出错且困难的，最终效
果通常不理想。所以我们应该直接对 LLM“说话”，这是值得做的事情。 
 
现在，大量的技术文档是写给人类看的，会包含列表、加粗、图片等内容，这些对 
LLM 来说并不直接可读。我看到有一些服务提供商开始转型，把他们的文档写得更适合 
LLM，比如 Vercel 和 Stripe 就是其中较早尝试的企业。他们将文档用 markdown 格式
编写，而 markdown 对 LLM 来说是极其易于理解的，这非常棒。 
 
94 
特别专题 | Agentic 软件革命 
 
 
我也有一个自己的小例子：你们中有些人可能知道  Three Blue One Brown 这个 
YouTube 博主，他制作了很多非常漂亮的数学动画视频。我很喜欢他开发的 Manim 库，
曾经也想自己做一个类似的动画。 
 
95 
InfoQ 架构师2025年第一季 
 
Manim 有一份非常详尽的使用文档，但我并不想花时间去读它。我把整份文档复
制粘贴给一个 LLM，然后描述了我想要做的动画，它直接就给我生成了代码，结果完全
符合我的需求。我当时就觉得太神奇了。所以如果我们能让文档对 LLM 具备可读性，
那将会解锁大量的实际用途，这真是太棒了，应该大力推进。 
但我要强调的是，仅仅把文档格式改成 markdown 还不够，那只是最简单的一步。
我们还需要真正改写文档的内容。举个例子，当文档里写着“点击这里”，这是不行的，
因为目前的 LLM 还不能原生地执行“点击”这个动作。像 Vercel 就在逐步把文档中的
“点击”都替换成等效的 curl 命令，这样 LLM 代理就可以代表你执行了。这种做法非
常值得参考。 
此外，Anthropic 推出的 Model Context Protocol 也是一个新的协议，它试图让我们
直接与 Agent 对话，把它们当作信息操作者来对待。这类协议我非常看好。 
 
96 
特别专题 | Agentic 软件革命 
 
还有一些我非常喜欢的小工具，它们可以帮助我们以更适合 LLM 的格式 ingest
（注入）数据。比如，我去看一个 GitHub 仓库，比如 nanoGPT 这个项目，我不能直
接把整个 GitHub 页面给 LLM 分析，因为 GitHub 是为人设计的界面。但如果你把链
接从 github.com 改成 get.ingest，就可以把整个仓库的所有文件拼接成一个大文本，还
自动附上目录结构。这个文本就可以直接复制粘贴给你最爱的 LLM 来提问。 
 
更进一步的例子是 Deep Wiki（由 Devon 提供），它不只是提取原始文件内容，还
会对 GitHub 仓库进行分析，并为它生成完整的文档页面。你可以想象，将这样的内容
直接提供给 LLM 是多么高效。所以我非常喜欢这些小工具，哪怕只改一个链接地址，
就可以让内容变得 LLM 友好。 
 
97 
InfoQ 架构师2025年第一季 
当然，未来某一天（其实现在就已经在发生），LLM 可能也能“自己动手”去点击
网页上的按钮，执行一些动作。但即便如此，我仍然认为我们应该主动“迎合”它们一
部分，让它们更容易访问和理解信息。 
 
毕竟目前成本仍然较高，使用也并不简单。大多数软件项目并不具备“数字基础设
施”的性质，而是静态的应用或页面，这些项目并不会主动适配 LLM。所以我们仍然需
要各种工具来“桥接”这种能力落差。而对于其他更动态、更重要的服务，我认为很值
得与 LLM “在中间相遇”。 
所以我对这两条路径都很乐观（希望这句话听起来合理）。总之，现在是进入这个
行业的黄金时代，我们需要重写大量的代码。而这些代码会由专业开发者来写，也会由 
LLM 本身来写。 
我认为，LLM 就像一种基础设施，就像“半导体工厂”或操作系统，但我们目前处
于“1960 年代”的阶段，才刚刚开始。这些 LLM 更像是“有缺陷的人类灵魂”，我们
必须学会与它们协作。而为此，我们需要调整自己的软件和系统结构。 
 
98 
特别专题 | Agentic 软件革命 
 
所以当你在构建 LLM 应用时，我前面介绍了一些能让你与 LLM 高效合作的方法、
工具，以及如何快速启动迭代循环，打造出 MVP（最小可行产品）。当然，也还有很
多代码是要专门为这些 Agent 写的。 
 
回到那个“钢铁侠战衣”的比喻，我认为未来十年我们会不断“把滑块往右推”，
Agent 会更智能，我们的工具也会更强大。我真的很期待看到这个过程会如何展开，也
很期待能和大家一起共建这个未来，谢谢大家！ 
 
99 
InfoQ 架构师2025年第一季 
 
参考链接 
• https://www.youtube.com/watch?v=LCEmiRjPEtQ 
 
100 
访谈文章 | Interview 
吴恩达评 Agent 现状：MCP 还欠火候，单 
Agent 跑通已是“奇迹”，A2A 协作堪称“双
重奇迹” 
 
前几天，吴恩达与 LangChain 联合创始人 Harrison Chase 展开了一场对话，而这场
对话的背景，正是当前 AI 领域既充满机遇又挑战重重的一个现实。 
过去几年，AI 工具公司构建出一套功能强大、模块丰富的工具体系。LangGraph、
RAG 等组件就像乐高积木，让开发者可以灵活拼装、快速搭建系统。但在真实场景中，
往往会卡在某个细节模块，比如上下文管理或评估逻辑。有经验的人能迅速换个解法几
天解决，没经验的可能要多绕几个月的弯路。AI 开发的“残酷”之处也在于此——没有
哪一个工具能包打天下，关键在于是否熟练掌握并高效组合整套工具链。 
编译 宇琪、Tina 
 
101 
InfoQ 架构师2025年第一季 
另一方面，工具之间的变化也很快。例如，随着 LLM 的上下文长度持续增加，一
年半前的很多 RAG 最佳实践，今天可能就不适用了。而 MCP 的出现则补上了另一个
明显的市场空缺，让工具、API、数据源之间的集成变得更容易。但正如吴恩达所言，
MCP 仍处在“蛮荒阶段”——网上有很多服务端实现，但“很多其实跑不起来”，身
份验证和 token 管理也尚不成熟。 
而且， 在 Agent 与 Agent 通信方面，吴恩达坦言，如今大多数人（包括他本人）
仍在努力让一个 Agent 正常运行；而要让两个不同人的 Agent 成功协作，则几乎像是
完成了两个奇迹。 
我们翻译了这场对话，以帮助更多中文开发者深入理解吴恩达对 Agent 构建路径、
MCP 现状、工具组合能力等核心问题的最新判断与实践思路。 
Agentic 架构核心在任务分解与流程编排 
Harrison Chase：你提议我们不去纠结一个应用是不是“Agent”（代理），而是
去关注它有多“Agentic”（具备代理性）。能不能再解释一下这个观点？ 
吴恩达：我之所以提出这个观点，是因为我发现大家在不停地争论：“这个是 
Agent 吗？”“这个不算吧？”——各种不同的定义争议：它够不够自主？是不是符合
某个标准？ 
我当时的感觉是，与其花那么多时间争论这个是不是 Agent，不如我们整个社区换
个方式思考：把“Agenticness（代理性）”看作一个光谱——有些系统代理性强，有些
弱。 
你想做一个稍微具备一点自主性的 Agentic 系统，或者一个非常自主的系统，都是
可以的，没必要非得争论“这算不算 Agent”。 
所以我提议，我们就叫这些系统“Agentic systems”，然后专注于怎么构建它们。
这种思维方式，其实帮我们节省了大量争论时间，让我们能更快进入实操阶段。 
Harrison Chase：你怎么看这个光谱——从“低自主性”到“高自主性”？现在
大家在构建系统时，大多处在哪个位置？ 
 
102 
访谈文章 | Interview 
吴恩达：很多企业里的实际工作，是让员工在网页上填写表单、搜索信息、查数据
库有没有合规风险、判断是否可以向某些客户销售某类产品；或者是复制粘贴一些数据，
再做个搜索，再粘贴到另一个表单中。这些业务流程，往往是非常线性的，偶尔出现一
点小循环或小分支，通常意味着流程失败，比如因为某个条件不满足被拒绝。所以，我
看到大量的机会，都来自这些简单流程。 
而我也注意到，企业在把已有流程转变为“Agentic workflow（代理性工作流）”时，
仍面临很大挑战：比如，你应该把流程拆分到什么样的粒度？任务要分成哪些微步骤？
当你构建了一个原型，但效果不够好时，你要改进哪一个步骤才能提升整体效果？这种
“从复杂任务中拆解出可执行的微步骤”、设计工作流结构、加评估机制等能力，其实
现在还比较稀缺。 
当然，更复杂的 Agentic 工作流也非常有价值，尤其是包含大量循环的那些。但就
“数量”来说，现在的机会，还是主要集中在这些更简单的线性流程里，大家正在一步
步把它们系统化、自动化。 
Harrison Chase：你做了很多深度学习相关的教学，也有很多课程是为了帮助大
家理解和构建 AI Agents。那么你觉得对于 Agent 构建者来说，哪些技能是最应该掌
握的？ 
吴恩达：我觉得最大的挑战在于：很多业务流程里，牵涉到的是合规、法务、人力
等团队的具体操作。那你要怎么构建一个“管道”把这些流程数字化？是用 LangGraph 
做集成？还是看看 MCP 是否也能帮助实现？ 
一个很重要但常被忽略的点是：要搭建一个正确的 Eval（评估）体系，不只是评估
整个系统的效果，还要能追踪每一步骤，这样你才能快速定位“是哪一步坏了”，“是
哪个 Prompt 没有发挥作用”。很多团队在这个过程中可能进展比应有的慢——他们一
直靠人手评估，每次改完 Prompt，就一个个看输出，人工判断，这会极大影响效率。 
我认为，构建系统性评估机制是最理想的方式。但问题是，很多团队还没有这种
“下一步该做什么”的直觉。技能不足的团队经常会走进“死胡同”——比如花几个月
去优化一个永远也做不好的组件。而经验丰富的团队会说：“这个方案我们放弃吧，换
一条路线。” 
 
103 
InfoQ 架构师2025年第一季 
我希望我能总结出更高效的方式，教会大家这种“经验性判断”，因为很多时候你
必须在几分钟甚至几小时内，看着 LangChain 的 Trace 输出、判断当前状态，然后迅
速做出决策，而这仍然是非常困难的。 
从工具到体系：AI 系统构建进入模块化时代 
Harrison Chase：你认为这种“经验性判断”，更多是和 LLM（大模型）本身的
限制有关，还是更偏向产品结构、任务拆解这些“构建能力”？ 
吴恩达：我觉得两者都有。过去几年，AI 工具公司构建了一套非常强大的工具体
系，包括 LangGraph 在内。你可以思考如何实现 RAG，如何构建聊天机器人，如何做
记忆系统、构建 Eval、加上 Guardrails 等等。 
我经常会用一个类比：如果你手上只有一种颜色的乐高积木，比如只有紫色的，你
其实很难拼出复杂的结构。但现在我们有了更多类型的“积木”工具：红的、黑的、黄
的、绿的，各种形状和功能。你拥有的积木越丰富，组装成复杂结构的能力就越强。 
我们提到的那些 AI 工具，它们其实是不同形状的“乐高积木”。在构建系统时，
你可能就正好需要那个“弯曲奇怪形状的那一块”，有经验的人知道用哪一块，能迅速
完成任务。但如果你从没做过某种类型的 Eval，可能会因此多花三个月时间，走弯路。
而有经验的人会直接说：“我们用 LLM 做 Judge，评估方式换成这个，三天就能搞
定。” 
这也是 AI 比较“残酷”的地方之一——它不是一个工具能解决所有问题。写代码
时，我自己也会用一堆不同的工具。我不能说自己精通每一个，但我熟悉足够多，可以
快速组合。而且，工具之间的变化也很快。例如，随着 LLM 的上下文长度持续增加，
一年半前的很多 RAG 最佳实践，今天可能就不适用了。 
我记得 Harrison 在这方面很早就开始探索了，比如早期的 LangChain RAG 框架、
递归摘要等。而现在，由于上下文窗口扩大了，我们可以往里面直接塞入更多信息。
RAG 并没有消失，但调参难度已经大大降低——现在有一大段“都还行”的参数区间。 
所以，随着 LLM 的持续进化，我们两年前的一些直觉，有些可能就已经不再适用
了。 
 
104 
访谈文章 | Interview 
Harrison Chase：有哪些“乐高积木”组件是现在被低估了，但你会推荐大家去
关注的？ 
吴恩达：虽然大家现在都在谈论评估这件事，但很多人其实并没有真正去做。我不
太明白为什么不做，可能是因为大家通常认为写一个评估系统是一项巨大而严谨的任务。
但我自己是把评估当成一个可以在 20 分钟内快速搭起来的小工具，可能做得不够完美，
但可以作为我人工 Eyeball（眼球）评估的补充。 
经常会发生这样的事：我构建了一个系统，然后某个问题不断出现。我以为修好了，
它又坏了，再修好，又坏了。这时候我就会写一个非常简单的评估，可能只包含五个输
入样例，用一些非常基础的 LLM 作为评审，只针对这个特定的回归问题做检测——比
如“这个地方是不是又坏了”。我并不会完全用自动化评估取代人工评估，还是会亲自
看输出。但这个简单的评估可以帮我减轻一点负担，自动跑一下，让我不用每次都手动
去检查。 
然后会发生什么呢？就像我们写论文一样，一开始只是搭一个非常简陋、明显有缺
陷的评估系统。但等你有了初版，你会想“其实我可以改进它”，然后就开始迭代优化。
很多时候我就是从一些糟糕透顶、几乎没什么帮助的评估开始做起的。然后随着你查看
它的输出，你会发现“这个评估系统是坏的，但我可以修好它”，就慢慢让它变得更好。 
另一个我想提的点是，虽然大家也讨论得挺多，但我觉得还远远被低估的，是 
Voice stack（语音技术栈）。这是我非常感兴趣的领域，我身边很多朋友也很看好语音
应用。我们也看到不少大型企业对语音技术极其感兴趣，而且是规模很大的企业、使用
场景也很大。 
虽然这个社区里也有一些开发者在做语音，但开发者的关注度相比这些企业的关注
度还是小得多。而且我们说的也不仅仅是实时语音 API，也不只是 Speech-to-text 那类
原生音频模型——因为那种模型往往很难控制。我更喜欢一种基于 Agentic 工作流的语
音技术栈，它更容易控制。我最近在和很多团队一起做语音栈相关的项目，有些希望很
快能对外公布。 
还有一个可能不算“被低估”，但我认为更多企业应该去做的事情是——让开发者
使用 AI 辅助编程。很多人应该都见过，使用 AI 辅助的开发者效率远远高于不使用的
 
105 
InfoQ 架构师2025年第一季 
开发者。但我还是看到很多公司，尤其是 CIO、CTO 们，还制定了一些政策，不允许工
程师用 AI 编程工具。我知道有时也许是出于合理原因，但我觉得我们需要尽快突破这
个限制。坦白讲，我和我的团队，已经完全无法想象在没有 AI 帮助的情况下写代码了。
但现在还有很多企业需要接受和适应这一点。 
还有一个被低估的观点是，我觉得“每个人都应该学一点编程”。我们 AI Fund 的
一个有趣事实是：我们公司每个人都会写代码，包括前台接待、CFO、法务总顾问……
所有人都会写。不是说我希望他们成为软件工程师，但在自己的岗位上，他们通过学一
点点代码，能够更清晰地告诉计算机他们想做什么。这带来了各个非工程岗位的显著生
产力提升，这个现象我也觉得挺令人激动。 
Harrison Chase：说到 AI 编程，你自己现在在用什么工具？ 
吴恩达：我个人现在会用 Cursor、WindSurf，还有一些别的。 
语音交互关键是对“延迟”的要求 
Harrison Chase：如果现在有人想进入语音这个方向，而他们之前已经熟悉了用 
LLM 构建 Agent，那你觉得他们的知识迁移性有多强？有哪些是相通的？又有哪些是
全新的需要学习的？ 
吴恩达：我觉得有很多场景语音其实是非常关键的，它带来了某些新的交互方式。 
从应用层面看，文本输入其实是一个“令人畏惧”的交互方式。你去跟用户说，
“告诉我你的想法，这是一个输入框，写点文字吧”，很多人会觉得压力很大。而且文
字输入还能退格，用户回复速度就更慢。但语音就不一样了：时间是往前推进的，你说
了就说了，也可以临时改变主意，比如说“我改主意了，忘了我前面说的”，模型其实
处理这些的效果还不错。所以很多时候，语音可以降低用户使用门槛。我们说“说说你
的想法”，用户就自然地开口了。 
语音系统和文本系统最大的区别在于对“延迟”的要求。如果用户说话了，系统理
想中需要在一秒内做出回应（最好是 500 毫秒以内，但至少不能超过一秒），但传统 
Agentic 工作流可能需要几秒钟甚至更久的处理时间。比如我们在做一个“我的虚拟分
身”项目，你可以在网页上和自己的虚拟形象对话。我们最初版本有 5~9 秒的延迟—
 
106 
访谈文章 | Interview 
—你说完话，沉默九秒钟，然后分身才回答，这是非常糟糕的体验。 
后来我们做了一些“预回应”设计。比如你问我一个问题，我可能会先说：“嗯，
这是个有意思的问题”或者“让我想想”。我们就让 ARM 模型去做类似这样的回应，
用来掩盖延迟，这招效果很好。还有一些其他小技巧，比如说，如果你做的是语音客服
机器人，在等待期间播放背景音（比如呼叫中心的噪音），而不是完全的静音，用户就
会更容易接受系统的“迟钝”。 
而且在很多应用中，语音让用户更容易进入状态，降低了“自我审查”的门槛。人
们说话的时候，不会像写字那样追求完美。他们可以随便说说，改口、反复、自由表达
——这让我们更容易从他们那里获取有用信息，也能更好地帮助他们推进任务。 
“如果说 MCP 还早期，那 Agent 间通信就更早期了” 
Harrison Chase：你认为 MCP 在应用构建方式、类型上，带来了哪些变化？你
怎么看它对整个生态的影响？ 
吴恩达：我觉得 MCP 非常令人兴奋。 
我个人非常喜欢 MCP，它补上了一个明显的市场空缺，而 OpenAI 的快速跟进也
说明了这个标准的重要性。我觉得 MCP 标准未来还会不断演进，目前它主要让 Agent 
更容易接入各种数据，但其实不只是 Agent，很多其他软件也可以受益。 
我们在用 LLM 的时候，尤其在构建应用时，往往会花很多时间在“管道”上——
也就是各种数据接入工作上。尤其是在大企业环境下，AI 模型其实已经很聪明了，只要
给它正确的上下文，它就能做出合理的事情。但我们往往要花大量时间处理接入工作，
搞清楚怎么把数据喂给模型，才能让它输出你想要的东西。MCP 正是在这方面起到了
很大的标准化作用，它让工具、API、数据源之间的集成变得更容易。 
当然，现在 MCP 还是有些“蛮荒”。你在网上能找到很多 MCP 服务端，但很
多其实跑不起来。身份验证系统也很混乱，就算是一些大公司，MCP 服务也存在 to-
ken 是否有效、是否过期等问题。 
此外，我觉得 MCP 协议本身也还很早期。现在的 MCP 会返回一个很长的资源列
 
107 
InfoQ 架构师2025年第一季 
表，未来我们可能需要某种分层发现（hierarchical discovery）机制。比如你要构建一个
系统——我不知道将来会不会有 LangGraph 的 MCP 接口——但像 LangGraph 这样的
系统，有成百上千个 API 调用，你总不能把所有调用都塞进一个扁平列表里让 Agent 
去自己筛选。 
所以我们可能需要一种层级式的资源发现机制。我觉得 MCP 是个非常棒的第一步。
我非常鼓励大家去了解它，它可能真的会让你的开发更轻松，尤其是如果你能找到一个
稳定好用的 MCP 服务端实现来帮你做数据整合的话。 
我也认为，从长远看这点非常重要——如果你有 n 个模型或 Agent，要接入 m 个
数据源，那你不该为了每一个组合都单独写接入逻辑，工作量不应该是 n × m，而应
该是 n + m。而我觉得 MCP 就是朝着这个方向迈出的非常棒的第一步。它还需要继续
演化，但的确是个好起点。 
Harrison Chase：还有另一种协议，虽然不像 MCP 那么热，但也值得关注，就
是 Agent 与 Agent 之间的通信。那么你怎么看 Agent 与 Agent 通信的发展？ 
吴恩达：现在的 Agent AI 依然非常早期。我们大多数人，包括我自己，在让自己
的代码正常运行这件事上都还在挣扎。所以要让我的 Agent 和另一个人的 Agent 正常
协作，就像是实现了两个奇迹。 
目前我看到的情况是：一个团队内部自己构建的多 Agent 系统，是可以运转起来
的。因为大家都在同一个团队，知道协议、约定、接口是什么，也知道怎么打配合——
这样就能跑起来。但要让一个团队构建的 Agent 能和另一个完全不同团队的 Agent 
协同，现在来看，还太早。我相信我们终究会实现这一点，但就我自己目前观察到的，
还没有看到太多真正成功、规模化运行的案例。不知道你们是不是有类似的观察？ 
Harrison Chase：没错，我同意你的看法。如果说 MCP 还早期，那 Agent 间
通信就更早期了。 
用“vibe coding”编程累死我了 
Harrison Chase：你怎么看待 vibe coding（氛围编程）？它和传统编程相比是
否是一种新的技能？它在当今世界中起到什么作用？ 
 
108 
访谈文章 | Interview 
吴恩达：我觉得我们很多人现在编程的时候几乎不再看代码了，这其实是一种非常
棒的进展。不过我觉得“vibe coding”这个名字挺不幸的，因为它会让很多人误解，以
为这件事只是“靠感觉”——比如这个建议我接受，那个我拒绝，仅凭直觉判断。 
但说实话，当我花一天时间用这种“vibe coding”方式，也就是借助 AI 编码助手
工作后，我通常会感到非常疲惫。这其实是一种非常需要智力投入的活动。所以我认为
虽然这个名字不好，但这个现象是真实存在的，而且它的确在发展，而且是件好事。 
过去一年里，有一些人在建议别人“不要学编程”，理由是 AI 会自动帮你写代码。
我认为未来回头看这将会是史上最糟糕的职业建议之一。如果你回顾过去几十年的编程
发展历史，每一次编程门槛降低，都会让更多人开始学习编程。比如从穿孔卡片转向键
盘和终端，或者从汇编语言过渡到 COBOL，我甚至找到了一些非常古老的文章，当时就
有人声称，“我们有了 COBOL，就不再需要程序员了”。但事实是，每次编程变得更简
单，学习编程的人反而变多了。 
所以我认为，AI 编码助手也将推动更多人学习编程。而且，未来最重要的技能之一，
无论是对开发者还是非开发者来说，都是“清晰准确地告诉计算机你想做什么，让它替
你完成”这件事。 
想要做到这一点，了解一些计算机的基本工作原理其实非常有帮助。我知道你们在
座的很多人已经理解了这一点。但这也是我为什么一直建议大家至少学会一门编程语言，
比如 Python。 
也许你们有人知道，我自己是一个 Python 能力比 JavaScript 更强的人。但在使用 
AI 编程助手之后，我写了比以往更多的 JavaScript 和 TypeScript 代码。即使是调试那
些 AI 帮我生成、而不是我亲手写的 JavaScript 代码时，理解其中的错误类型和含义，
对我来说仍然非常重要，帮助我去修复它们。 
胜负决定于速度和技术能力 
Harrison Chase：你最近宣布了一个新基金——AI Fund 的新进展，对于在座有
创业想法的人来说，你有什么建议？ 
吴恩达：AI Fund 是一家 Venture Studio（风险投资孵化器），我们不仅投资公司，
 
109 
InfoQ 架构师2025年第一季 
而且只投资我们共同创办的公司。 
回顾 AI Fund 的经验，我觉得创业成功的首要预测因素就是速度。虽然我们身在硅
谷，但我发现很多人其实从未真正见识过一支高效团队可以有多快地执行。如果你见过
的话，你会知道，这种速度是传统企业完全想象不到的。 
第二个关键预测因素是技术能力。虽然像市场营销、销售、定价这些商业技能也很
重要，而且相关知识已经积累了很久、相对普及，但真正稀缺的资源是“技术理解力”
——因为技术在快速演进。我对擅长 go-to-market（商业推进）的人非常尊重，定价很
难，营销很难，产品定位也很难，但这些知识是更容易被学习到的。真正稀缺的是那些
真正懂技术的人，知道什么该做、什么不该做、怎么可以让事情加速两倍。所以 AI 
Fund 非常喜欢和技术背景深厚的人合作，尤其是那些对方向有直觉判断的人。而商业
相关的能力当然也很重要，但它们相对更容易补足。 
参考链接 
• https://www.youtube.com/watch?v=4pYzYmSdSH4 
 
110 
访谈文章 | Interview 
“我已经过时了”！83 岁图灵奖大师、龙书
作者在大模型时代的技术焦虑：越来越难以适
应新技术 
 
在计算机科学领域，Jeffrey Ullman 是一个无法忽视的名字。这位 83 岁的斯坦福大
学荣誉教授，既是《编译器：原理、技术和工具》（俗称“龙书”）的合著者、数据库
理论的奠基人，也是 2020 年图灵奖得主。他的职业生涯贯穿了计算机科学发展的关键
时期——从编译器开发到数据库理论构建，再到算法研究突破，他的工作深刻影响了计
算机科学的发展，尤其通过《编译器：原理、技术和工具》和《数据库系统原理》等经
典教材，塑造了无数程序员的思维方式。 
前几天，这位白发苍苍的学者在一次采访中展现出了令人敬佩的坦诚。他在对话中
编译 核子可乐、Tina 
 
111 
InfoQ 架构师2025年第一季 
多次提到，技术是年轻人的游戏，而随着年龄的增长，他自己也越来越难以适应新技术。
例如，他谈到使用大模型时自己不太理解其逻辑，以及在使用手机导航和连接车载系统
等日常技术时遇到的挑战。这番感慨恰如其分地映照出当下技术变革的独特性：当一位
曾定义过“技术标准”的宗师级人物，也开始在 AI 浪潮中重新寻找方向，这本身便成
为了关于技术进化的深刻隐喻。 
 
在本次专访中，Ullman 教授还系统回顾了自己跨越半个多世纪的学术生涯，分享
了他对技术迭代的思考，并强调了技术发展的不可预测性。他指出，许多重大的技术变
革在出现之前常常是无法预见的。举例来说，1992 年他与其他学者讨论“信息高速公
路”时，几乎没有人提到万维网，而当时瑞士的万维网技术已经出现。这表明，许多技
术突破往往在专家和业内人士未曾意识到之前悄然到来。 
在展望未来技术时，他说软件工程领域最大的转变是从机器语言到高级语言（如 
Fortran）的演变，而如今大模型直接生成代码，这一变革的意义甚至超过了当初的变化。
他认为，软件工程正在经历从“算法”到“算法+数据”的转变，并行化的引入也肯定
会逐渐改变我们对计算机能做什么的思考方式。 
并行计算对计算机科学的深远影响 
主持人：很高兴与您交谈。我知道您有着丰富而多样的职业经历。您最近又有什么
新见闻？我注意到您说您所做的很多事情都已经是“老古董”了。那您如何描述当下的
自己？ 
 
112 
访谈文章 | Interview 
Jeffrey Ullman：我基本上已经退休了，已经是个“过气”人物了。没错，我们总
得面对现实，计算机科学是年轻人的游戏，而我已经不再年轻了。 
主持人：这话也对。不过，我认觉得您所做的一些事情在这个快速变化的领域中仍
然有着惊人的生命力。当我向别人提到“我将要采访 Jeffrey Ullman”时，他们立刻
就会说：“哦，〈龙书〉，我知道〈龙书〉。”我想花几分钟时间谈谈这本书。虽然它
有些年头了，但当我与新的计算机科学毕业生交谈时，他们仍然在谈论《龙书》。您觉
得这本书为什么会如此成功？ 
 
Jeffrey Ullman：我想答案就在名字里。它的封面很酷。很明显，人们喜欢拿着这
本印有龙图案的书在校园中走来走去，甚至因此而选择计算机科学专业。这可能就是我
一生中做出的最大贡献——让一些原本可能浪费生命在物理学等领域的聪明孩子转而投
身计算机科学。 
主持人：说起编译器，我知道在您开始研究时，那会编译的语言可能要比今天的主
流语言简单得多。语言设计和编译器方面也经历了一系列发展。多年来，您在语言设计
和编译器方面有没有发现什么特别有趣的变化？ 
 
113 
InfoQ 架构师2025年第一季 
Jeffrey Ullman：我觉得最大的变化在于，我们现在主要是为并行机器编译，包括
八核处理器啦、大型超级计算机之类。这些问题在《龙书》的最后一版中才开始被提及，
主要是因为 Monica Lam 专门研究这个领域——也就是并行编译。但在我们撰写前三版
时，甚至都没有真正考虑过这个问题，可能当时这确实也不值得考虑。毕竟那会是 
1977 年，并行编译真的没那么重要。 
主持人：是的。而且我在研究您的职业生涯和著作时发现了一个贯穿始终的主题，
您会从基本的理论抽象开始思考，考虑不同层次的抽象，然后将它们应用到实际问题中，
从非常抽象的理论方法逐步深入到更高层次的上下文或实现细节。我很好奇，随着我们
向更多的并行计算转变，我们所需要的抽象是否发生了变化？ 
Jeffrey Ullman：当然，如果是为串行机器编译，就不需要处理并发控制等问题的
任何抽象。我认为 MapReduce 就是并行计算的一个有趣抽象，这些东西在 20 世纪 
70 年代是没有任何意义的，也完全没有任何实用价值。 
主持人：没错，MapReduce 是一个很好的过渡——我看到您最近的一些工作确实
跟大规模数据挖掘和处理网络规模的数据有关。您在这个领域也写了一本书，其中将 
MapReduce 作为分析案例。随着我们不断扩展规模上限，您认为还有哪些关键部分被
整合进去了？ 
Jeffrey Ullman：很明显，将世界上所有的数据整合在一起给了我们一种惊人的力
量，这是之前从未有过的。 
我指的自然就是大语言模型。当然，其中仍然有一些粗糙的细节需要打磨。但无论
如何，它已经赋予了人们做出各种奇妙事情的力量。同样的情况也发生在 5 到 10 年
前，那时候首次出现了大规模神经网络。 
让它们发挥作用的原因不仅是出现了廉价的大规模计算设备，还有大规模数据集的
诞生。当然，人们常常忽视数据的重要性。这不仅仅是算法的问题，数据同样重要。比
如我听说人们现在担心大语言模型已经达到了极限，因为我们已经吸收了所有人类创作
的文字，再也没有更多素材了。 
这个问题很有意思，我估计会在几年内看到这种观点是否属实，或者说，大家会想
 
114 
访谈文章 | Interview 
办法制造从未真正存在过的数据。比如说，如果你想通过机器学习来识别肿瘤，你可以
拍摄一张肿瘤图像，将其放大 10% 或旋转 10%，从而创建额外的可用数据，而无需实
际拥有更多数据。总而言之，这是一个值得关注的方向。 
主持人：是的，我认为这里的关键在于，有没有用到底该如何定义。在图像示例中
这应该是成立的，因为可以推理出在计算机看来不同的东西，而我们又可以验证它们是
相同或说等效的。但在其他领域，情况会不会有所变化？ 
Jeffrey Ullman：说得对，而且我也没有答案。就像我前面说的，这是个有趣的问
题。我相信人们现在正在朝这个方向研究。 
主持人：是的。既然谈到了这一点，我想到您曾经写过关于在处理这些大规模数据
情况时降维的重要性。我在想，这是否与人们在蒸馏模型时试图提取相关维度的方式有
关。 
Jeffrey Ullman：说实话，我不记得自己有写过这方面的内容…… 
主持人：可能是在您的书里…… 
Jeffrey Ullman：《数据集》这本书确实讨论了降维。在理解数据时，降维通常是
一个有用的工具，因为对于某些非常复杂的高维数据，有各种技术帮助我们聚焦于真正
重要的部分。 
主持人：没错。说回抽象视角，您提到了大语言模型存在一些粗糙的细节。您觉得
具体有哪些挑战？作为软件工程师，我们在使用这些东西时的关键抽象是什么？或者，
我们是否遗漏了一些需要弄清楚的抽象元素？ 
Jeffrey Ullman：这是个好问题。我觉得我们还没有真正找到该如何使用大模型的
终极答案。 
有趣的是，斯坦福大学从来没开设过如何使用谷歌搜索、如何创建搜索查询的课程。
但现在我们正在教授一门名为“提示词工程”的课程。许多学校可能都在开设或将会开
设这样的课程。就是说“提示词”这个概念似乎得到了广泛认可，而且也许需要被抽象
化或以某种方式进行理解。但无论如何，我还没看到能切实指导我们使用提示词的具体
 
115 
InfoQ 架构师2025年第一季 
原则。 
我自己也摆弄过提示词。在职业晚期，我做过大量的编辑工作。我为一家期刊工作，
那里有 300 名编辑，而我是唯一的计算机科学编辑，所以很多工作都堆到了我这里。 
我发现自己根本读不懂眼前的内容，也没有人可以交流。所以我开始使用各种大语
言模型。 
我会先向模型提供优秀审稿人的标准，然后说：“这是论文摘要。请推荐一些理想
的审稿人选。”有时，它确实能准确推荐，并介绍一些我从未见过但实际上相当先说的
人选。也有时候，它会坦率地讲：“这对我来说太难了，您可以参考以下审稿人筛选方
法。首先阅读文献，然后……”这基本上是一个完全无用的评论。 
坦白说，我不知道为什么会出现这样的差别——是摘要的详细程度，还是我对想要
的东西的定义方式？我不知道。这里有很多非常神秘的东西，我认为还有待人们去探索。 
主持人：一点没错。我觉得这里机会很大。我见过一款工具，尝试像简单的编译器
那样编译提示，其中包含不同的层次，可以看到它将组件组合并整理成不同的风格，以
此适应不同的大模型。我猜它并没有做任何优化，只是专注于如何从高级组件集转换为
适用于 Anthropic 与 ChatGPT 的提示词形式。但我认为，它其实已经是某种类似于编
译器的等效物，它能理解提示这些不同模型的正确方法，并将需求意图跟更好的提示方
式映射起来。 
Jeffrey Ullman：是的。这是个不错的研究课题。 
自动化与教师角色 
主持人：这会是个不错的研究课题。说到研究，您提到最近自己一直在积极从事一
个不同的项目。能给我们多透露点细节吗？ 
Jeffrey Ullman：我想想哈。再次强调，我希望自己在晚年也能有点事干。我想大
概是在 2000 年初，我和几个朋友创办了一家名叫 Gradiance 的小公司。它的目标是实
现家庭作业自动化，我们采用的基础架构是：对学生来说，它看起来像是一系列的简短
选择题，整个解题过程就是从四个选项中选择正确的答案。 
 
116 
访谈文章 | Interview 
但跟常规的选择题不同的是，我们希望家庭作业实现的不仅是测试，还要进行教学。
如果你答错了，我们会给你一个提示，并要求你重新做一遍整个过程。传统选择题的问
题是，如果你猜 A，但结果是错的，然后猜 B，最终总会在不付出任何努力的情况下得
到正确答案。我们的解决方法是开发了所谓的“根问题”，即拥有多个正确答案的问题。 
那这个思路是如何实现的呢？比如，问题是解方程 2x 加 5 等于 10，求 x。现在，
这只有一个正确答案，应该是 2.5。看起来只有一个正确答案。但事实上，我们要求学
生做的是提供解题过程。你要把步骤写下来。如果你答对了，就是 x 等于 2.5。所以我
们提供的选项不是 x 的值，而是告诉关于计算 x 的一些正确信息。 
比如你可以初步得出一个正确答案，即 x 不是整数，或者 x 小于 3，或者 2x 是
质数。如果你知道 x 是什么，就很容易认出这些正确答案。如果你不知道 x 是什么，
你就只能猜测。我们认为可以通过出售这项服务赚很多钱，但实际上商业反响并不好。
其实大家还是愿意使用，目前的用户也不少。比如开罗大学就是我们最大的用户，但他
们不会为此付费。 
大概是两年前，我在班加罗尔做过一次关于这项技术的演讲，听众中有一位 Infosys 
的创始成员，他非常兴奋，说：“我们可以用这个来教数学，”其他人也可以。看看 
Gradiance 的网站，就会发现这里在用同样的方法教授计算机科学，从 Java 编程到编译
器、再到操作系统和数据挖掘。 
但项目的商业化还是决定从数学入手。于是他筹集了一些资金，组建了一支团队。
产品还在制作当中——到目前为止，我们一直针对九年级的数学课程。顺便说一下，印
度的九年级数学大多是我十年级和十一年级学的东西。但这个不重要。总而言之，我们
希望这些素材能在印度乃至世界各地供大家免费使用。 
同样的，这个想法不仅是要确保学生掌握了材料，还要在他们遇到困难并给出错误
答案时，提供相关的提示或解释。我们希望这最终能帮助他们答对所有的问题。 
主持人：是的。我觉得现在人们非常关注广泛的 AI 应用领域，即我们如何扩展教
育规模，让学生们按照自己的节奏学习，而不过分依赖个别教师。 
Jeffrey Ullman：这确实是个有趣的点。同样的，大约在我们创办 Gradiance 的同
 
117 
InfoQ 架构师2025年第一季 
时，MOOCs（大型开放在线课程）风靡一时，人们甚至觉得以后大学都没必要存在了。 
后来怎么样了？我认为回答这个问题很重要。关键在于：当通用汽车决定用机器人
取代装配线工人时，没有人会问装配线工人是否相信机器人能够胜任工作，对吧？但当
学术界引入替代方式，也就是把技术引入教学流程时，教师却完全有权说：“不，不应
该这样。我不想在自己的课堂上用这种技术。”这跟其他产业应用完全不同。 
所以结果自然很糟糕。技术的应用不仅遇上了明显的阻力，特别是那些工作受到威
胁的人。 
而且事实证明，对于 95% 的学生来说，MOOC 并不足以达成教育目标，因为人会
需要帮助。也就是说，我们只能自动化其中一部分。我认为 Gradiance 做到了一点，但
人们遇到困难时总还是需要求助。MOOC 无法消除对教学专业人员的需求。它或许可以
减轻教师们的一些负担，从而提高他们的效率，这意味着整个行业对教师的需求量会减
少。生活就是这样，每个领域都是如此。 
我觉得我们还可以做更多——如果教师们愿意，比如说播放 MOOC 视频课，就像
他们过去会选定教学大纲一样。教科书和教学大纲承担了一部分教学工作，现在 MOOC 
可以承担更多的教学工作，但仍然需要有人来主持课堂，特别是处理不断出现的各种特
殊情况。当然，还有验证的问题。比如学生可以注册一个 MOOC 账号，然后让家里的
哥哥姐姐帮自己完成所有作业，没有人会知道。单纯注册了某门课程并完成了 MOOC 
的测验，并不代表你自己有解决问题的能力。 
好吧，其实也有相应的技术方案——比如要求学生们接受指纹验证等等。但这不是
长久之计。教师的意义就在于随时帮助学生解决他们遇到的各种问题，确保大家能够得
到帮助和引导。我觉得这样的教学方式是不会改变的。所以虽然每个环节都有相应的技
术方案，但我也完全能够理解为什么学校对于教学自动化会比较抗拒。 
技术适应的代际挑战：从提示工程到人机交互新挑战 
主持人：下面咱们聊点同样跟学习相关，但又有点不同的方向。您谈到了斯坦福大
学现在开设提示词定义、或者叫提示词工程课程。 
Jeffrey Ullman：行业术语应该叫提示词工程。 
 
118 
访谈文章 | Interview 
主持人：对对，提示词工程。我们当然可以讨论这到底能不能算工程，但您谈到了
这一点。我们谈到了应用技术成果来帮助扩展学习，包括您在 Gradiance 所做的工作，
以及大型在线课程做出的相关尝试。我觉得我们现在处于一个对于技术变革有着很多抵
触情绪的时代。世界各地的人们对于大模型都有很多抵触情绪。所以我很好奇，您觉得
我们该如何让人们适应正在发生的技术变化？ 
Jeffrey Ullman：这事可不容易，我觉得恐怕得等到下一代人成长起来才行。 
主持人：您是这么理解这个问题的吗？直接放弃目前比较抗拒新技术的这一代人？ 
Jeffrey Ullman：没办法。随着我年龄的增长，我发现自己也越来越难以适应新技
术。顺便说一下，人机交互（HCI）领域已经存在了很长时间。但我没有看到任何人在
解决怎么跟老年人打交道的问题，所以我个人是比较悲观…… 
主持人：我也有体会。我的父母越来越老了——我父亲现在 80 多岁，母亲已经去
世了。老一辈人根本跟不上变化。我觉得在科技行业，从业者经常为了改变而改变，却
忽视了这样做的成本。 
Jeffrey Ullman：也不完全是为了改变而改变。比如说我就认真学过怎么用手机，
现在已经完全离不开了。我孙子已经 25 岁了，他出去遛弯时喜欢借车四处开。我习惯
用车载导航系统，但他说：“我只想用谷歌地图。只要买个适配接口，就可以把你的手
机连到车上。”我从来不知道这事，但年轻人们都很清楚，他们现在就是这么导航的。
我之前去看望我的儿子和五岁的孙女，我当时带了一个平板电脑。他们平时不让她玩平
板，但爷爷奶奶来的时候，孩子可以稍微玩一会……（这应该是爷爷奶奶的特权）。 
她想下载一个应用来画画，但应用有广告。现在这些应用的生态系统都很奇怪，因
为据我观察，所有应用的广告都是其他应用产品。这基本上就是个互相洗钱的生态系统。
我不确定这背后的商业逻辑是什么样的，但目前的情况就是这样。她在应用之间跳来跳
去。我说：“看，你得躲开这些广告，别去点。”15 分钟后，我发现她找到了自己的
方法，而且比我的办法更好。五岁的孩子很早就适应了，但我们老年人不行。 
主持人：是的。这很有趣，让我想起了我儿子小时候的情景。我们也会给他一个设
备让他玩。几分钟后，大家会说：“等等，你是怎么做到的？那是什么？” 
 
119 
InfoQ 架构师2025年第一季 
Jeffrey Ullman：没错。我觉得最重要的一点，是如何让老年人适应——我们这个
群体有自己的经验和习惯，但这些方式现在往往不好用了。所以说，开发者要怎样识别
用户的身份，推测他们认为哪些设计和功能比较符合习惯，或者通过哪些机制来帮助他
们搞清楚设备的用法。 
主持人：我觉得这非常重要，而且这是一个很大的盲点。因为正如您在谈话开始时
所说，科技往往是年轻人的游戏。做决策和负责思考这些事情的人可能自己很少与老年
人互动。即使是中年人，个体之间也有很大的不同。我 40 多岁了。我习惯了通过文本
与大多数技术产品互动。我的孩子们对一切都使用语音和视频。这让我感到惊讶。比如
想要解决一个问题，我会用文本输入、搜索相关的文章教程。但他们会搜索 YouTube 
视频。作为一个行业，尤其是考虑到我们的社会正在加速老龄化，大家确实需要弄清楚
如何纳入老年人的偏好、需求和差异。 
Jeffrey Ullman：真的很期待能看到这样的变化。 
主持人：那您有没有看到比较好的成果？ 
Jeffrey Ullman：是指相关研究还是实际应用？ 
主持人：这两类都可以算，有没有让你印象深刻的探索和尝试？ 
Jeffrey Ullman：我还没看到真正好的系统设计，比如说一种完全不同的设计思路。
但我有种感觉，这事应该有戏。 
主持人：应该是有戏。我在想，现代机器学习和大模型技术能不能帮上点忙，因为
它能向特定群体提供更多基于意图的用户交互范式，还能够随时做出解释。依靠这些技
术，我们可以推理出用户的基本意图——哪怕你的自然互动方式与我不同，与我孙女的
自然互动方式不同，它也能弄清楚你在尝试做什么并帮到你。 
Jeffrey Ullman：是的。我认为新的大模型技术应该有这种能力，毕竟它都能总结 
YouTube 视频了。 
我之前提到的案例，大模型应该就能帮上忙。比如说我的手机上有一张地图，我想
把它连接到汽车的屏幕上。我不知道该怎么操作，也许手册里有，但手套箱里那本手册
 
120 
访谈文章 | Interview 
足足 500 页，笑死根本不可能去看。它一定在某个地方告诉用户如何做到这一点，但
我甚至不知道有这回事。 
我儿子还告诉过我另一个功能，就是车子后备箱有一个按钮，可以在不按下车内的
按钮的情况下从车外打开后备箱。我想现在所有的车都有这个功能。但没人告诉过我有
这回事。 
主持人：是的。这是一个信息传播问题。只有明确知道要问这个问题，才可能找到
答案。但我们如何提出问题呢？ 
Jeffrey Ullman：完全正确。汽车应该说：“我注意到您一直在按车内的后备箱开
启按钮。您知道您可以……”我不确定大模型能不能解决这类问题。 
主持人：但这事确实有搞头，而且让我想起了您在 Gradiance 所做的工作。即如果
有基座模型，就会有很多正确答案。我们能否构建我们的技术，以注意到有一类答案一
直被用户系统地忽略或者压根没听说过，并主动展示给用户。 
Jeffrey Ullman：总之，我觉得 HCI 人员应该试试看。 
技术变革的前瞻与回顾 
主持人：是的。我喜欢跟像您这样年长且对这个行业有很多了解的人交谈，因为您
比那些只在这个行业呆了 10 或 20 年的人有更广阔的视野。您还发现我们当前的科
技生态系统和环境中存在哪些其他空缺？人们应该在哪些领域开展工作，但目前还没有
行动？ 
Jeffrey Ullman：这问题可不好回答。回顾那些巨大的变化，在有人拿出方案之前，
没人意识到这种需求。 
我举一个例子。日期非常重要。记得那是 1992 年 3 月左右，我被邀请到华盛顿
特区讨论当时被称为信息高速公路的事情。那里有一群像我这样的学者，以及电信公司
和其他行业的负责人。我们写了一份报告。有趣的是，根本没有人提到万维网。我想说
的是，那里有一些关于 GOPHER（计算机信息查找系统）的讨论；另外还有个大问题，
信息高速公路到底该通过电缆还是电话线传输。 
 
121 
InfoQ 架构师2025年第一季 
当时在瑞士，万维网已经出现。但这 300 位专家中，包括我在内，都没有意识到
这会是个选项。记得几个月后问我的一位同事，“你为什么总在说‘://’？”甚至没有
人知道需要像万维网这样的东西。 
手机、个人电脑的情况也差不多。DEC 当时的负责人有句名言：“为什么有人想在
办公桌上放一台电脑？”答案是，我真的没法预测下一件大事会是什么，甚至无法猜测。 
主持人：当然，但您已经指出了一件现在缺失的东西，那就是关注老年人的 HCI 
研究。 
Jeffrey Ullman：我认为这是一个不错的研究课题，但它倒不至于改变世界。我的
意思是，如果非要让我猜，我肯定会选量子计算。我一直对量子计算持怀疑态度。它确
实有一些应用，比如可以利用量子现象更好地模拟一些东西。顺便一提，量子通信似乎
是真实可行的，就是所谓量子隐形传态。 
人们实际上可能能够做到这样的事情，把技术理论变成现实。但人们真正期待的，
是这些量子计算机能拥有正确的量子比特，能够运行 Shor 算法，从而破解 RSA 代码
和椭圆曲线代码等。我的意思是，如果真是这样，那我们的思维方式肯定会发生转变。
但到底可不可行，着实令人怀疑。 
毕竟像 D-Wave 那样的机器，显然使用的是无法运行 Shor 算法的量子比特，而且
他们还在不断扩大规模。另外，量子计算可能并不遵循摩尔定律，只是我们还没意识到。
我们能否每两年将可用的量子比特数量翻倍，特别是那些能够真正运行 Shor 算法的量
子比特？我不知道到底行不行。可能可以，可能不行。如果再选一个，那可能是通用人
工智能。 
主持人：是的。您提到了一些过去没有预料到的技术转变。如果我们回顾软件工程
这个领域本身，而不是一般性技术，毕竟我们的大多数都是软件工程师嘛，那么多年以
来，整个行业在思考软件的方式上最大的转变或者说突破是什么？ 
Jeffrey Ullman：我当初写的第一个程序用的是种机器语言，然后我升级到了 FAP。
那是 IBM 790。然后我学会了 Fortran，大大提升了我的设计空间。我觉得现在大家都期
待大模型能直接为自己编写代码。确实，我觉得这可能会让生活变得稍微轻松一些，让
 
122 
访谈文章 | Interview 
大家更有效率一些。而且从某些证据来看，这种情况正在实惠，就像当初从 FAP 升级
到 Fortran 一样。我觉得这波变革甚至比当初更意义重大。 
正如我所说，并行性的引入肯定是个巨大的变化。只是这次的变革更微妙、更渐进，
代表着从将软件视为算法、转变为将软件视为算法加数据。这肯定会逐渐改变我们对计
算机能做什么的思考方式。 
主持人：绝对的。有人可能会说，机器学习方面的许多进步都被更多用于数据编程，
相比之下算法哪怕没有被彻底遗忘、也至少只能算“二等公民”。 
Jeffrey Ullman：因为要先有数据，之后才能谈得到算法。 
比如我们之前谈到过提示词工程。最终，我觉得这个主题将落地成将意图转化为文
本序列的算法，而文本序列在某种程度上甚至可能不再是文本的形式。比如变成视频或
其他形式。 
主持人：是的，未来会使用人类语言算法来引导大模型。 
Jeffrey Ullman：没错。从某种意义上说，这也是一种算法，或者是以其他工具的
形式。到时候将由计算机帮助用户制定提示词。与大家习惯的老办法相比，未来可能会
出现完全由计算机实现的算法。 
主持人：绝对的。我认为 AI 领域的许多发展都是围绕这些工具生成提示词，借此
帮助用户实现各种功能。非常值得期待。感谢您抽出时间参加节目，非常感谢。 
Jeffrey Ullman：也感谢你的邀请。 
原文链接 
• https://softwareengineeringdaily.com/wp-content/uploads/2025/03/SED1813-Jeffrey-
Ullman.txt 
 
123 
InfoQ 架构师2025年第一季 
“不是 Cursor 不够强，是 Claude Code 太
猛了”！创始人详解 Claude Code 如何改写
编程方式 
 
对于许多开发者来说，每月 20 美元的 Cursor 和 Copilot 已经是“无限量”好用
的标配。然而，Anthropic 的 Claude Code 却是个异类。它在处理大型代码库方面表现
相当出色，但价格却直接翻了几倍。如果你只是周末写写代码，几美元的 API key 兴许
就够了；可一旦用于日常开发，每月账单轻松就能突破 50、100 甚至 200 美元。有用
户直言不讳地指出：“Claude Code 的能力比 Cursor 更强。我还在用 Cursor 的唯一原
因，就是 Claude Code 实在太贵了。” 
价格似乎阻止这款产品爆发增长的主要因素，毕竟对比其他一票工具，Claude Code
作者 Tina 
 
124 
访谈文章 | Interview 
“真的很猛”。 尽管 Cursor 的底层大模型同样来自 Anthropic， Steve Yegge 却评价
道：“Claude Code 让 Cursor、Windsurf、Augment 这些工具看起来都像是过时产品。” 
 
我用了 Claude Code 几天，它在清理我那堆乱七八糟的旧代码里的遗留 bug 时
简直毫不留情，像一台烧着钞票运转的碎木机。只用对话，它就能完成一些令人
震惊的高难度任务。你甚至不需要手动选上下文。你只需要敞开心扉、打开钱包，
Claude Code 就会接管一切。它还会每隔八秒提醒你一遍，问能不能运行一些
只读命令——那些你连朝鲜黑客都不介意他们在你机器上跑的命令。 
 
125 
InfoQ 架构师2025年第一季 
不过你会学会盯紧它，因为它真的很猛。只要银行的扣款授权还在，它就会一直
往前推，直到 bug 修好上线，然后开始扫描用户日志，看看自己表现得怎么样。 
Claude Code 的使用体验说实话有点笨重，没有多模态支持，和其他工具配合也
不太顺手。但这都不重要。它看起来也许有点老派，但用起来却让 Cursor、
Windsurf、Augment 那一票工具（是的，包括我们自己的产品，还有 Copilot，
说实话）都像是上个时代的产物。 
我知道这还只是个实验品，我们也不知道它的极限在哪。但从我目前的体验来看，
它比自从代码助手诞生以来的任何东西，都更像是向未来迈出的一大步。 
所以，Anthropic 不仅拥有目前最适合真实开发场景的大模型，他们看起来也确
实比其他人更懂得怎么用好它。结合他们一贯拥有最强模型、最顺手的聊天界面、
CEO 最准的判断，还有现在这个 Claude Code——我已经开始怀疑，Anthropic 可
能真的是这个星球上唯一知道自己在干什么的公司了。 
最近，Claude Code 创始人 Boris Cherny 在一次采访中，难得地全面分享了他对这
款产品的定位与设计理念。他讲述了开发这款编码助手的初衷、目标用户群、定价策略
背后的考量，还分享了一些他们的使用技巧。 
同时，他也谈到了作为当前在编程领域最强大模型企业之一，Anthropic 对 Claude 
Code 的未来设想与愿景：帮助开发者从“写代码的人”转变为“判断代码是否正确的
人”。换句话说，未来的开发者不再是单纯敲键盘的人，而是技术决策的主导者。 
以下是访谈实录： 
Alex Albert：Boris，我们先从最基本的问题开始：Claude Code 是什么？它是
怎么诞生的？ 
Boris Cherny：Claude Code 是一种在终端里实现代理式编程（agentic coding）的方
法。你不需要学习新的工具、不用更换 IDE，也不必打开特定的网站或平台。它就是一
种能够在你原本工作环境中直接使用的代理式编程方式。 
这个想法，其实源于 Anthropic 的工程师和研究人员日常的工作习惯——因为每个
 
126 
访谈文章 | Interview 
人使用的技术栈都五花八门。真的非常多样，根本没有统一标准。有些人用 Zed IDE，
有些人用 VS Code，还有人坚持说：“谁也别想从我手里夺走 Vim，除非我死了。”所
以我们想做一个对所有人都友好的工具，最终选择了终端作为入口。 
Alex Albert：我明白了，终端几乎是所有界面中最通用的，它灵活，而且早就融入
大家的工作流了。 
Boris Cherny：完全正确。而且终端本身也非常简单，正因为它够简单，我们的迭代
速度就特别快。回头看，这是一个意外的优势，虽然最初并不是我们有意为之。 
Alex Albert：那如果我是一个新开发者，刚开始接触 Claude Code，我怎么样才
能真正让这个产品运行起来呢？ 
Boris Cherny：只需要从 NPM 下载：npm install -g @anthropic-ai/claude-code。安装
完之后，只要你的系统里有 Node.js，就可以直接运行了。启动后它会一步步引导你完
成剩下的配置流程。安装完后就可以直接和它对话，它就会开始写代码。 
它能在任何终端中工作，无论是 iTerm2、Mac 自带终端，还是 SSH/TMUX 会话都
没问题。很多人其实是在 IDE 内的终端里使用 Claude Code，例如 VS Code 的内置终端。
在这种情况下，你不仅能看到文件被修改，还能在 IDE 里看到它以更美观、更清晰的
方式呈现出来。我们也会利用 IDE 提供的更多信息，让 Claude 更智能。不过无论在哪
个终端里，使用体验是一致的，你只需要在终端运行 Claude 就可以了。 
Alex Albert：Claude Code 是今年 2 月发布的，到现在差不多三个月了。社区
反馈怎么样？ 
Boris Cherny：太疯狂了，完全出乎我们的意料。其实在发布之前，我们还犹豫过要
不要放出来。这个工具在我们内部用得非常多，极大地提升了工程师和研究人员的效率。
我们甚至讨论过：“这是我们的秘密武器，我们真的要公开给外界用吗？”毕竟这就是 
Anthropic 内部每天都在用的工具。 
但后来事实证明，把它发布出去是一个非常正确的决定。它确实能提升生产力，而
且大家真的很喜欢。起初只有我们核心团队的少数几个人在用，后来我们开放给所有 
Anthropic 员工使用。当时我们看到一个 DAU（每日活跃用户）图，短短三天内几乎是
 
127 
InfoQ 架构师2025年第一季 
垂直上涨状态。我们当时就觉得：“这太疯狂了，肯定是个爆款。” 
之后我们挑了一些外部用户做试点，想确认是不是我们自己太乐观了，结果收到的
反馈也都是非常积极的，那时候就很清楚了——它确实有价值。 
所以它首先在 Anthropic 内部引起轰动，所有的工程师，所有研究人员都在使用它，
这才让我们决定把它发布出去。 
而且我们整个开发过程也特别有意思：Claude Code 本身是用 Claude Code 写出来
的。几乎所有的代码都经过了多轮用 Claude Code 编写和重构。 
我们非常相信内部测试，因为这真的很重要。你能明显感觉到哪些产品是开发团队
自己每天在用的，那些产品的细节打磨都非常到位。我们希望 Claude Code 也成为那样
的产品——你一用上它，就能感受到开发者的用心。 
Alex Albert：你觉得目前 Claude Code 最理想的用户是谁？谁在用它？是怎样的
开发者？ 
Boris Cherny：我认为最重要的事情是——Claude Code 其实是相当昂贵的。 
如果你只是周末写写代码玩玩，那可以尝试下，比如你拿个 API key 充个五块钱试
试。但如果想拿它做更严肃的工作，每月大概需要花五十、一百，甚至两百美元。取决
于用途，一般而言，每月大概会花五十美元左右。 
现在其实有很多企业在用 Claude Code。对于大公司来说，它非常合适。特别是在
处理大型代码库时，它表现非常好。不需要额外做索引，也不需要复杂配置，基本上开
箱即用，几乎适用于任何语言的大型代码库。 
至于 Claude Code 跟 Claude Max 的整合，是因为我们之前发现，用 API key 支付
的用户常常会担心用量问题，这反而影响了他们使用的积极性。为了改善这个问题，我
们把 Claude Code 纳入了 Claude Max 订阅计划中。你只需要订阅 Claude Max，就能无
限使用 Claude Code，每月是 100 美元或 200 美元不等。用户可以根据需求选择不同
的价格和使用上限，但实际上很少有人能用到限制，基本可以看作是“无限量”的 
Claude Code。 
 
128 
访谈文章 | Interview 
Alex Albert：那我作为一个开发者，机器上有自己的代码库，我打开终端，输入 
claude 并回车，接下来会发生什么？ 
Boris Cherny：Claude 会调用各种工具，分步骤执行任务。如果你之前用过那种在 
IDE 里的编程助手，助手所做的就是完成一行或几行代码。我们这跟那个完全不一样。 
Claude Code 是非常“agentic”的——它会理解你的请求，然后使用它可以用的一
切工具，比如 Dash、file editing 等等，去探索代码库、读取文件，获取上下文，然后再
对文件进行编辑或做出你希望的更改。 
Alex Albert：听起来像是对过去 20 到 30 年编程范式的一种全新变革。 
Boris Cherny：对我来说，这种变化是有很深的历史背景的。我自己写代码已经很多
年了，但其实，我外祖父在上世纪 40 年代就是苏联最早的一批程序员之一。 
那时候还没有软件编程，他是用打孔卡片写程序的。在美国，当时 IBM 提供了一
套类似 IDE 的系统，他每天就用这套系统编程，把纸质卡片带回家。我妈妈小时候会
拿这些卡片当画纸，用蜡笔在上面涂涂画画，那是她童年的一部分。 
从那时起，编程不断演进：先是打孔卡，然后是汇编语言，接着出现了 COBOL、
FORTRAN 这些第一代高级语言。80 年代是 Java 和 Haskell 这类静态类型语言的时代，
到了 90 年代，我们又有了 JavaScript 和 Python——解释型语言，但又具备一定的安
全性。 
编程语言的演进和编程体验的演进一直是同步进行的。比如 Java 出现的同时，我
们也看到了 Eclipse 这样的 IDE，第一次出现了代码补全功能。你打一个字符，它就弹
出建议列表问你“是不是想写这个？”对开发者来说，这是一次革命性的体验，因为你
不再需要记住所有细节。 
所以在我看来，今天 Claude Code 代表的是又一次演进的开始。语言本身已经相对
稳定了，现代语言基本上可以归类为几个大的家族，而且彼此相似。 
但现在的核心变化，是编程体验的变革：你不再需要处理打孔卡，不用写汇编，甚
至不一定非得写代码，而是通过 prompt 与模型对话，然后模型计算出编码部分。这对
 
129 
InfoQ 架构师2025年第一季 
我来说，是一件非常令人兴奋的事情。 
Alex Albert：我们基本上是从打孔卡片时代进入了提示词时代。我们稍后展开聊，
但在此之前，我想先聊聊模型方面的内容。在不久前，Claude Code 主要是由 
Claude 3.7 Sonnet 驱动的。那么现在 Claude 4 系列模型在底层驱动 Claude Code，
这带来了哪些新变化？你认为接下来会走向哪里？ 
Boris Cherny：大概是在模型发布前的几个月吧，我们内部开始尝试这些新模型。我
还记得第一次用时真的被它的能力震惊到了，我认为这可以解锁许多新的使用场景。尤
其是在终端里同步使用 Claude Code 时，一个很大的变化是：Claude 在理解和保持指令
方面变得更强了。不管你是通过提示词还是通过 Claude.md 给它下指令，它都能很好
地执行并坚持你的要求。 
这点变化非常大，因为 Claude 3.7 虽然是个很强的编程模型，但是，它很难驾驭。 
比如你让它写测试，它可能会直接 mock 掉所有测试代码，然后你还得对它说，
“不，我不是这个意思。”通常你得说上一两次，它才会明白，但因为它太强了，所以
你愿意容忍这些小问题。而现在，Claude 4 系列的模型基本上第一次就能准确地理解你
的意图并按要求完成任务。 
Opus 更是比 Sonnet 再上一个台阶，它不仅能很好地理解我的意图，还能一次性
完成很多以前模型做不到的事情。比如说，我已经几个月没有亲自写过单元测试了，因
为 Opus 会直接帮我写好测试，而且几乎每次都是一次性写对。在终端环境下这非常有
用，因为你可以更“放手”地使用它。 
但我觉得最酷的使用方式是放在 GitHub Actions 或其他环境里。你只需要给它一个
任务，它就可以自动执行，等它带着正确结果回来时，那种“一次就成功”的体验真的
很棒。 
Alex Albert：所以现在在  GitHub Actions 里，我们可以直接在  GitHub 中 
@Claude，让它在后台处理任务，最后带着结果和新的 PR 回来？ 
Boris Cherny：对，没错。你在终端像平常一样打开 Claude，运行 claude 命令，然
后执行 /install GitHub Action，它会一步步引导你安装这个功能。只有几个步骤，但基本
 
130 
访谈文章 | Interview 
都是自动的，只需要点两下按钮，Claude 的 GitHub 应用就能安装到你的 repo 里。 
整体体验真的挺酷的。在任何 issue 里，你可以直接 @Claude。我每天都在 PR 里
用它。比如说，同事发了一个 pull request，我不会再说“嘿，你能不能修一下这个问
题”，我会直接说“@Claude，修一下这个问题”，然后它就会自动修好。同样，我也
不会再说“你能不能写一下测试”。以前说这个我还觉得有点不好意思，现在我只需要
说“@Claude，写个测试”，它就会自动完成。这些事情现在根本不再是问题了。 
Alex Albert：这基本就是一种全新的编程方式——你可以随时调用一个随时可用的
程序员来帮你解决问题，而且它不是在你的本地电脑上运行，而是在后台直接完成所有
操作。 
Boris Cherny：对，我觉得这是开始以“协作程序员”的方式来与模型互动。以前你
是 mention 一个同事，现在你可以直接 mention Claude。 
Alex Albert：那在这种模式下，软件工程会发生什么变化？当我们开始管理这些在
后台运行的 Claude Code 实例时，会发生哪些转变？ 
Boris Cherny：我觉得这确实需要一些思维上的转变。 
有些人非常享受控制代码的感觉，如果你习惯于手写代码，那么你就需要适应整个
行业正在转向的方式——你不再是亲自写代码的人，而是协调 AI 代理来帮你写代码。
你的工作重点也将从“手写代码”转向“审查代码”。 
我认为对于程序员来说，这个变化非常令人兴奋，因为你可以在更短的时间里完成
更多的事情。当然，还有一些我必须深入研究并手写代码的情况，但说实话，我已经开
始害怕（dread ）了——因为 Claude 在这方面非常擅长。 
我认为，随着模型能力的进一步提升，那些你不得不亲自写代码的场景，比如特别
复杂的数据结构，或者多个系统组件之间的高复杂交互，又或者是你实在描述不清楚的
需求，这些情况会越来越少。未来，越来越多的编程工作将会是关于“如何协调这些 AI 
代理去完成开发任务”。 
Alex Albert：我想更深入聊聊你们的工作流，也就是你们如何使用这套工具组合：
 
131 
InfoQ 架构师2025年第一季 
从 IDE 集成、Claude 在终端中的使用，到它在 GitHub 中做的这些后台操作。你现
在是怎么把它们结合起来使用的？ 
Boris Cherny：我现在的工作基本可以分为两类。一类是很简单的任务，比如写一些
测试，修复一个小 bug，这类任务我通常会直接在 GitHub Issue 里让 Claude 来完成。
还有一种情况是我通常会并行跑多个 Claude。我本地有好几个代码库的副本，所以在某
个终端标签页里，我会让 Claude 做一件事，按下 Shift+Enter 进入自动接受模式，然后
过几分钟回来看看，Claude 通常会完成并给我发一个终端通知。 
另一类工作就比较复杂了，需要我更深度地参与，我认为这仍然是工程的主流。大
多数工程任务其实都不是“一击即中”，它们依然很难。这种情况下我会在 IDE 的终
端里运行 Claude，让它做一部分工作，然后它可能会卡住，或者代码不太完美。那我就
会在 IDE 里手动改改，把最后那一段路完善掉。 
Alex Albert：感觉和 Claude 的交互是基于任务难度的，越简单就越自动化，越复
杂你就越得亲自参与。 
Boris Cherny：是的，一开始用这类工具的时候会有个学习过程。有些人刚开始用，
就试图让 Claude 一下子搞定太复杂的事情，结果它卡壳了，输出也不理想，你自己也
不开心。这是大家必须经历的一个学习阶段，得慢慢形成一个“内在校准”，知道 
Claude 能做什么、哪些是它能一发搞定的、哪些需要你引导两三次才能完成的。 
而且每换一个新模型，这个校准都得重新来一遍。因为能力是在不断提升的，每次
模型一更新，Claude 就能一次性做对更多的事。也就是说你可以逐步把更复杂的任务交
给它。 
Alex Albert：我也注意到了，哪怕是在非代码领域，这些模型进化得也非常快。如
果你六个月前用某个模型做不了某个任务，你现在还按那个标准判断就不对了。你得每
次都重新设定自己的直觉。 
Boris Cherny：对，完全正确。 
Alex Albert：我挺好奇的，还有没有一些实用技巧或窍门？比如你们内部或者开发
者社区里，有哪些有趣的用法是你见过的？ 
 
132 
访谈文章 | Interview 
Boris Cherny：我觉得目前见过最有价值的一个技巧是：无论是 Anthropic 内部还是
外部的重度用户，现在很多人都会先让 Claude 做“规划”，再开始写代码。新用户常
见的误区是直接让 Claude 实现一个很复杂的功能，结果它写出来的内容和你预期差距
很大。 
一个更有效的方法是先让它出一个方案。我会明确告诉 Claude：“这是我要解决的
问题，在写代码前，请先给我列出几种思路，别着急动手。”然后它会给我列出几个方
案，比如方案一、二、三。我可以挑出一和三，说：“我们可以结合一下，现在你可以
开始写了。”Claude 在这方面的配合度一般都很高。 
如果  Claude 已经有一些上下文信息，你可以让它进入“扩展思考”模式
（extended thinking），这个时候它表现会特别好。但如果它完全没有上下文，一开始就
让它冥想，它其实啥也想不出来——就像人一样，你不读代码，不看上下文，只是坐着
想，是没用的。我的做法通常是：先让 Claude 把一些相关文件读一遍，然后暂停一下，
让它开始头脑风暴，列思路，再让它开始写代码。 
Alex Albert：所以你是采用那种“交替式”的工作流：调用工具→思考→再调用工
具→再思考，这种来回往复的方式。 
Boris Cherny：对，正是这种方式。我们内部做模型评测的时候也是这么设计的：先
提供上下文，然后让模型进行思考，再让它用工具来编辑或者使用 Bash 命令，效果普
遍会更好。从使用者角度来看，体验也的确是这样的。 
Alex Albert：聊聊 Claude.md 文件吧，这东西看起来很强大。 
Boris Cherny：对，我们用 Claude.md 做很多事。它是 Claude 的“记忆”，可以让 
Claude 持续记住你对它的指令。这些指令可以在你的团队中共享，也可以在你的所有项
目中共享。 
最简单的做法就是在你的代码库根目录下新建一个 Claude.md 文件，一个普通的 
markdown 文件（CLAUDE 全大写，md 小写），Claude 启动时会自动读取它。你可以
写一些通用指令，比如常用的 Bash 命令，重要的架构决策变更文件，MCP 服务器，
等等，全都写进去。 
 
133 
InfoQ 架构师2025年第一季 
这种是团队级的 Claude.md，这种是你写好之后跟团队其他成员共享的，这样大家
不用每个人都单独写一份。 
第二种是你自己的专属版本，叫 Claude.local.md，放在同样的位置，但不提交到代
码库（可以被 .gitignore 忽略），只对你个人生效。 
第三种是全局的 Claude.md，放在你的 home 目录下的 .Claude/ 文件夹里，大多
数人其实不怎么用这个，但如果你希望与 Claude 分享指令，这是个办法。 
最后，还有一种是嵌套型的 Claude.md，可以放在代码库的任何子目录下，Claude 
会在它认为相关时自动加载。 
Alex Albert：所以这些 Claude.md 文件，其实可以定义很多内容，比如你的编码
风格、Claude 与你交互的方式、它应该知道你的哪些偏好等等，对吧？ 
Boris Cherny：对，完全可以。有时候我跟 Claude 对话的时候，它表现特别好或者
特别差，我就会按下“#”（井号），这相当于进入“记忆模式”。我会告诉它：“你
应该记住这一点。”比如说，“我每次写代码都要运行 linter”，我会明确告诉它，它
就会自动把这条写进合适的 Claude.md 文件里。 
Alex Albert：Claude Code 接下来有什么计划？ 
Boris Cherny：我们现在考虑的方向有两个。一个是让 Claude 更好地与各种工具协
同工作。目前它已经可以配合所有终端使用，现在也能支持很多 IDE，还能与多个 CI 
系统配合。 
我们在探索如何进一步拓展，让 Claude 能够“原生地”使用你常用的所有工具，
理解这些工具的使用方法，并能无缝集成。 
另一个方向是让 Claude 更擅长处理那些你可能不想专门开个终端的小任务。比如，
我能不能在聊天工具里 mention Claude，让它自动修复一个问题？就像现在你在 GitHub 
上那样操作一样。 
我们正在尝试多种方式，希望能找到真正“好用”的方案，然后再开放给用户使用。 
 
134 
访谈文章 | Interview 
参考链接 
• https://www.youtube.com/watch?v=Yf_1w00qIKc 
• https://x.com/Steve_Yegge/status/1898674257808515242 
 
135 
InfoQ 架构师2025年第一季 
微软重磅开源 Copilot！64 岁 VS Code 创始
人亲口承认：眼红 Cursor，但真正价值在后
端，它“抄”不过去！ 
 
北京时间 5 月 20 日，在 Build 2025 开发者大会上，微软正式开源了 GitHub Co-
pilot Extension for VS Code 项目，并采用 MIT 许可证。全球开发者将可免费访问其完整
源码，并参与功能优化。 
根据微软 VSCode 团队的声明，微软计划先开源 GitHub Copilot Chat 扩展的代码库，
随后会将该扩展的相关组件重构整合至 VS Code 核心代码中。微软为此制定了一个为期 
4 周的迭代计划，新的 VSCode 将于 6 月初发布。 
作者 Tina、褚杏娟 
 
136 
访谈文章 | Interview 
GitHub Copilot 自 2021 年起就作为 VS Code 的插件存在，后者是其最早、最主要
的运行平台之一。对于今天的开源公告，Eric 强调，Copilot 的服务端不会开源。但 
VS Code 的所有客户端逻辑，包括模型返回的 diff 渲染、inline chat、Agent UI 等都是开
源的。 
在官方宣布开源的同时，VS Code 创建者  Erich Gamma 和  Copilot 负责人  Kai 
Maetzel 接受了 Syntax 的独家专访。 
两人都是 VS Code 项目的早期成员，多年来一直在推动这个项目的发展。其中，
Eric 在 15 年前加入微软时启动了 VS Code 项目并长期领导该项目，最近几年开始退居
一线，变成了一个独立贡献者。如今，Kai 负责主导整个 VS Code 项目，他大约在 10 
年前加入了这个项目。两人在这档节目中，围绕 Copilot 开源进行了一系列独家分享。 
为何开源：真正的“护城河”不在客户端 
VS Code 此前架构是“开源核心 + 闭源扩展”的模式。VS Code 本身是开源的，里
面有一些 AI 支持功能是开源的，但 Copilot 本身是一个闭源扩展。这背后是有原因的，
也有一个发展过程。 
最初 Copilot 是从代码补全功能开始的。当时 VS Code 和 GitHub Next 的团队有非
常紧密的合作，后者主要在探索这类新交互方式的 UI。于是 VS Code 引入了“Ghost 
Text”补全功能，这在当时是全新体验——在此之前，编辑器里只有提示窗口。 
早期的 Copilot 主要依赖一个定制化、精调过的 GPT-3.5 模型，很多“智能”其实
都是在扩展中完成的，比如 prompt 构造等。总体上，Copilot 是一个巨大成功，用户
反馈是“太神奇了”。当然也会有用户指出它偶尔会出错、建议不准确等问题。 
之后，随着 ChatGPT 和 GPT-4 的发布，每个人几乎都开始重新思考“AI 还能做什
么”。 
微软团队意识到，AI 的 UI 部分必须深入嵌入用户的工作流中。而问题是，当时 
AI 要真正有用，代价是非常高的：prompt 需要大量处理，数据需要预处理，结果还得
后处理，模型回复的格式也常常不理想。得写很多代码去构建 prompt、控制上下文、
处理各种用例的问题。 
 
137 
InfoQ 架构师2025年第一季 
“从业务角度看，当时也没人知道该怎么用这玩意赚钱。运行成本极高，我们又不
想轻易把东西免费送出去，市场趋势也不明朗。于是我们采取了‘开源核心 + 闭源功
能’的策略，UI 和部分功能进入核心，我们必须通过例如 API 来启动一些元素，所以
我们创建了更多 API，但核心内容还是在 Copilot 中。”Kai 说道。 
而如今，整个 AI 工具链已经变得成熟了很多，比如 OpenAI 的 cookbook、An-
thropic 的教程、Google 那份 68 页的 prompt 指南等。像 CodeX 这样的开源项目也
展示了如何构建完整的 prompt 流程。 
过去两年，情况发生了很大的变化。prompt 工程本身已经不再神秘，也更易掌握
了。 
另外一个重要因素是，现在微软对商业模式的理解也更清晰了。Kai 表示，真正的
“护城河”其实在服务端，而客户端更多是交互逻辑，这部分其实很适合开源。因此，
微软认为现在是时候将 AI 功能也开源，让大家能够： 
• 克隆 VS Code； 
• 自行构建并运行； 
• 使用开源的 Code OSS 版本体验完整流程； 
• 调试所有从输入到服务端响应的过程； 
• 优化逻辑、提交问题、贡献代码。 
换句话说，微软希望复制 VS Code 本身开源成功的路径。在一个市场逐渐成熟的时
候，开源能帮助开发者更深入地参与讨论、提出想法，而不只是等着项目发起团队实现。
微软认为，这一时机现在已经成熟。 
原生 AI 编辑器是什么样的 
“像 Cursor 这样的新对手让我有点‘眼红’。”如今已经 64 岁的 VS Code 
创始人 Eric 在采访中不止一次提到。 
Eric 认为，一个原生 AI 编辑器不应该把 AI 功能“挂载”在 VS Code 上，而应该
是核心的一部分。用户编写开源软件时，它就在那里，不需要安装任何东西。因此，团
队现在要做的就是把 AI 彻底变成核心的一部分，而不是额外安装的插件。 
 
138 
访谈文章 | Interview 
现在所有关于 prompt 构造、模型交互的逻辑都还藏在 Copilot 扩展中，Eric 希望
将这些开源后能吸引更多开发者的关注和贡献。这样也更安全，对开发者来说也更透明。 
“我喜欢这个变化的另一个原因是，我们之前在开源和闭源之间来回切换其实挺痛
苦的。”Eric 补充道，“项目属性不同，我们的说话方式也不同。开源社区友好热情，
而闭源产品的用户则更像‘客户’，要求多、没啥耐心。能把整个工作流程都统一为开
源，对我们开发团队本身来说也是一种‘解压’。” 
Eric 还提到，将 AI 部分开源的一个原因是从一些组织那里听到的反馈——他们真
的不喜欢闭源的 IDE。将其他部分也开源就满足了这些企业的需求，他们可以选择闭源，
也可以选择开源。 
值得注意的是，Copilot 是有免费选项的，开发者可以直接注册并免费使用，但会
受到调用高级模型的请求次数等限制。Copilot 中有一个没有调用限制的基础模型。如
果用户有自己的 API Key，也可以用“Bring Your Own Key”（BYOK）方式接入，直接使
用指定的模型服务，而非通过 GitHub 后台服务，费用开发者自己承担。 
如果用户付费使用更高级的 Copilot 功能，那就和开源没有直接关系了。“总得有
人为模型的计算资源买单，这和是否开源是两码事。”Kai 说道。 
所谓开源，是用户可以看到所有客户端上发生的内容，也能看到团队发送到服务器
的内容。而客户最终所支付的费用，基本上就是后端运行大模型所产生的计算费用。对
于企业用户来说，还有一些附加服务，比如免责保障（indemnification）之类。 
Copilot 开源后的一些变化 
开源后，上下文是公开的 
Copilot 的一个重要组成部分就是确定要将哪些信息发送给模型。如果 Copilot 开源，
我们就能清楚地看到究竟哪些内容被发送给了模型，以及这些数据是如何被处理的，进
而能够进行改进。 
最开始时候，模型还不支持像样的工具调用（tool calling），所以 Copilot 团队几乎
必须把所有需要的信息预先打包好，一起发给模型。但那时也会遇到“干草堆里找针”
 
139 
InfoQ 架构师2025年第一季 
的问题——发得太多反而会让模型困惑。 
现在，Copilot 对于生成补全的请求，还是会尽可能地将上下文预先打包，然后让
模型去补全。但这里面还是有很多思考，比如该发哪些东西。比如说，打开的标签页会
被用来计算额外的上下文，如果语言服务器支持的话，我们会以某种精简形式把类型信
息等也一并发过去。 
但也有些场景并不再那么依赖这些了，比如 Agentic 流程。 
在  Agentic 流程中，Copilot 用的是内部称之为“面包屑提示”（breadcrumb 
prompts）的方式。比如现在问题面板里有个问题，终端里也有输出，那就只是告诉模
型这些信息存在于哪里，并且给模型提供检索这些信息的工具，然后模型自己判断它是
否需要这些信息。这种方式更简单。当然有时候也需要用合适的提示方式来引导，比如
告诉模型在思考时该优先考虑哪些方面。但总体来说，引入工具调用之后，整个上下文
处理过程简单了许多。 
生成提示词方面，Copilot 现在用类似 tsx 的结构来描述提示语，而不是简单的字
符串拼接。这种方式带来了灵活性，比如可以给树结构中的节点分配不同优先级，按需
要添加或减少内容，这些都能根据上下文窗口灵活调整。 
但提示词本身也可以告诉模型优先使用哪些工具。比如可以写明：先检查问题面板
里是否有错误信息，不要一开始就看终端输出之类的。用户可以引导模型按照希望的顺
序去处理上下文信息。不过，Copilot 仍然面临上下文窗口的限制。有的限制是模型本
身的技术约束，有的则是希望通过控制窗口大小来提高吞吐量，这些都需要权衡。 
Copilot 团队的一个明确经验法则是：用尽可能短的提示语得到尽可能好的结果。
所以如果你把整个上下文窗口都填满了，然后得到了一个不错的结果，问题就变成了：
能不能删减一些内容，依然得到同样好的结果？ 
这也正是 Copilot 整套提示语渲染逻辑的关键：要知道在什么时间点可以删掉哪些
内容。比如“历史记录”这一部分，如果不希望把所有历史记录都灌进去把上下文窗口
填满，那就得有一个自然的截断方式，比如判断对话是从什么时候开始转到与之前无关
的新话题的。 
 
140 
访谈文章 | Interview 
实际上，提示词本身并没有太强的语言特异性。 
在早期阶段，Copilot 确实有一种被称之为“cookbooks”的方式，比如不同的 
linter（代码检查器）会抛出不同的错误和错误信息，然后团队就会针对这些情况创建一
些“ cookbooks”。像是出现了类似在  Python 文件中使用了  PyLint 的错误，
“cookbooks”会给出一些修复方式、错误的含义，以及如何可以解决等。这种做法是
强语言相关的，有时甚至需要提供语法方面的提示。 
但随着系统的成熟，这些特定语言的“cookbooks”变得不那么必要了。 
当然，在处理补全上下文（completions context）的时候，仍需考虑语言的特性。例
如，支持某个语言支持类型，比如接口（interfaces），那就需要编写一些逻辑代码去找
出你想包含的合适接口。 
另一个有点语义性质但并不属于语言特异性的部分，是关于工作区知识的处理方式。
整个流程中，如何找出最相关的代码片段添加到提示词，这背后有一个非常复杂的索引
基础设施。这个系统是基于一个类似树结构的数据库，具有范围（ranges）等的语法结
构知识，然后再去计算嵌入（embeddings）。但这一套逻辑是在语法级别上完成的，而
不是语言特定的。 
Copilot 服务端有哪些“秘密武器”？ 
Copilot 在某些场景下用的是微调后的模型，而且不只用一个模型，而是好几个。 
“我们会越来越多地看到这种趋势：虽然一个大模型很聪明，能做很多事，但它既
昂贵又慢。你希望给用户提供的是一个快速体验，给自己减少基础设施负担——这就意
味着你需要不断‘蒸馏’，一步步缩小模型，直到针对每个特定任务都有一个足够小但
效果很好的模型。”Kai 说道。 
虽然用户能用 BYOK，但 GitHub Copilot 服务本身其实还附加了一些其他能力。 
比如，GitHub 团队会对所有开源仓库做索引（也可以选择让你的私有仓库被索
引）。这个“索引”的实际含义是把代码分块（chunking），然后对每个代码块计算 
embedding。查找相关代码时，比如用户输入了某个查询，Copilot 就会对这个查询计算 
 
141 
InfoQ 架构师2025年第一季 
embedding，然后去代码库中查找距离这个 embedding 最近的代码块。这就是 GitHub 
提供的服务之一。 
这是否算是“秘密武器”呢？或许可以这么说，但 Kai 拒绝了用“秘密武器”这个
词。在他看来，这更准确地说是规模化能力，要支撑 GitHub 仓库的数量级，需要很强
的基础设施。 
现在的新模型还在编辑文件方面表现特别好。比如 set 命令的替换功能，用户可以
搜索某个字符串并替换成其他内容。新出的  OpenAI GPT-4.1 之类的模型支持  ap-
ply_patch 功能，能以 diff 的形式返回补丁，非常精准。 
但也有很多场景下，模型并不擅长直接做 diff 操作。这时团队会用另一种方法：
告诉模型，“请给我一个这个文件的重写版本”，保持没改的地方一致，明确哪些地方
是旧代码，然后再把原文件和新文件一起送给模型，让它拼出一个合理的合并版本。 
这就是一个自定义模型的例子，Copilot 团队每天都要用很多次。要支持高频调用，
就得足够便宜。而要便宜，就需要定制化的模型，即能很好完成任务的最小模型。 
“再如代码补全的模型肯定是定制的，还有类似‘下一步编辑建议’功能（比如你
在某处改了代码，它提示你后面也要同步修改），这个功能也需要定制模型。当然用户
也可以什么都不做，直接用大模型处理所有任务，但那会非常昂贵且缓慢。”Kai 说道。 
对 Cursor 和 Windsurf 意味着什么？ 
VS Code 一经发布就“引燃”了整个圈子。当时虽然已经有 Atom 和 Sublime 等编
辑器，但社区里很大一部分人都迅速转向了 VS Code。 
“当时我们注意到出现了一批新型开发者，我们常称之为‘纯粹的 Web 开发者’。
我们希望为他们提供工具，让我们对他们也具有吸引力。虽然我们当时已有 Visual Stu-
dio 这样一个功能强大的集成开发工具，但我们想打破这种一体化的传统模式，吸引那
些更喜欢命令行、喜欢自定义工具、使用多种语言的 Web 开发者。就是在这样的初衷
下，我们创建了 VS Code。”Eric 追忆了 15 年前建立 VS Code 项目的时候。 
Eric 将 VS Code 归为“十年磨一剑”后的一夜成名。“在发布前，我们其实已经按
 
142 
访谈文章 | Interview 
照每月迭代的节奏持续开发了五年。真正的增长是逐步发生的，不是一蹴而就的。” 
VS Code 一直是渐进式增长的，现在有大约 4000 万用户。从 Github 上看，VS 
Code 目前已经有了 3.2 万个分叉项目。 
业内基本认为，Cursor 也属于 VS Code 的众多分叉项目中的一个，其保留了 VS 
Code 的核心架构、界面布局和扩展生态。这也是与 Cursor 初期最为人道的一点。 
别人是否可以拿 VS Code 所有的东西，如整个系统，然后重新贴一个标签说“这是
我们的”，并以他们的名义推广？Kai 的答案是“当然可以”。但他会对这些项目不回
馈上游社区、不回馈整个开发者生态的做法表示质疑。 
“现在很多人做 VS Code 的 Fork（分支版本），但我觉得他们中的一些已经偏离
了 VS Code 原本的设计精神，用 VS Code 的精神误导人们。”作为项目创建者，Eric 
直白地说出了自己的担忧，“比如在某些 Fork 中，随处都能看到一个蓝色的 AI 按钮，
承诺 AI 无处不在。当然，你可以这样做，但我们一直强调的是平台思维，要为更多人
服务，而不是垄断一切。” 
Eric 还提到了一致性的问题。“我们非常重视 VS Code 中的一致性，但在很多 
Fork 中，这一点被忽略了。比如，Cursor（一个 VS Code 的衍生项目）已经改变了 UI，
把活动栏放到了顶部，这个设计挺好的，我们也引入了类似功能。但我很难理解，为什
么他们没有回馈这个功能到主项目中？” 
“更令人沮丧的是，我们注意到他们实现这个功能时，他们给用户的实现并不完整。
比如我们在 Git 视图中还会显示有多少文件发生了变更的徽章（badge），他们就没有
实现。这种对一致性的忽视让我挺受伤的。”Eric 表示，“我们做每一个功能都非常认
真地考虑一致性，而 Fork 项目有时会为了短期目的而忽略平台整体性。我们关注的是
长期发展，而不是某一波 AI 热潮。” 
Kai 认为，竞争将越来越多地集中在围绕这些东西所提供的服务上，即真正运行在
后端的那些部分。 
在他看来，和很多情况一样，真正重要的地方是在服务器端——如何构建服务、如
何扩展、如何真正为客户提供持久价值。而人们希望能够信任客户端上的东西，特别是
 
143 
InfoQ 架构师2025年第一季 
对于大型企业用户来说，他们希望能够看到、审计客户端到底做了什么，并确保一切合
规、可靠。 
“我无法准确说这对 Cursor 的业务意味着什么——那是他们的事情，不是我的。
但我可以从刚才提到的角度去思考：竞争的核心已经转移到服务器端了。客户端的设计
也影响了我们的决策——比如我之前提到的 prompt 设计、上下文处理等，这些已经越
来越成熟了。”Kai 说道。 
“对于我们这些开源项目维护者来说，看到别人 fork，当然不是令人愉快的经历。”
Eric 则坦诚道。 
根据 Eric 的说法，那些 fork 项目之所以要 fork VS Code，是因为 VS Code 
的 API 不够强大，无法支撑他们想要的 AI 创新。“这确实是事实——我们现在的 
API 不允许开发者用它做很复杂的 UI，这是有意为之，因为我们很注重稳定性和性能。
比如，你不能用我们的扩展 API 来构建一个像 Copilot 那样的联合补全（co-completion）
UI，那根本不支持。” 
“但我也要强调一点：虽然这些 API 不支持深度创新，但社区还是在现有 API 基
础上做出了很多创新。比如很多不错的 AI 编辑器扩展。但确实，想做更底层的创新会
面临挑战。当然这也是我一直‘嫉妒’的原因。”Eric 说道。 
Eric 补充称，“另一点，那些 fork 直接内置了 AI。fork 允许他们做我们没有做的
事情。而我们团队因为各种结构原因，一直要把 AI 功能封装在一个闭源扩展里。现在
不用这么做了，对我和开发团队来说是种解脱。我们终于可以采用真正‘AI 优先’的
思维方式去构建产品，这意味着别的扩展也可以……” 
“毕竟，真正的价值最终还是在后端。”Eric 说道。 
而 UI 交互形式上，Eric 指出，客户端的某些 UI 交互形式已经开始趋同了，比如 
inline chat、如何展示 diff（差异）、如何流式展示内容等已经有了一定的共识和统一趋
势。 
 
 
144 
访谈文章 | Interview 
如何运营开源社区 
根据 Eric 的说法，AI 部分开源后，VS Code 的整个思路没有改变：在 VS Code 里
用户可以通过写一个扩展来参与，通过扩展 API 来贡献功能；也可以通过提 Pull Re-
quest 的方式参与，比如发现某个 AI prompt 有问题想改进它。 
新的变化是：第一，开源社区有了源码访问权限，如果扩展有问题，可以直接去看
源码更深入地理解；第二，社区可以通过 Pull Request 的方式参与，也就是说“写一个
扩展”其实就是在做贡献。 
宣布开源后，整个团队还有很多工作要做。这和当年开源 VS Code 的模式不一样。
那时候是一个完全闭源的项目，我们准备好后直接切换为开源。而这次是从已经开源的 
core 出发，要继续“调优”和“重构”，以开放 AI 的相关能力。 
“这意味着社区会看到我们对 core 的一些改动，”Eric 说道。“大家也会有疑问：
发生了什么？大家真的会盯着每一条 commit 看。我们当然不能靠写假的 commit mes-
sage 糊弄过去，而是真的是在做有意义的改进。” 
此外还有需要考虑很多因素。首先是法律问题，比如要确保每段代码都有合适的版
权声明、没有引用内部问题单或者泄露内部信息等等。这些听起来都是“显而易见的”，
但确实需要处理。 
然后是代码质量的问题。如果没人看代码，代码质量可能就不太行；但如果全世界
都在看，写代码的心态就完全不同了，有人可能会因此感到压力。不过 Kai 明确表示，
现阶段不会把“代码质量”作为开源门槛的重点考虑因素，反而更关心的是诸如内部已
经有的那些 issue 怎么处理等问题。 
另外，还有测试用例本身的问题。好用的测试用例往往来自我们内部的 flight（测
试回归运行），我们会看到一些真实的 prompt、源代码片段，观察失败的案例，然后
把它们加入到 S 测试套件中。 
但现在我们需要把这些内容清洗掉，才能对外发布。这又是一个工作量不小的过程。 
“今天我们从‘宣布开始’这件事正式起步，然后我们会一步一步推进。我们最重
 
145 
InfoQ 架构师2025年第一季 
要的承诺是：我们不会‘扔下包袱’就走人了，这不是我们的风格。”Eric 说道。 
Eric 给出了承诺：“我们会确保整个过程是完全透明的。你可以清楚地看到我们每
一步在做什么，未来计划是怎样的。我们的变更计划是公开的，每个月的计划甚至都固
定贴在 GitHub 的 Issue 上。你可以随时关注，可以提出问题，这就是我们的运作方式。
我们会确保整个过程完全透明。” 
不过，开发者现在还看不到所有内容，“因为我们前面也提到了——有很多工作还
没做完。” 
参考链接 
• https://www.youtube.com/watch?v=GMmaYUcdMyU 
 
146 
访谈文章 | Interview 
年赚三亿美金、估值近百亿，Cursor 竟无护
城河？ 
 
5 月 6 日，AI 编程黑马 Cursor 的母公司 Anysphere 完成了一轮 9 亿美元（约合
人民币约 65 亿元）融资，估值增长两倍多，达到约 90 亿美元（约合人民币约 654 
亿元）。这款全球增长最快的 AI 代码编辑器，推出仅两年便达到了 3 亿美元的年经常
性收入，其背后成功的秘诀是什么？ 
最近，Anysphere 的联合创始人兼首席执行官 Michael Truell 在播客节目中，与主
持人 Lenny 详细回忆了 Cursor 构建过程中的经验教训，团队搭建的心得，以及如何为
即将到来的 AI 未来做好准备的建议。基于该播客视频，InfoQ 进行了部分增删。 
 
编译 傅宇琪  策划 褚杏娟 
 
147 
InfoQ 架构师2025年第一季 
核心观点如下： 
• 未来工程师将更多地像是逻辑设计师，真正的工作将是明确表达你对软件如何运
作的意图。 
• 成功的关键在于持续的焦虑和对改进的不断追求。 
• 在模型开发过程中，不要集中精力在基础模型已表现优秀的地方，而是要聚焦于
它们的弱点，并思考如何补充它们。 
• 软件的需求超出现有技术，未来工程师的需求将更大。 
Cursor 的构建 
Lenny：Cursor 正在改变人们构建产品的方式、职业生涯、行业等等，这一切是
如何开始的呢？初期有没有什么难忘的时刻？ 
Michael：最初，两个关键时刻让我们对 AI 产品充满兴奋。其一是在使用 Copilot 
测试版时，我们感受到 AI 从虚拟演示转变为了真正实用的工具。另一个是 OpenAI 发
布的关于技术扩展的研究论文，表明了 AI 可以通过简单手段持续提升，如扩大模型规
模和增加输入数据量等。 
到了 2021 年底，我们对 AI 技术的未来充满期待，认为它将变得更加成熟。我们
意识到，尽管许多人在构建模型，但很少有人思考 AI 如何改变知识工作领域。于是，
我们开始思考：随着技术的成熟，哪些领域会变化？工作状态如何？工具与模型如何演
变？ 
我们最开始的时候认为编程领域已被充分探索，AI 将改变这一领域，但大家依然在
关注。因此，我们把目光放在了一个较为冷门且发展缓慢的领域——机械工程。然而，
最终我们意识到这并非我们的兴趣所在。尽管编程领域已有积累，但变化不大。许多人
未能意识到新模型将如何改变软件开发方式，这促使我们开始构建 Cursor。 
Lenny：有时候人们会建议你进入一个“无聊”的行业，因为没有人关注，反而有
机会。这个方法有时确实有效，但我更喜欢你后来提出的想法：“实际上，我们应该追
求最热门、最受关注的领域，比如 AI 编程和应用构建。”这最终带来了成功。你提到，
你没有看到足够的雄心，觉得很多事情还没有完成。这似乎是一个有趣的教训。即使某
 
148 
访谈文章 | Interview 
个领域看起来已经被占领，例如 GitHub、Copilot 等产品已经出现，如果你发现这些产
品缺乏雄心，或者它们的方式存在某些问题，你依然可以看到巨大的机会。 
Michael：我认为 AI 的令人兴奋之处在于，它在许多领域依然有巨大的潜力，技
术上限非常高。拥有一个有如此高上限的空间，在是软件领域的独特之处。 
Lenny：各大公司也在走不同的路线。一个是为工程师构建一个 IDE，并为其添加 
AI 魔法；另一个是完全以 AI Agent 为基础开发产品；还有一个则是专注于构建最优
秀的编码模型。那么是什么让你们决定选择 IDE 路径，并认为这是最佳选择呢？ 
Michael：从一开始，专注于只做模型的团队是在追求端到端的自动化编程，而我
们关心的是让人类掌控他们构建的最终工具中的所有决策。那些团队倾向于设想一个 AI 
主导的未来，而我们更倾向于保持现实主义态度。 
我们一直强调“自我使用”，作为最终用户参与其中，使我们对当前技术有了更为
现实的认知，也促使我们相信人类应该掌控一切，AI 不能做到所有事情。我们希望赋予
人类这种控制权，因此我们避开了单纯做模型的公司和没有人类控制的端到端自动化方
式。 
至于为何选择做 IDE 而非插件，我们认为编程将在这些模型的推动下发生巨大变
化，而现有编码环境的可扩展性极其有限。如果你认为 UI 和编程方式将有重大变化，
就必须对整个应用程序拥有控制权。 
Lenny：我很好奇，你认为未来会有“AI 工程师”在 Slack 里为你做事吗？这是 
Cursor 未来的一部分吗？ 
Michael：我认为，未来你需要能够在不同工作方式之间轻松切换。有时候，可能
希望让 AI 独立运行一段时间，然后再将其工作拉入并快速处理，之后可能再次让它独
立运行。因此，后台与前台的工作方式应该在同一平台上顺畅运作。后台工作特别适合
那些可以明确指定需求、且对正确性要求不高的编程任务，例如修复 bug，但这并不是
编程的全部。 
因此，我认为 IDE 的定义会随着时间的推移发生变化，我们选择自己的编辑器方
式也是基于这个前提：它必须不断发展。这包括可以从不同平台（如 Slack、问题跟踪
 
149 
InfoQ 架构师2025年第一季 
器等）启动任务，同时，界面也会发生很大的变化。目前，我们主要将 IDE 视为构建
软件的地方。 
Lenny：我觉得，人们在谈论 Agent 和这些 AI 工程师会为你做所有事情时，往
往没有充分讨论的一点是，我们实际上都在变成工程经理，管理着很多不那么聪明的员
工，需要做大量的审核、批准和指定。你对此有什么看法？有没有什么方法可以让这一
过程变得更轻松？ 
Michael：我们发现最成功的客户在使用 AI 时其实比较保守，他们非常注重缩小
交给 AI 处理的任务范围。无论是来自 Agent 还是整体 AI，对于需要审查的代码有两
种模式：第一种是，你在前期指定任务，AI 完成后你再审查工作；另一种是，将任务拆
分成多个小部分，先指定一些内容，AI 写一些代码并审查，接着继续指定，AI 继续写
和审查，自动补全通常是在这种模式下进行的。我们看到最成功的用户往往采取拆分任
务并保持灵活性的方法。 
Lenny：回到你们第一次构建 Cursor 的时候，你们是在哪个时刻意识到这个产
品已经准备好了？觉得现在是时候发布它了？ 
Michael：当我们开始构建 Cursor 时，我们对推向市场持谨慎态度，避免过早发布。
最初，我们从零开始构建原型，并没有以 VS Code 为基础。我们必须自己开发许多现代
代码编辑器所需的功能，包括多语言支持、语言间导航、错误追踪等，还需要集成命令
行和远程服务器功能。 
我们迅速投入到开发工作中，五周后开始完全使用自己的编辑器，放弃了之前的工
具。当我们发现它有用时，就让其他人试用，并进行了短期测试。几个月后，我们将其
推向市场，大约是在写出第一行代码后的三个月。那时我们的心态是：“让我们尽快发
布这个产品，公开构建。” 
令我们惊讶的是，我们本以为会长时间为几百个用户开发，但一开始就收到了大量
兴趣和反馈，这些反馈非常有帮助，并促使我们从手工构建的编辑器转向基于 VS Code 
的版本。从那时起，我们就开始在公开环境中进行迭代。 
 
 
150 
访谈文章 | Interview 
成功的秘诀是什么 
Lenny：我觉得你们从零到一亿的年经常性收入（ARR）只用了大约一年到一年半
的时间，这真是历史性的成就。你认为你们成功的秘诀是什么？ 
Michael：三个月的版本其实并不太好，所以我认为成功的关键在于持续的焦虑和
对改进的不断追求。我们始终意识到，产品有许多改进的空间。我们的最终目标是发明
一种全新的编程方式，自动化大部分当前的编码工作。尽管 Cursor 已经取得了一些进
展，我们仍然觉得离这个目标很远，仍有大量工作需要完成。 
Lenny：你们构建了一个自己作为工程师非常喜欢的优秀产品，然后发布出去，人
们喜欢它，再纷纷告诉别人。 
Michael：我们也确实花时间做了很多其他工作，比如组建团队和支持轮换等。但
一些通常在公司早期阶段需要关注的问题，我们选择让它们在一段时间内处于“燃烧状
态”，尤其是在销售和市场营销方面。我们专注于产品，先打造一个自己和团队都喜欢
的产品，然后再根据用户反馈进行调整。听起来简单，但实际上要做得好非常难。 
有很多方向和路径可以选择，专注于正确的事物并有效排序优先级是非常具有挑战
性的。在这个领域的另一个难点是，这种产品构建方式非常跨学科——我们既不是普通
的软件公司，也不是基础模型公司，因为我们要为数百万用户开发产品。在保证产品质
量的同时，还需要在合适的地方做更多的科学研究和模型开发。因此，如何在这方面做
到平衡，一直是一个挑战。 
Lenny：到目前为止，你在构建 Cursor 和其他 AI 产品过程中，做过的最违背
直觉的事情是什么？ 
Michael：最违背直觉的一点是，刚开始时我们完全没想到会进行自己的模型开发。
进入这个领域时，已有公司专注于从零开始训练模型，我们也计算过训练成本，知道这
对我们来说难以实现。外面已经有很多优秀的模型，尤其是在预训练方面，我们认为不
必重复做别人已经完成的工作。 
然而随着发展，我们进行了大量模型开发，并专门招聘了一个优秀的团队。到目前
为止，Cursor 中的每个“魔法时刻”都与自定义模型相关，这个过程是逐步发展的。最
 
151 
InfoQ 架构师2025年第一季 
初，我们训练自己的模型是因为没有合适的大型基础模型可用，结果非常成功。在模型
开发过程中，一个有帮助的策略是精心选择合适的领域，不要集中精力在基础模型已表
现优秀的地方，而是要聚焦于它们的弱点，并思考如何补充它们。 
Lenny：很多人听到你们有自己的模型时会感到惊讶。人们谈论 Cursor 和这个
领域的其他公司时，会认为它们只是建立在 ChatGPT 或 Sonnet 之上。能谈谈你们
的模型背后技术堆栈的情况吗？ 
Michael：我们确实在多个场景中使用了最大的基础模型，它们是 Cursor 体验的重
要组成部分。我们自己开发模型的地方，通常是为了处理一些基础模型因成本或速度原
因无法满足的用例。例如，自动完成功能。在编码中，有时你可以完全预测接下来的一
段工作，就像写作时的自动补全一样。不同的是，编码时，下一步修改的内容通常可以
从之前的代码中预测出来。 
Cursor 的核心功能之一就是针对自动完成功能的优化，能够预测你在多个文件或同
一文件多个位置上的操作。要让模型在这个用例中表现出色，首先需要解决速度问题，
模型必须在 300 毫秒内提供自动补全，并且要控制成本，因为每次按键时都需要运行
大量模型并不断调整预测。另一个特殊用例是代码差异的自动补全。我们训练了专门的
模型，使其能够精准地预测代码库中的变化，包括删除和添加部分。 
此外，我们还使用自有模型来辅助像 Sonnet、Gemini 或 GPT 这样的基础模型，
作用于输入和输出端。在输入端，我们的模型遍历整个代码库，找出与大模型相关的部
分，像一个专门的小型搜索引擎；在输出端，我们获取大模型的修改建议，并通过我们
自己的模型填补细节。这样，我们能够将高层次的修改转化为完整的代码差异，极大地
提升了产品的质量和速度。 
Lenny：我之前在播客上采访了 OpenAI 的首席产品官 Kevin Weil，他称之为
“模型集成”，就是以这种方式运作——利用每个模型的最佳特性，并且如你所说，使
用成本更低的模型带来的优势。其他模型是指基于 Llama 之类的开源模型吗？ 
Michael：是的。我们从现有的最优秀的预训练模型开始，通常是开源的，有时也
会与这些大模型提供商合作，尽管他们并不公开分享模型的权重，因为我们最关心的不
是逐行读取权重矩阵来获得特定的输出。我们更关心的是能够训练这些模型，并进行后
 
152 
访谈文章 | Interview 
训练。 
Lenny：这引出了一个很多 AI 创始人和投资者总是思考的话题，那就是护城河
（moats）和 AI 领域的防御能力。你如何看待定制模型的长期防御能力，特别是知道
有其他公司，不断发布新产品，试图抢占你的市场份额？ 
Michael：我们处于一个必须不断努力构建最好产品的领域，这对我们和整个行业
都是如此。天花板非常高，无论构建什么样的壁垒，都有可能被超越。尤其是在搜索领
域，增加分发渠道能帮助改进产品，因为可以根据用户反馈调整算法和学习方式。 
我认为，这种动态同样适用于我们的市场。虽然这对我们公司来说可能是令人沮丧
的事实，但对世界而言却是令人兴奋的，那就是：有许多可以超越的机会，还有更多有
用的东西等待被构建。我们离能够在未来 5 到 10 年内完全竞争还有很长的路要走，
而我们的责任就是保持这个进步。 
Lenny：持续做到最好，让人们一直使用你的产品，而不是通过创建“锁定”机制
之类的方式来留住用户，像 Salesforce 通过与整个公司签订合同，迫使他们使用你的产
品。 
Michael：我认为，如果你所在的领域很快就会耗尽创新机会，那就不太适合。但
如果你处在一个需要大量投资，并且通过让更多优秀的人在正确的路径上工作来不断创
造价值的领域，你就能获得研发规模的经济效益，并推动技术朝着正确方向发展，最终
形成竞争壁垒。尽管如此，我相信最关键的还是要打造出最好的产品。 
关于未来是否会有主导者，我认为这个市场非常庞大。我们面临的挑战是自动化大
量繁琐的知识工作，真正提升各领域的效率和生产力，远远超出了为开发者提供构建工
具的市场规模。我相信会有多个不同的解决方案出现，最终会有一家公司构建一个通用
工具，能够构建几乎所有软件。但同时，市场中也会有细分领域，你可以为特定市场或
开发生命周期中的特定环节提供解决方案。总体来看，编程将从单纯的编程语言转向更
高层次的工作，最终会有一个主导者，这将是一个巨大的商业机会。 
Lenny：沿着这个思路，微软实际上最早站在了这一切的中心，推出了一个令人惊
叹的产品，拥有出色的分发渠道。然而，微软似乎并没有赢得这场竞争，反而感觉有些
 
153 
InfoQ 架构师2025年第一季 
落后了。你觉得这是怎么回事？ 
Michael：我认为，Copilot 没有完全达到一些人预期的原因，既有市场问题，也有
特定的历史原因。首先，微软显然是我们工作的一个重要灵感来源，通常他们做了很多
出色的工作，我们也是许多微软产品的用户，但我认为这个市场对现有公司
（incumbent）并不友好。 
对现有公司友好的市场通常是那些有一定局限并很快商品化的市场，你可以将产品
捆绑销售，且不同产品之间的投资回报率差异不大。在这种情况下，可能并不值得购买
创新的解决方案，而是选择与其他产品捆绑的东西。 
另一个有利于现有公司的市场是那种一开始就将所有内容集中在一个地方，且转换
成本非常高的市场，无论是好是坏。而在我们的情况下，你可以尝试不同的工具，决定
哪个产品更好。所以，这对现有公司并不友好，反而是对那些拥有创新产品的公司更有
利。 
至于历史原因，参与 Copilot 第一版开发的团队大多已经转向其他项目。我认为，
协调涉及这个项目的不同部门和团队一直存在一些困难。 
Lenny：如果你可以坐在每个第一次使用 Cursor 的新用户旁边，悄悄告诉他们
一两个成功的秘诀，让他们能更好地使用 Cursor，你会说什么？ 
Michael：成功使用 Cursor 的关键之一是了解模型的能力。用户需要明白模型能处
理的任务复杂度，了解需要多大程度上指定任务给模型，同时也要认识到模型的质量、
局限性以及它能做什么和不能做什么。 
为了帮助用户培养这种“taste”，我有两个建议。第一个是避免一开始就直接对模
型说：“嘿，做这个任务”，然后看输出结果，结果要么失望，要么直接接受。我建议
将任务拆分成小块，分步指定，这样可以逐步获得反馈，而不是一次性给出一个大任务。
这样做能更有效地利用模型，避免可能的负面效果。 
第二个建议更适合副项目，而非正式工作。我鼓励开发者，尤其是那些习惯传统开
发流程的人，尝试“跌倒”，在安全环境下探索模型的极限。大胆地使用 AI，很多人低
估了 AI 的能力，所以在安全环境中进行大胆尝试，你可能会惊讶于模型的表现。 
 
154 
访谈文章 | Interview 
Lenny：要培养对模型能做什么的直觉，了解它能带领一个想法走多远，而不是只
是引导它走一段路。我敢打赌，每当有新的模型发布时，你就需要重新建立这种直觉，
譬如当 4.0 版本发布时，你就得重新做一次。这种说法对吗？ 
Michael：是的。过去几年，这个问题并不像人们第一次接触一些大型模型时那样
显著。这也是我们希望能更好地解决的问题，减轻用户的负担。 
Lenny：人们总是会争论，像 Cursor 这样的工具对初级工程师更有帮助，还是
对高级工程师更有帮助？你认为今天谁从 Cursor 中受益最大？ 
Michael：这两类人群都从 Cursor 中受益匪浅，很难确切说哪个群体更受益。 
初级工程师往往有点过度依赖 AI，几乎依赖它做所有事情，而目前我们还没有达到
能够在专业工具中完全依赖 AI、与数十人、数百人合作并且在长期维护的代码库中工作
的阶段。至于高级工程师，我认为平均而言，高级工程师往往低估了 AI 对他们的帮助，
仍然坚持他们现有的工作流程。因此，比较这两者的相对受益是有些困难的，我认为他
们都陷入了不同的反面做法。 
Lenny：这就像是光谱的两端——一端期待过多，另一端则期望不足。 
Michael：是的，也许是处于两者中间地带的高级工程师受益更大。 
Cursor 招聘什么样的人 
Lenny：如果你可以见到刚开始做 Cursor 时的 Michael，你会给他什么建议？ 
Michael：许多宝贵的知识往往是潜在的，且难以用语言表达。生活中的一个遗憾
是，在某些领域，你确实需要“跌倒”才能学到正确的东西，或者需要身边有一个在某
一领域非常出色的榜样。 
对我们来说，能够组建一支世界级的工程师和研究团队来共同打造 Cursor 至关重
要。引入合适的人才，可能是我们最为关注的事之一,我们因此等了很长时间才扩展团队。
我认为许多成功的公司都面临“招聘过快”的问题，而我们一开始确实招聘得太慢了。
我们在招聘过程中经历了许多关于如何找到合适的候选人、如何判断他们是否符合团队
条件、伟大是什么样子的，以及如何激发他们兴趣的教训。 
 
155 
InfoQ 架构师2025年第一季 
Lenny：哪些是你们的经验教训？有什么是你们错过或学到的？ 
Michael：一开始，我们在招聘时过于偏向寻找“名校毕业、非常年轻、在名校环
境中做过高认可项目”的典型人选。实际上，我们运气很好，在早期找到了愿意和我们
一起做这件事的非常出色的人，他们已经有了一定的职业经验。我们花了很多时间寻找
那些可能不完全符合我们需求的候选人，部分原因是资历问题，另一部分则是兴趣和经
验问题。 
另外，我们的面试流程逐步演化，现在我们有一套自定义的面试问题，核心环节是
让候选人到现场待两天，和我们一起做工作测试项目。这种方法非常有效，我们也越来
越倾向于这种方式。此外，我们在了解候选人真正兴趣方面也做得越来越好，我们学会
了如何展示机会并展开对话。 
Lenny：你有没有一个最喜欢的面试问题？ 
Michael：我们最初认为这两天的工作测试无法扩展到更多人参与，但它却出乎意
料地有效。它让候选人完成从头到尾的真实项目工作，而不是使用现成的项目清单。这
种方式也不需要占用其他团队太多时间，你可以将原本需要半天或一天的面试安排，分
散到两天内，给候选人足够的时间展示工作成果，帮助面试流程扩展。 
这个测试还帮助验证你是否愿意和这个人一起工作，因为你们将共同度过两天，甚
至共享几顿饭。我们原本没想到这会持续下去，但它在我们的价值观和流程中变得非常
重要，尤其是在公司早期阶段。当时，产品还不成熟，用户也不多，唯一能吸引人的就
是团队成员。通过这两天的相处，我们给候选人机会了解我们。这个做法出乎意料，更
像是一次前置面试，而不是传统的面试问题。 
Lenny：你给候选人安排一个任务，比如“在我们的实际代码库中构建这个功能，
并和团队一起编码、发布”？ 
Michael：对的，在我们的代码库中，我们会给候选人安排一个真实的小型两天项
目。基本上，候选人会被单独安排，当然也有协作的部分。而且，我们公司是一个相对
封闭的公司，几乎所有的情况都是候选人实际坐在办公室里和我们一起工作。 
Lenny：你们现在有多少人？ 
 
156 
访谈文章 | Interview 
Michael：我们现在大约有 60 人。 
Lenny：对于如今的规模和影响力来说，60 人还是很少的。我猜工程师占了最大
比例吧？ 
Michael：是的。未来我们的一大任务是组建一个更大、更出色的团队，继续改进
产品和提升我们提供给客户的服务。但这个数字目前较小的部分原因是，我们公司内工
程、研究和设计的比例非常高。许多软件公司，如果有大约 40 名工程师，通常员工总
数会超过 100 人，因为有很多运营工作，且通常从一开始就非常依赖销售。而我们从
产品驱动、非常精简的起点出发，现如今已经服务了大量的市场客户，并且在不断扩大
团队。 
Lenny：AI 领域每天都有新的东西发布，作为一家位于风暴中心的公司，如何帮
助你的团队保持专注，低头做事，而不被这些闪亮的新事物分心？ 
Michael：如果你能招到心态正确的人，许多问题都能迎刃而解。我们在这方面做
得不错。我们注重招聘那些专注于高质量工作、保持冷静的人，他们情绪波动小，有助
于保持专注。此外，更多讨论这个话题，并通过以身作则非常重要。我们见证着 AI 领
域技术的兴起与消逝，形成了免疫机制，知道哪些事件会真正影响我们。 
Lenny：你认为人们在理解 AI 的发展方向时，最常误解或没有完全掌握的是什么？
尤其是在构建和改变世界的过程中 
Michael：我认为人们对技术变革的看法有些偏激，要么觉得一切会发生得非常快，
要么觉得这只是炒作。我认为我们正处于一个深远的技术变革中，这场变革将比互联网
和任何自计算机出现以来的技术变革都要重要。这是一个跨越几十年的过程，许多不同
群体将推动这一进程。 
为了实现计算机能做更多事情的目标，仍有许多问题需要解决，其中一些与科学相
关，如如何让模型理解不同类型的数据，使其更快、更便宜、更智能，能在现实中行动。
另一些则与人与计算机的交互、如何控制技术有关。 
我相信，这将是几十年的长期过程。特别重要的是，那些致力于自动化和增强特定
知识领域的公司，它们不仅要为该领域构建底层技术，还要整合各方最佳技术，并创造
 
157 
InfoQ 架构师2025年第一季 
相应的产品体验。这些公司，特别是在软件领域，将变得极为重要。它们不仅能为用户
提供价值，还将推动技术进步，创造庞大的企业。 
Lenny：我知道你们现在在招聘。你们目前在寻找哪些职位？有没有特别急需填补
的职位？如果有人感兴趣，他们应该了解什么？ 
Michael：我们团队还没有完全准备好做所有事情。首先，如果你觉得某个职位没
有开设，可能应该主动联系我们，实际上我们可能还没有意识到需要这个职位。 
总的来说，今年我们最重要的目标是：拥有行业最好的产品，并将其发展壮大。我
们现在处于“领土争夺”阶段，全球许多人还没有使用像我们这样的工具，或者正在使
用发展较慢的工具。所以，扩展 Cursor 是一个重要目标。我们尤其需要优秀的工程师、
设计师和研究员，同时也需要各个业务领域的人才。 
Lenny：“AI 会写我们的所有代码”，但是大家还是在疯狂招聘工程师。你认为
工程师职位会在某个临界点上开始放缓吗？工程师的需求会越来越大，还是最终会有很
多像 Cursor 这样的 Agent 为我们构建代码？ 
Michael：这个过程是漫长而复杂的，不会立刻跳跃到把所有工作交给工程团队完
成。我们希望编程方式逐步演变，人类始终主导，即使在最终阶段，专业人士仍需控制
软件的方向。我曾为生物技术公司构建内部工具，现有工具无法满足需求，远超我能完
成的工作。尽管计算机性能强大，但当前仍有许多摩擦。我认为对软件的需求超出现有
技术，未来工程师的需求将更大。 
代码之后是什么 
Lenny：你提到过一个非常有趣的概念——“代码之后（what comes after 
code）”，你对从代码到其他东西的转变有什么愿景？ 
Michael：当前社会中存在几种不同的愿景。一些人认为未来的软件构建将与现在
类似，依然是文本编辑和正式的编程语言，如 TypeScript、Go、C 和 Rust。另一种观点
认为，未来的软件构建将是与聊天机器人进行对话式编程，你可以让它帮你构建一些东
西，之后再要求它修改你正在构建的内容。 
 
158 
访谈文章 | Interview 
我们认为这两种愿景都有一定问题。我们想象的“代码之后”的世界，是一个能够
以更接近自然语言的方式来表示软件逻辑的世界，写下的软件逻辑可以以类似伪代码的
形式展示，也可以在高层次上进行编辑和指引。那时，软件不再是几百万行难以理解的
代码，而是更加简洁、易于理解的内容。我们正在朝着这个目标努力，使得那些复杂且
难以理解的符号逐渐发展成更具可读性和可编辑性的形式，让软件构建过程变得更加人
性化。 
Lenny：也就是说，你所设想的是人们将摆脱直接用代码编程，取而代之的是一种
抽象的形式——本质上是伪代码，用英语句子描述代码执行的功能。 
Michael：我们认为最终会发展成这样，而且我们坚信这个过程肯定是以人为主导，
人在整个软件构建中依然拥有极大的控制权。同时，人类也将具备快速做出改变的能力，
能够在短时间内进行调整，而不是依赖后台运行的缓慢系统，花费数周时间才完成工作。 
Lenny：对于那些目前是工程师、考虑成为工程师、设计师或产品经理的人来说，
在“代码之后”的世界里，哪些技能将变得越来越有价值？ 
Michael：我认为“taste（品味）”将变得越来越有价值。当人们谈到软件领域的
“taste”时，往往会想到视觉效果，诸如流畅的动画、色彩搭配、UI、UX 等设计方面，
但定义软件的另一半是它的逻辑和功能。我们目前拥有出色的工具来定义软件的视觉效
果，但一旦涉及到软件如何运作的逻辑，实际上，代码是目前最好的表现方式。 
因此，我认为未来工程师将更多地会像逻辑设计师，真正的工作将是明确表达你对
软件如何运作的意图。这将更多是关于“做什么”，而不那么关心“如何实现”。 
Lenny：这让我想到了“vibe coding（氛围编码）”，你是指不必过多关注细节，
而是更倾向于顺其自然，跟着感觉走的那种方式吗？ 
Michael：我觉得“vibe coding”正好描述了这种创造状态。这种状态比较有争议，
虽然你在生成大量代码，但并没有真正理解细节。这种创造状态会带来很多问题，因为
如果你不了解代码背后的细节，你很快就会遇到限制，创造出一些大到无法修改的东西。 
因此，我们正在探索的一个问题是：人们如何在不理解代码的情况下，依然能够持
续控制所有细节。这对于现在从事 vibe coding 的人来说是非常关键的。 
 
159 
InfoQ 架构师2025年第一季 
Lenny：你说的“taste”具体指什么？ 
Michael：我指的是拥有正确的构建理念。未来，用户将会更容易地把“想要构建
的内容、希望一切如何运作、希望它看起来如何”这些想法无缝转化为现实。有了构建
的蓝图后，用户能够在计算机上轻松实现这一点，不再需要艰苦地将其转化为计算机可
以理解和执行的格式。 
参考链接 
• https://www.youtube.com/watch?v=En5cSXgGvZM 
 
160 
访谈文章 | Interview 
月烧 4 万元，两工程师用 Claude Code 跑出 
15 人团队效率：值不值全网吵翻了！ 
 
近日，初创公司 Every 总经理 Kieran Klaassen 在 x 上表示自己用 Claude Code 编
程时平均每天花 250 美元，也就是说一个月花费 6000 美元（约合 4.3 万人民币）。
他还晒出了详细的花费列表： 
编译 傅宇琪、褚杏娟  策划 Tina 
 
161 
InfoQ 架构师2025年第一季 
 
帖子发出后，网友们纷纷对 Klaassen 的花费表示不理解：“你是买了 30 个 Max 
套餐账户吗？还是说你用的是 API 付费方式？我不懂。”“如果你花了这么多钱，还
不如直接雇个开发者呢！” 
当然，也有开发者认为 Klaassen 的做法很有启发性，毕竟“6000 美元不算什么，
只是高级工程师一周的薪资。”但批评的声音还是占了大多数。 
随后，他在 x 上解释称，“花了 6000 美金，多个 Agent 并行运行，一天提交了 
10 个 PR，还完成重构并部署上线了。”他还表示： 
我知道这让你不爽。又一个“看看我 AI 开销”的爆款帖子，都是炒作，都是噪
音。 
但你翻白眼时，错过了关键点：这些智能体彻底改变了我们的构建方式，不只是
更快——而是完全不同。就像从所有乐器的演奏者变成了交响乐团的指挥。音乐
变得更丰富，而你与创作的关系也完全改变了。 
我们只有两个工程师。服务数千用户。过去需要数月的功能，现在几天就能上线。
不是因为我们写代码更快了，而是因为我学过音乐，学会了“指挥”😅😅。只是
这次，指挥的是智能体。 
 
162 
访谈文章 | Interview 
没错，账单看起来的确像是在博眼球。但成果呢？那些是实实在在的。当怀疑的
人还在争论这是否可持续时，我们已经在交付成果了。当他们在抱怨炒作时，我
们正在培育超越我们自身能力的系统。 
软件开发变成了一场不同思维方式头脑之间的协作艺术。 
你可以继续无视这些账单截图，把一切都当作炒作。但你每观望一天，理解它的
人和无法理解它的人之间的差距就会更大一些。 
在评论区，他也说明了并非实际花掉了 6000 美元，而是产生了与其价值相当的成
本。他也认为每个月花 6000 美元有点贵，但每月支付 2000 美元是可以接受的。 
Klaassen 推文中提到的成果也很显著：他和 Nityesh Agarwal 确实在一周内交付了
六项新功能、五个 Bug 修复和三次基础设施更新。两人实际上在短短三个月内，通过 
AI 工具完成了 AI 邮箱管理工具 Cora 的开发，并在发布后迅速吸引了超过 10,000 用
户注册。 
近期，Klaassen 和 Agarwa 两人在 Dan Shipper 的播客节目上详细演示了其是如何
使用 Anthropic 的 Agentic 编程工具 Claude Code 工作并借此提升工程效率的。InfoQ 
对此次访谈进行了翻译，并在不改变愿意上进行了增删，希望能给大家带来一些启发。
其中，部分核心观点如下： 
• 使用 AI 编程，不应仅限于写代码本身，还应用于调研和工作流程的构建，几乎
涉及开发过程中的每一个环节。 
• 该团队表示已经很久没有使用  Cursor 或者其他 Agent 类编程工具了，因为 
Claude Code 的体验远胜一筹，几乎将复杂度降低了十倍。 
• Cora 团队现在的方式是尽量让 AI 完成实际工作，而人类开发者专注于管理这
些 AI 工具。 
• 一个点子能带来很多后续产出，这就是所谓的“复利效应”。团队有六七个任务
是同时进行的，因为每当有了新想法，就立刻开始执行。 
• 务必记得在“价值最低”的阶段对 AI 的输出进行严格审核，确保尽早发现并修
正问题。 
 
163 
InfoQ 架构师2025年第一季 
放弃 Cursor，选择 Claude Code 
主持人：虽然 Cora 团队只有两个人，但整个开发节奏和产出却像是一个 15 人的
团队。Kieran，你前几天说的一句话让我印象很深刻：你们正在探索所谓“复利式工程
（compounding engineering）”的可能性——每完成一个任务，都会让后续任务变得更
容易。我觉得你们的经验值得被更多人了解，因为我们拥有了新的工具，就需要建立新
的原则和工作流。 
Kieran：开发 Cora 是一件非常有趣的事情，更有趣的是能在 Enjoy 这样的环境中
工作，这里不仅有先进的工具，还有丰富的思想资源和全新的工作方式。这让我们重新
思考“如何构建产品”本身。我们正在一边做产品，一边尝试新方法，这种探索本身就
很吸引人。 
我们常常会遇到各种新模型、新研究工具，别人问我们怎么看，我们就在边用边学
的过程中不断尝试。最近几周，Nityesh 和我都明显感受到一种转变正在发生：一场由
新模型、思维方式、MCP 等推动的系统性变革。 
主持人：具体改变了什么？你们目前逐步成型的工作流程大致是什么样的？ 
Kieran：对我来说，最关键的转变是我意识到“AI 编程”不只是代码本身的生成。
它应该贯穿整个流程，从前期的研究到设计工作流程，再到具体执行，每一步都能借助 
AI。如今的 Agent 已经足够强大，能胜任几乎所有环节，因此我们需要彻底重新思考整
个开发方式。 
以前我们用 Cursor 或 Windsurf 之类的工具，属于更传统的“代码补全”阶段。
而现在，我们开始直接给 Agent 布置任务就能完成。尽管最终的产出仍然需要人工协
作与指引，但我们要学会更深入地拥抱这一方式。 
Claude Code 就是一个非常优秀的编码 Agent，能很好地理解和执行复杂指令，尤
其在与新模型结合之后，能力显著提升。突然之间，我意识到：我们已经进入了那个
“Agent 时代”。这不再是实验室里的概念，而是真正能用来构建真实产品的技术。我
们就是在用它做应用，而且它真的在工作。 
主持人：你们怎么使用 Claude Code？ 
 
164 
访谈文章 | Interview 
Kieran：Claude Code 是 Anthropic 推出的一款编码 Agent，底层使用的是 Claude 
模型，它以命令行界面（CLI）的形式运行在本地终端中。对于不太懂技术的人来说，命
令行可能看起来有些吓人，但我已经成功说服一些非技术背景的朋友尝试使用 Claude 
Code，他们也觉得很好用。 
主持人：打开的终端是那种黑底白字的经典界面，看起来像 DOS 系统。Kieran 只
输入了“Claude”，屏幕上就出现了“Welcome to Claude Code”的欢迎信息，并有一个
输入框可以键入命令。 
Kieran：Claude Code 和普通的 Claude 模型不同，它拥有对本地目录和计算机的
访问权限，能浏览文件、运行本地命令、截取网页截图，甚至进行网页搜索，内置的工
具远比普通 Claude 丰富。这点很关键，因为做工程开发不仅仅是写代码。你需要知道 
GitHub 上的需求和任务状态，了解 CI/CD 流程是否正常，测试是否通过等等。而拥有
这些功能的编码 Agent，才真正具备完成一个完整工作流程的能力。 
我可以让它自动执行我平常要做的事。这正是“复利式工程”产生作用的地方——
Agent 不仅写代码，还参与了整个开发上下游流程。大多数工程师的时间并不主要花在
编写代码上，而是花在搞清楚“下一步要做什么”，如何理解用户反馈并作出响应。这
些事情，Claude Code 现在都可以帮上忙。 
比如我现在就可以让它帮我查看我们上周发布了哪些内容，然后整理成清单。这不
仅可以用于团队同步，也可以为产品营销提供素材。例如它会总结出我们修复的 bug、
主要新功能，像简报摘要、聊天面板状态、邮箱汇总、XML 标签、时区自动识别等等。 
主持人：而且这些内容的组织方式很清晰，既能让工程师读懂，也方便非技术团队
使用。 
Kieran：我们现在几乎全力在推进“让 AI 做事、我们来管理 AI”的模式。比如，
有人来问我某个功能现在的状态或下周发布什么内容，我可以直接让 Claude 查 pipe-
line，看即将推出的内容。 
大家应该已经能感受到 Claude Code 的逻辑了：一旦你把所有信息系统接入它，使
用起来就非常顺畅。对我来说，Claude Code 是目前最灵活的工具，不仅能解决编程问
 
165 
InfoQ 架构师2025年第一季 
题，还能参与整个工程流程。很多编码 Agent 只聚焦在写代码上，但我希望它能成为
整个工程工作的辅助系统。 
Anthropic 的团队在设计这款工具时，显然考虑到了这一点。它没有被限定在某个
特定用例上，而是保持了高度的通用性，同时又能精准解决问题。它能理解上下文、分
析自己哪里做错了并进行修正。这些能力结合起来，才真正让它具备了现在可以投入实
际使用的水平。 
主持人：传统的“写代码”和现在的“Agent 协作编程”之间，最本质的区别是
什么？ 
Nityesh：相较于我们熟悉的 Cursor 和 Windsurf 等工具，Agentic Coding 其实提
供了类似的能力，但 Claude Code 更进一步，把整件事简化了一个数量级。 
Kieran：虽然命令行界面对有些人来说可能看起来很复杂，但其实它比像 Wind-
surf、Cursor 那样的图形界面更简单。Claude Code 的界面只有一个文本输入框，没有快
捷键、没有界面按钮，只有纯文本交互。因为底层 Claude 模型能力更强了，它可以持
续工作、调用更多工具，所以反而变得更强大。虽然它和 Cursor 背后都是 Claude，但 
Claude Code 简洁的界面带来了更大的灵活性。 
我今天早上就用它查了一下数据指标。当时我在想：“为什么这份用户反馈表单完
全没有任何回应？” 
Nityesh：给大家补充点背景：我们每周会发一份问卷，问用户如果不能再用 Cora 
会有多失望，用来判断产品的价值。我们也每周开会看这些指标，但 Kieran 发现这周
根本没人填写这份表单。 
Kieran：对，我感觉肯定是哪里出问题了，也许表单压根没发出去。我就问 
Claude Code：“14 天前是不是出了什么问题？帮我查一下。” 
Claude Code 的反应是列出一组待办事项，比如检查 controller 的最近代码改动、
搜索代码库等。它自动查到了在那个时间点，我们删掉了负责把用户加入表单的那段代
码。Claude 还告诉我：“你只需要加回这段代码就行了。”我就说：“那你帮我加回去，
并创建一个 Pull Request。”它就立刻执行了。我还补了一句：“顺便也生成个脚本，
 
166 
访谈文章 | Interview 
把之前漏掉的用户补回来。”它也做到了。 
整个过程特别轻松，我几乎没有费什么精力。这就像我在 GitHub 上记一个任务备
忘一样简单，只不过这次它直接帮我做完了。 
用 Claude Code 研发的细节 
主持人：如果没有 AI，这样的排查和修复任务可能要花 30 分钟到几小时不等。
而且关键不是时间长短，而是你必须停下手头的事，专心处理它。而现在，你可以把它
当作“发个请求”，然后再发一个、再发一个，有多个任务可以并行进行。具体说说你
现在的工作流程是什么样的？比如你到底在干嘛？你自己还会写代码吗？ 
Kieran：或许可以先讲一下我们最早拿到 Claude Code 的时候做了什么，当时我们
都非常兴奋。 
Nityesh：对，那是在 Claude 发布直播的前一天。我们当时觉得，从明天起编程
方式就会被彻底改变，我们将获得一个能力更强的模型，就像拥有一个“编程神灯精
灵”。 
于是我们决定，最有生产力的做法不是继续日常工作，而是开一个两小时的会议，
集中列出所有希望新模型能帮我们解决的问题。我们确实做到了，列了大约 20 个 is-
sue，包括 bug 修复、功能开发等，还为 Claude Code 的到来提前准备好了系统。 
Kieran：当时挺有意思的，Nityesh 还用 ChatGPT 生成了一个 prompt，大概内容
是：“明天我们就有 AGI（通用人工智能）了，帮我们列出我们需要它完成的一切。”
然后我们把这个 prompt 输入到 Anthropic 的 prompt 优化器中，再拿这个优化后的 
prompt 去生成具体任务。 
主持人：你们当时在 GitHub 中用的是那种类似 Trello 的看板系统吗？每个 issue 
都是一个卡片，不管是新功能还是 Bug，每个卡片都有详细文档，包括问题描述、解决
方案、技术要求，甚至还有实现步骤和预估所需时间。 
Kieran：对，比如这张卡片里写的功能是“生成 AI 合成数据”，它的文档里从问
题定义、解决思路、技术要求，到实施步骤都写得很清楚。 
 
167 
InfoQ 架构师2025年第一季 
我们用 Claude Code 配合一个自定义的 prompt（在 Claude Code 中叫 command）
来生成这些任务文档。哪怕是用 ChatGPT 来生成也挺费劲的，因为你得读很多代码，
还得思考、整合，这其实是个挺重的脑力活。所以我们做了一个 command，目的是把
这个流程自动化。 
主持人：你说的 command 是 Claude Code 里的命令，还是 Cursor 里的？ 
Kieran：我是用 Cursor 编辑代码，但运行的还是 Claude Code。我还有个命令，就
是语音转文字（voice to text）直接启动。我和 Nityesh 经常一起头脑风暴：“如果我们
做这个会怎么样？听起来不错！”然后我就直接语音输入，它就开始跑。比如我刚刚说：
“我想在 Cora 加一个无限滚动功能，当我读完一个简报（brief）后，它应该自动加载
下一个，直到所有未读的简报都读完为止。” 
主持人：我想让大家了解一件事：Kieran 几乎从不敲键盘。他基本上全程通过语音
输入来操作，比如刚才他是直接用语音在终端中输入，通过 Claude Code 进行交互。我
记得他用的是一个目前还未正式发布的内部孵化项目——叫 Monologue，他是该工具的
第四大用户。虽然这个工具还在保密阶段，但我们在这里算是提前预览了一下。 
从我观察来看，它的工作方式是这样的：Kieran 说出任务内容后，这个系统会将语
音转录成文本，并插入到任务说明中，然后自动执行一系列操作。 
Kieran：对，Monologue 会把我说的内容填入“功能描述”区域，接着执行一整套
步骤。首先，它会在代码库中查找相关实现，相当于先了解现有内容；然后它会上网搜
索最佳实践，包括开源项目中的常见模式；最后它会生成一个计划并让我审核。我很喜
欢这个“人类审核环节”，虽然偶尔它会搞错，但大多数时候都能命中要点。确认后，
它就会创建 GitHub issue，并自动分配到正确的工作流中。 
主持人：原来你们是在 GitHub 的看板里，把想要实现的功能直接用语音讲出来，
然后 Claude Code 会自动完成所有调研，生成完整文档，最后转成 GitHub issue。 
Kieran：对，这是个很关键的环节。它和 Cursor 编码方式很不同。在 Cursor 里
你可能会跳过这一步，因为它主要是用来写代码的。虽然你也可以在那里面写 Mark-
down，但它不是专为任务管理设计的。相比之下，Claude Code 的设计更贴近 issue 跟
 
168 
访谈文章 | Interview 
踪系统——这本来就是开发者熟悉的工具，我们可以直接把文档交给开发者去实现。 
主持人：当我们第一次看到 Claude Opus 4 的时候，我们都震惊了，因为它能一直
运行，无需人工干预，最终还能给出很好的结果。虽然我们以前也见过一些 Agent 模
式，但 Claude 的这种自动化和完成度完全不一样。它像是在稳定、高质量地一项项完
成清单上的任务，这是其他 Agent 循环很难做到的。 
Nityesh：我和 Kieran 之间还在玩一个小游戏，看谁能让 Claude Code 连续运行更
久，Kieran 现在是领先者。 
Kieran：我这次运行了 25 分钟。 
Nityesh：我目前只跑了 8 分钟。 
主持人：Kieran，你是怎么做到让它跑这么久的？ 
Kieran：我给了一个特别长的计划，内容很复杂，还包括大量测试任务。我让它运
行所有测试并修复全部失败项，这样整个过程就会持续很久。 
主持人：你那个能自动生成调研文档的 prompt 是怎么写出来的？你是靠感觉拼
的，还是也用了 Claude 的 prompt 改写器？ 
Kieran：这就涉及我们所说的“复利式工程”了。最早是 Nityesh 给我发了一个 
prompt，他写得很戏剧化：“AGI 已经实现了，我们可以开始写软件了。”我当时觉得
这个 prompt 还行，但我也问了一句：“你知道 Anthropic 的 prompt 改写器吗？”这
个工具非常好用。你只要粘贴一个 prompt，然后点击“生成”，它会帮你优化改写。
看起来简单，但效果很不错。你甚至不需要花很多时间验证它好不好用——有时候试一
试，不好就删掉，成本很低。 
那天我们要写 30 个调研任务，所以必须先有一个好用的 prompt。于是我就把刚
刚那个 prompt 粘进去，生成一个版本，然后把它当作基础 prompt 用。之后，我们只
需更改参数，就能反复复用。 
主持人：本质上，你们做的第一步是花时间写了一个 prompt，这个 prompt 的作
用是自动生成其他 prompt。而那些调研文档，其实本质上也是给 Claude Code 的任务
 
169 
InfoQ 架构师2025年第一季 
指令。 
也就是说，你们不再需要每次都手动写“先调研、再拆解需求、再规划执行细节”
等等，而是只要讲出一个简单的功能需求，Claude 就能自动展开所有细节，写出一整份
完整计划。以前每次都要手动说清楚的那些内容，现在都可以自动生成了。 
而且更有趣的是——它现在就在我们聊天的时候运行着，这完全改变了写代码的方
式。我们前几周在通话时测试过这个系统，我当时甚至在通话中就上线了一个功能，这
种边说话边构建功能的“社交式编程”在以前是不可想象的。 
Kieran：刚才我们在聊天的时候，其实 Claude 已经完成了调研，并自动生成了一
个 issue。而且我们那会儿同时跑了六七个任务，因为我们当时的状态就是“有新想法
就立刻执行”。我们一边翻看用户反馈、读邮件，把能找到的信息都整理出来，一边不
断头脑风暴。 
这种状态真的很有趣——你只要一有想法就能立刻启动一个 Agent，然后等一会儿
再集中审阅它们的结果。这也是我非常同意的一点：在语音通话中协作是一种很棒的体
验，因为这种碰撞中常常会有“魔法时刻”。 
当然，目前仍然需要人类来做复审。我们发现必须检查输出是否合理、是否遗漏了
什么，这就需要经验、判断力和直觉。比如我之前修复了一个邮件无法发送的 bug，
Nityesh 也用 Claude Code 做了类似的事，但它给出的解决方案却错了。我在 prompt 
里特别强调了“查看历史记录”，这引导 Claude 朝正确方向思考。而 Nityesh 没有加
那句话，它就说“看起来一切正常”。 
所以，确实需要人类的判断。这不是靠“神奇 prompt”一劳永逸的问题，而是你
是否知道如何正确地使用它，把它的长处发挥出来。 
“我每天觉得自己什么都不会” 
主持人：Nityesh，我很好奇你怎么看待这一切。毕竟 Kieran 是一个非常资深的
开发者，而你在编程上的经验可能还比较早期。你是怎么适应这种全新的协作方式的？ 
Nityesh：对我来说，这整个过程非常震撼。我真正接触编程是从 ChatGPT 出现后
 
170 
访谈文章 | Interview 
才开始的，那时候我觉得 AI 出现正好是机会，于是决定自学编程，构建我一直想做的 
SaaS 应用。后来转到 Cursor，接着又用上 Windsurf。我一直觉得自己已经站在最前沿
了，我周围的朋友都没有这么用 AI 的。但直到我加入了 Every，开始和 Kieran 一起工
作，才发现他完全是另一个层次。他在会议中从不敲代码，基本上都是对着电脑说话。
而 Claude Code 发布后，Kieran 推动我去用它，现在它已经成为我们主要的编程方式了。 
过去三周里，我和 Kieran 几乎都没再碰 Windsurf 或 Cursor，哪怕用了也只是因
为我们没装 VS Code。本质上，如果只是为了看代码内容，用哪个编辑器都无所谓了，
因为所有核心 AI 交互都发生在 Claude Code 里。而且真的很神奇，整个编程的方式每
三个月就会发生一次巨变，让你不断意识到“没人真的站在最前面”。 
主持人：我真的有点羡慕你们这些在 ChatGPT 时代开始学编程的人，我是二十年
前靠看书学的…… 
Kieran：《PHP for Dummies》。 
主持人：对对，还有什么《24 小时学会 Basic》那种……你刚才说以为自己已经处
在 AI 编程的前沿，结果加入 Every 跟 Kieran 一比就发现完全不是，这让我想到《星
球大战前传》里有一幕。他们在水下被怪物袭击，看起来要没命了，结果突然有个更大
的怪物出来把那个怪物吃掉。主角说了一句：“总有更大的鱼。”Kieran 就是那个更大
的鱼。 
Kieran：其实我自己也有同样的感觉。你刚才说我很厉害，但我每天也觉得自己什
么都不会，还在拼命追赶。有太多东西要做，太多想法要实现。这就是当下 AI 编程的
真实状态：总有更多的东西，但核心还是要练习。你必须每天都练习使用 AI，持续推动
自己，否则就会错过很多令人兴奋的东西。 
主持人：像你们这种“几乎不写代码、而是在更高抽象层次操作”的方式，会带来
新问题吗？你们是如何应对这些问题的？又发展出了哪些新的工程实践，以保证整个过
程的高效与稳定？ 
Nityesh：对我来说，一个最重要的认识来自一本老书——《高产出管理》，是 In-
tel 的 CEO 在五十年前写的。它在第一章提到一个观点：所有问题都应该在“价值最低
 
171 
InfoQ 架构师2025年第一季 
的阶段”被解决。 
今天，AI 尤其是 Claude Code 已经能帮我们完成很多任务，这让“前期阶段”的
质量变得格外关键。比如，我们会用 AI 生成一份非常详细的 GitHub issue 文档，那很
容易让人想直接调用 Claude Code 去实现它。但如果 issue 本身的方向就偏了，Claude 
就会在错误的路径上继续推进，结果只会浪费时间。 
所以，我们现在的做法是：必须在人类 review 阶段就尽早发现这些潜在偏差、提
前修正，而不是等到 Claude 执行完了再回头返工。 
主持人：这让我想到杠杆原理：你越靠近杠杆末端，力量越大，但方向偏差带来的
后果也越严重。哪怕一厘米的偏差，最后结果也可能相差几千公里。就像发射火箭时瞄
准月球，初始角度偏一点，最后就完全偏离轨道。我自己其实很容易跳过计划阶段——
对着一堆文档集中精力很困难。你们是怎么处理这个问题的？ 
Kieran：老实说，大多数时候读这些文档确实挺无聊的。但我们会想办法让它“更
有趣一点”。比如我会要求 Claude 给出更简洁的内容，但这样它又容易遗漏关键点。
所以我更偏好让它聚焦在用户故事或问题清单上，比如：“一个优秀的产品经理会提出
哪些问题？有哪些不同解法？”这种格式会更容易阅读，也更容易引发思考。 
总的来说，传统的 PRD（产品需求文档）太无趣了。但我们可以加入更多例子或反
问，把它“变形”为一个信息更丰富、也更易于人理解的材料。这也正是我们人类审查
阶段要做的事：查找潜在问题、补充缺漏——因为这一步能为后面节省大量时间。 
主持人：这让我想到我们业务中另一个项目。我们在 Spiral 正在开发一个“写作 
Agent”，有点像  Claude Code，但专为写作任务设计。我们也遇到类似问题：写作 
Agent 如果只是“生成一堆内容”，那人类还是得花大量时间去筛选和修改。所以我们
现在尝试让 Agent 主动“进入访谈模式”，先了解用户是谁、想要什么，再输出内容，
这样更准确，也省事。听你们这么说，我感觉 Coding 这边其实也存在类似问题。也许 
Claude Code 未来可以多问一些有启发性的问题，帮用户厘清思路，而不是只吐出一大
堆文本。 
Kieran：是的，这确实是我们应该自动化并持续优化的事情。Claude Code 的强大
 
172 
访谈文章 | Interview 
之处在于它可以访问你的整个代码库，理解你的风格，这非常有用。 
除了在一开始就尽量做好问题定义，我认为传统的测试方式和邮件通知也非常关键。
否则你怎么知道自己改的东西真的能用呢？当然你可以打开控制台手动点击测试，但完
全没必要。我们可以让 AI 写一个测试，比如最基本的 smoke test（冒烟测试），看看
功能大致是否正常运行，这样 Claude 也可以根据测试结果自主修复问题。 
我们还在尝试一项新工作流程。我们会用 Claude Code 实现 Figma 设计，然后截
取移动端页面截图，与设计图进行对比，验证还原度。虽然我们还没在生产环境大量使
用，但非常期待它的效果。这种方式本质上就是把原本人工执行的验证流程“编进系统
里”。不仅仅是代码测试，对 prompt 也可以做 eval（评估），就像给代码写测试一样，
给 prompt 写评估脚本。 
上周我就让 Claude Code 运行一个邮件发送流程的 eval，让它跑 10 次，失败了 4 
次，它告诉我失败原因是调用了错误的工具。我查看了一下提示词，不够具体，于是我
让它继续尝试，不断优化提示词，直到能每次都通过测试。我中途还下楼喝了杯咖啡，
回来一看，已经搞定了。 
所以说，即便是最传统的测试流程，对 AI 编程同样重要——测试能告诉你提示词
是否可靠，就像传统单元测试告诉你代码是否工作一样。乍一看枯燥，但其实非常有效。 
给各编码类 Agent 打多少分？ 
 
 
173 
InfoQ 架构师2025年第一季 
主持人：我想花五分钟时间，让 Kieran 来给各类智能编码 Agent 做一个从 S 级
到 F 级的打分。我来报 Agent 的名字，你来给出评级。先从 Cursor 开始，拿最优配
置的 Cursor 来看。 
Kieran：就算是传统的、在最好的设定下的 Cursor，也不是最好的 Agent。我喜欢
某些 Agent 的原因就是它们明确告诉你，“这就是我们最好的版本”。而 Cursor 有点
让人困惑。所以我会把它评为 A 级，它确实很不错，特别是在使用 Claude 的情况下。 
主持人：Windsurf？ 
Kieran：C 级。因为他们还不支持 Claude 4，这让我很难理解。三周前我还会给它 
A 级，但现在不行了。 
主持人：Devin？ 
Kieran：B 级。集成度不高，搭建过程也稍微麻烦一些，代码质量也没有 Cursor 
或 Claude Code 那么全面。我不确定它是不是用 Claude 4，但用起来没有其他工具顺畅。 
主持人：Charlie？ 
Kieran：Charlie 主要用于代码审查，我们现在基本只在这个场景下用它。作为代
码 Reviewer，我会给它 A，但作为 Agent 我只能给 B。 
主持人：Friday？ 
Kieran：我会把 Friday 排在 Cursor 之上，介于 S 和 A 之间。虽然它现在还没用 
Claude 4，而是用的 3.7，但依然表现非常优秀。Friday 的流程非常有主见，能高效地把
事情做完。你给它一个 issue，它就能自动生成计划、等待你确认、然后执行、提交 PR。
我见过它成功一次性还原 Figma 设计，也完成过一些 Claude Code 做不到的任务。让
我真的有种“看见未来”的感觉，而且它背后的团队规模很小。 
主持人：Codeex？ 
Kieran：B 级。 
主持人：Copilot 呢？ 
 
174 
访谈文章 | Interview 
Kieran：我没怎么用过。三年前用过一次，半年前又试了一次，但不到一分钟就关
了。如果一定要打分，我只能给 D。它当时还不具备 Agent 能力。当然，我得承认还
没用过它的新版本，所以也许并不完全公平。 
主持人：Claude Code？ 
Kieran：当然是 S 级。 
主持人：Factory？ 
Kieran：Factory 在某些方面其实比其他工具都好。但它不是我的风格，更适合那
些偏企业级的开发者，尤其是对代码质量要求极高的用户，比如做多仓库开发的。它用
起来有些繁琐，因为是 Web 和本地混合部署。我会给它 B，略低于 Codeex 和 Devin。 
主持人：AMP 呢？ 
Kieran：S 级，仅次于 Claude Code。它非常擅长完成任务，使用体验也很好。你
能感受到 AMP 背后的团队是真正热爱 Agent 工具的开发者，他们自己也在用。 
我更倾向于把智能 Agent 看作招聘开发者解决特定问题的过程。比如 Friday 非常
擅长处理界面工作，遇到这类任务我会用 Friday；需要做调研时，我会选择 Claude；如
果是代码审查，我则用 Charlie。Agent 之间是可以协同工作的，不必局限于只用一个。 
主持人：Charlie 集成在 GitHub 上，你可以直接“@Charlie”，它就会对 Pull Re-
quest 进行代码审查。 
Kieran：对，我们用 GitHub、Pull Request 和常规的开发流程，这样人类开发者也
能介入。我们能“聘用”专长于特定任务的 Agent 来审查代码，而闭环代码工具则执
行具体工作。这套体系是经过我们几十年经验不断优化的，非常强大。也正因如此，
Copilot 也许能适应这套流程，因为它本来就集成在 GitHub 里。 
主持人：你们最近似乎引入了一位真正的专家，并通过 Agent 化的方式协作，既
达到了预期，也减轻了对方的负担。 
Kieran：是的。虽然当时还没有真正的“问题”要解决，但我们希望更清晰地掌握
 
175 
InfoQ 架构师2025年第一季 
关键任务的交付进度。鉴于我自己对这方面并不十分擅长，我们就邀请了专家参与。做
法是这样的：我们开了一个两小时的电话会议，我把整个对话录了音；会后，我把录音
内容输入到 Claude Code，并要求它根据对话生成两个资源 issue。大约十分钟后，
Claude Code 给出了初稿，我再请专家审核。他对结果非常惊讶——他平时对 AI 生成
的方案往往持怀疑态度，毕竟有些任务 AI 目前还没那么擅长。但这次他不仅认可了文
档，还提出了非常有价值的改进建议。于是我们又围绕这些建议快速迭代。第二天，专
家完成了人工复审，我再用 Claude Code 实现了对应的代码，并一起做了 Code Review。
整个流程本来可能要两周时间，现在几小时就搞定了。 
主持人：还有什么想说的吗？ 
Kieran：我建议大家都去试试 Claude Code。哪怕你不是技术出身，也可以订阅 
Max 或 Pro 计划，每月约 100 美元就能获得无限使用权。我一个朋友从 Cursor 转到 
Claude Code 后，重做了所有工作流程，都觉得效率大增。大家真的应该多尝试，积极
推动工具的使用。 
Nityesh：但务必记得在“价值最低”的阶段对 AI 的输出进行严格审核，确保尽
早发现并修正问题。 
参考链接 
• https://www.youtube.com/watch?v=Lh_X32t9_po 
 
 
176 
热门演讲实录 | 落地和进化 
AI 驱动的智能化单元测试生成：字节跳动的
实践与创新 
 
在软件开发的生命周期中，自动生成单元测试成为提高代码质量和开发效率的关键
技术。在不久前举办的 QCon 全球软件开发大会（上海站）上，字节跳动质量效能专家
赵亮作了“基于 LLM 的单元测试用例自动生成”的精彩演讲，针对字节研发内部需求，
基于大模型技术结合深度程序分析，实现存量及增量单元测试的自动生成。通过真实业
务流量采集、单测框架能力和路径提升技术，有效解决单元测试的用例真实性和覆盖率
问题，提升测试用例的生成效率和代码覆盖率。此外，在断言工程、语法修正技术和效
果度量上，确保测试的准确性和可靠性；在支持快速迭代的开发流程中，显著提升研发
效率和降低迭代周期。 
 
分享嘉宾 赵亮  编辑 Kitty 
策划 QCon 全球软件开发大会 
 
177 
InfoQ 架构师2025年第一季 
内容亮点 
• 技术优势：结合真实数据流量，通过深度程序分析和大模型泛化生成能力的优势，
解决单元测试在日常研发中的编写慢、维护难的情况。 
• 模式优势：通过数据蒸馏及抽象化提炼高质量数据用于模型 SFT，结合真实业务
数据并以深度程序链路分析为辅助，提升单元测试在生成效果上的可测性、数据
真实性及用例有效性。 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。【PPT 不公开】 
我将与大家分享我们团队在大模型自动生成单元测试方面的研发经历，这个议题涵
盖了我们从问题识别到解决方案的全过程。我将这个议题分为六个部分。 
1. 痛点与现状。这部分涉及到我们为何要着手解决这个问题，以及我们与业务团队
合作时收集到的问题和痛点。这些痛点和现状是我们行动的出发点。 
2. 目标与挑战。基于前期收集的问题，我们设定了一系列目标，并在实现这些目标
的过程中遇到了一系列的挑战。 
3. 数据质量提升。分享我们为了提升单测生成效果所做的工作，以及我们如何通过
这些措施来提高整体效果。 
4. 代码生成效果提升。介绍我们采用的一些方法，以提升模型在生成单元测试方面
的性能。 
5. 效果演示。展示我们所取得的一些成果，让大家更直观地了解我们的工作效果。 
6. 总结与规划。总结我们在整个过程中的不足之处，并分享我们的一些思考和未来
的规划。 
现状痛点 
许多有编写单元测试经验的人普遍面临一个问题：业务代码的单元测试覆盖率普遍
不高。这并非因为开发人员不愿意编写测试，而是因为他们没有足够的时间。许多业务
的上线时间紧迫，留给开发代码的时间本就不多，更不用说编写单元测试了。在这种情
况下，开发人员往往将质量保障工作交给测试团队，但测试团队的主要精力在于验证需
求和功能的正确性，而不是逐行审查代码。 
 
178 
热门演讲实录 | 落地和进化 
我将这些问题归纳为三个主要部分： 
首先，编写单元测试耗时较长。在与业务团队的研发人员沟通时，我们发现编写单
元测试的时间可能比编写业务代码还要长，这取决于被测方法的数据复杂度和代码复杂
度。平均而言，编写单元测试的时间大约需要 5 到 15 分钟，取决于目标被测函数的
逻辑复杂度和数据构造的困难程度。 
其次，由于业务上线时间紧迫，研发人员可能同时处理多个需求，导致他们没有精
力保证单元测试的编写，这使得存量问题愈发严重。例如，我们内部许多业务团队由于
快速迭代导致大量未编写单元测试的代码，许多仓库的代码行覆盖率小于 10%，这使得
暴露给线上的风险非常大。 
最后，现有工具的效果不足。尽管许多公司和团队尝试通过工程化方法或基于搜索、
遗传算法的单元测试生成方式，以及近年来随着大模型的兴起，尝试使用模型生成单元
测试，但普遍存在几个问题：生成数据的可读性低，用例多样性不稳定，以及编译通过
率低。这些问题导致生成的单元测试存在许多编译或运行问题，增加了研发人员修正的
成本。 
目标及挑战 
目标 
对单元测试的现状和痛点，我们设定了明确的目标，并将其拆解为几个关键部分，
以期通过这些目标来解决我们面临的问题。 
首先，尽管模型和插件如 Copilot 在代码生成方面取得了一定的进展，但它们还不
能完全理解业务需求，也不能保证 100% 的编译通过率。因此，我们基于工程分析建立
了精准的数据分析基础，以此为我们的单元测试生成产品提供支撑。我们的目标是提高
工程化分析的准确性，通过将控制分析、数据分析和约束求解的方案相结合提升前期单
测数据语料分析的精确性。 
其次，我们对模型本身也设定了目标。我们正在尝试并将持续引入偏好对齐和强化
学习算法，以及已经在使用的思维链方式和微调技术，以持续提高模型在生成单元测试
 
179 
InfoQ 架构师2025年第一季 
方面的效果。 
最后，我们希望提升单元测试生成产品的用户体验。我们的目标是打造一个开箱即
用、快速且轻量化的单元测试工具，以降低业务研发人员的使用成本。 
在目标设定上明确了四个具体的目标：覆盖率目标、有效性目标（包括断言、mock 
和流量）、目标仓库覆盖目标，以及研发投入产出比目标。 
在代码质量与研发效能之间，我们面临着一个场景权衡。我们希望在保证代码质量
的同时，又不会牺牲较多的需求迭代时长。业务的快速上线对于市场份额的占领至关重
要，因此我们希望通过这套工具打破原有的平衡，既保障代码质量，又提升业务研发的
效率。 
挑战 
在研发过程中，我们遇到了不少挑战，其中两个尤为关键：数据质量和代码生成效
果。 
首先，数据质量是我们面临的一个重大挑战，它可以分为三个部分。第一部分是模
型训练，众所周知，训练数据集的质量直接影响模型的最终效果，这比选择哪种模型基
座或训练方式更为重要。第二部分是提示词的目标性，即使模型训练效果再好，如果提
示词不明确，模型生成的内容也可能出现幻觉或达不到预期的问题。第三部分是业务研
发侧关注的数据准确性、有效性和真实性。生成的单元测试所构造的数据必须与业务高
度匹配或近似，因为模型有时会生成一些不符合业务逻辑的数据，比如电话号码，模型
可能将其理解为基础变量而生成不真实的数字，这对于业务来说是不可接受的。 
其次，代码生成效果也是我们关注的重点。我们希望通过工程化的方式，结合现有
模型的能力，使代码生成在风格和业务语义上具有更好的包容性。这对于业务是否采纳
单元测试至关重要。我们期望生成的代码不仅在技术上无误，而且在风格和语义上与业
务需求相匹配，以便业务研发团队能够轻松地采纳和集成这些单元测试。 
 
 
 
180 
热门演讲实录 | 落地和进化 
数据质量提升：工程化分析解决数据难题 
数据充分度提升 
在提升数据质量方面，我们采取了一系列工程化的分析方法来协助模型解决数据难
题。我们主要关注两个问题：数据的真实性和充分度如何提升，以及如何确保模型的提
示词数据能够被准确理解。以下是我们面临的几个具体问题和相应的解决方案。 
首先，被测方法的入参构造非常复杂，不仅仅是基础变量，还包括复杂的结构体或
嵌套结构体，这使得在编写单元测试时构造数据的难度大大增加。其次，脱离业务理解
生成的数据价值不高，这是我们与业务团队沟通后得出的结论。第三，出入参的相关性
不足，有些函数依赖大量数据库、外部 RPC 或三方组件，这些依赖数据的变化会导致
目标函数的条件路径发生变化，这是一个关键环节。最后，断言的目标性是否准确将决
定生成的单元测试是否具备一定的有效性和通过率。 
针对这些问题，我们提出了一个三部分的解决方案。第一部分是基于流量来源的收
集和采纳。首先，我们与业务团队沟通后发现，真实的流量主要来自真实的手工测试或
线上业务流量，这是一部分来源。其次是接口自动化，其维护成本和时间成本相比编写
单元测试要低一些，通过业务真实定义的接口自动化能够实现函数链路间的调用，从而
实现函数级数据的采集。最后是测试技术服务，包括模糊测试和端到端全流量回放技术，
结合代码插桩技术，拦截代码中的出入参和依赖参数。从而获取数据流量中具体参数值
信息作为单测生成的数据来源。 
第二部分是流量蒸馏。这些数据不仅用于单元测试生成对应的出入参构造，还有一
部分数据将作为模型训练和提示词工程的数据语料。因此，我们需要对数据进行基本类
型策略的管理和拆解，包括数据的分析、路径推导、导入导出类型的拆解，以及后期的
数据加工和合规隐私处理。由于我们获取的线上流量中包含真实用户数据，我们需要对
这些数据进行脱敏，确保数据的真实性不被泄露。 
第三部分是流量分发。经过流量蒸馏后的数据将分发给几个场景，包括存量生成、
面向 IDE 的数据生成，以及面向 MR 流水线的单元测试生成过程。 
通过实施我们的数据充分度提升方案，我们取得了一些显著的数据结果。例如，在
 
181 
InfoQ 架构师2025年第一季 
没有使用流量构造方法之前，生成的单元测试中包含了一个 mock 构造函数，这个函数
虽然具备了基本的框架，但是其内部的数据结构字段缺失严重，模型并没有完全理解它。
当我们引入流量构造方案后，情况有了显著的改善。新的数据结构中包含了大量丰富的
数据语料，这些数据更加贴近真实业务场景，因此业务团队对这些单元测试的接受度和
采纳率大大提高。与以往仅依赖模型或工程方法相比，我们在数据利用率和用例真实性
占比上都有了明显的提升。首先，我们看到了用例的可信度得到了显著提升。其次，用
户的采纳率也有所增加。最后，问题的发现率也有所提高。 
等价类提升思想 
在进行单元测试时，我们不仅关注于发现问题，还致力于提升测试覆盖率。为此，
我们采用了等价类设计等方法，并在前期对现状问题进行了梳理。以下是我们关注的几
个关键部分： 
1. 被测方法中存在许多复杂路径，我们需要识别这些路径，并确保模型在生成过程
中能够覆盖到这些路径。这是一个挑战，因为路径的复杂性可能会导致测试用例
无法覆盖到所有可能的情况。 
2. 模型推理过程中非常依赖于提示词的准确性和完整度。如果提示词不准确或不完
整，模型生成的测试用例可能无法达到预期的效果。 
3. 路径条件的差异可能导致断言失败。例如，如果预期是通过路径 1 或 2 到达断
言点，但实际生成的测试用例却通过路径 2 或 4 到达，那么断言可能会失败，
导致业务团队需要修复大量的失败案例。 
4. 异常路径的考虑。在时间紧张的情况下，开发人员可能只会编写主要路径的单元
测试，而忽视了异常路径，如异常捕获或难以到达的小模块路径。这些路径的参
数构造复杂度高，且难以覆盖。 
5. 对于已有的单元测试，我们的目标是提升那些测试覆盖率较低的存量代码。我们
需要确保新生成的单元测试与业务代码风格保持一致，并且避免重复已有测试用
例已经覆盖的类目。 
我们目前的解决方案是将模型和工程方法相结合，以解决单元测试中的路径覆盖问
题。以一个具体的函数为例，该函数包含多个调用分支路径。我们的目标是让测试能够
覆盖到特定的路径，比如 ACD 或 ACE 路径。在拿到原始的被测方法后，我们会通过工
 
182 
热门演讲实录 | 落地和进化 
程化的方式将其调用路径拆解出来。拆解后，模型会对 mock 层和依赖返回数据做出决
策并生成测试用例。生成测试用例后，我们会在闭环迭代中持续优化矫正，直到测试路
径达到预期条件。在这个过程中，我们会结合静态代码分析的能力，识别未覆盖的路径
和条件是否真正达到了我们的生成预期。如果达到了，那么这个测试用例就可以被认为
是符合预期的。 
在整个拆解过程中，我们分为三个部分：被测函数的要素分析、逻辑分析和路径分
析。我们会利用 AST 和 IR 等程序分析技术，对代码中的控制流和数据流进行条件组合
关系的前期拆解。具体来说，我们首先将被测方法中的入参、出参和中间变量参数拆解
为可控变量。通过对这些可控变量的分析，我们能够完成数据流的前期分析工作，为后
续的测试用例生成打下基础。然后，在逻辑分析阶段，我们关注于代码中的分支类型控
制组合以及变量溯源过程。这一步骤涉及到理解代码中的逻辑结构，包括条件语句、循
环语句等，以及变量是如何在这些逻辑结构中被操作和传递的。最终，我们将这些分析
结果综合起来，进行路径分析。路径分析的目的是识别代码中所有可能的执行路径，包
括路径 X、路径 Y 和路径 Z 等。这些路径的前期拆解条件对于模型来说至关重要，因
为它们为模型提供了在生成不同路径的单元测试时所需的详细信息和依据。 
数据的效果和模块的价值在这个过程中非常重要，它们不仅为模型推理提供了语料
补充，还为断言生成提供了支持依据。此外，这些数据对于我们后续计划中的存量单元
测试的保鲜也是一个重要的环节。 
模型与程序分析的融合 
在我们的工作中，模型与程序分析的融合是一个关键的发展方向。在这一过程中，
工程化的方法和模型各自承担着不同的角色。这个过程可以分为三个阶段：首先，在程
序分析的前期，我们需要对目标函数的语料进行拆解和解析。这一步骤是基础，为后续
的模型生成框架补充和路径提升提供必要的数据。接着，模型会利用这些解析后的数据
进行生成框架的补充和路径的提升。在单元测试生成过程中，由于不能保证所有生成的
测试都能一次性通过，因此还需要进行语法修正。最后，对于断言部分，我们也有相应
的修正策略。 
 
183 
InfoQ 架构师2025年第一季 
 
总结这一融合过程，我们可以从两个方面来看其优势。首先，工程化方法的优势在
于其对代码分析的准确率和稳定性。程序分析技术在国内已经应用多年，因此在分析的
准确性和稳定性上具有很高的水平。由于单元测试需要实际运行来验证效果，工程化方
法具备检测运行的能力。其次，模型的优势在于其灵活性和泛化效果。模型在灵活决策
和泛化效果上比工程化的固定策略要好，这也是现在很多智能代理所做的事情。 
这里我总结了两个关键点以及模型生成和模型修复所需的语料条件： 
首先，当模型生成时，它需要一些特定的条件数据，包括路径数据、函数签名以及
依赖结构体的定义。这些数据对于模型来说是至关重要的，因为它们构成了模型生成所
需语料的基础。 
其次，在模型修复方面，我们需要修复在编译或运行过程中遇到的问题。例如，如
果在运行过程中出现了错误堆栈，我们需要将这些具体的报错信息提供给模型，以便它
能够更好地理解并进行修复。这包括错误相关的数据结构和错误类型。简而言之，当我
们希望模型进行修复时，提供给它的信息和语料越详细，模型的准确性就越高，同时它
需要考虑的约束范围也就越小。 
代码生成效果提升：模型化建设提升代码效果 
这一部分重点介绍我们在单元测试生成过程中，通过模型实现的一些关键进展，可
以分为四个主要部分： 
 
184 
热门演讲实录 | 落地和进化 
1. 模型泛化效果如何提升。这是至关重要的，因为它决定了模型在面对未知代码时
是否能够生成有效的测试用例。 
2. 模型提示词选择策划。在生成过程中，选择合适的提示词对于引导模型生成高质
量的测试代码非常关键。 
3. 无流量单侧方法如何生成有效的单测数据。尽管我们尝试使用真实的业务流量来
生成单元测试，但在这个过程中，我们遇到了一些挑战。例如，一些业务的流量
非常有限，或者某些方法根本没有被执行到。此外，由于工程的不稳定性，我们
收集到的数据往往是不完整的。因此，我们研究了如何利用模型来补全这些无流
量的数据，以便对未被测试的方法进行测试。 
4. 模型效果生成评价。对模型的评测是能够持续不对的矫正和确认效果是否提升的
主要方式，将辅助我们持续调整正确的方向。 
模型工程整体架构 
模型工程的整体架构。最底层是数据环节，包括数据的收集和训练评估工作，其中
有些工作仍在进行中。 
我们的数据训练策略分为三个步骤： 
STEP1 训练样本构建。我们选择了内部大量的代码库作为数据来源，这是因为不同
公司在编写单元测试时会使用自己的测试框架和标准，这有助于模型更好地理解公司内
部的标准和组件使用策略。其次，我们选择了 GitHub 上星标高的项目作为训练目标，
以确保数据的质量和多样性。第三，对于数据量有限的小场景或特殊组件使用情况，我
们会借助市面上效果较好的模型，如 GPT，来扩充我们的模型。第四，我们正在进行基
于反馈的优化，但这部分的优化工作尚未完全启动。 
STEP2 模型的选用。经过前期的尝试，我们最终选取了一款适用于本项目的模型作
为后期持续训练基座。在提示词的选取上，我们从单纯的 one-shot 或 few-shot 方案转
变为结合思维链的方案，这在路径提升、语法修正和断言生成方面取得了显著的效果。 
STEP3 模型的评测及优化。我们采用人工主观评测和机器评测相结合的方式，通过
评测结果进行偏好打分，并将优质数据反馈给模型进行多轮迭代，以持续提升模型效果。 
 
185 
InfoQ 架构师2025年第一季 
 
数据工程建设 
在数据工程建设方面，我们需要确保数据集的可信度和纯净度。我们希望数据集中
包含多种编程语言，以覆盖不同的编程阶段。同时，我们将其细分为六个关键环节，以
确保数据的质量和有效性，从而提升大模型的理解能力和泛化效果。 
1. 样本打标，是至关重要的一步。通过为样本打标，我们帮助大模型更好地理解数
据，同时，打标数据的多样性也有助于进行多样化评估。这种评估能够使模型在
更广泛的模式中学习，进而提高模型的泛化能力。 
2. 样本筛选，采用业界标准做法，排除低质量数据。 
3. 隐私过滤，处理隐私相关问题，确保数据的合规性和安全性。 
4. 格式处理，数据风格的统一和特殊字符的处理。我们进行一系列的数据优化工作，
以确保数据的一致性和可读性。 
5. 数据简化，是我们特别关注的一个环节，尤其是在代码简化方面。这不仅有助于
单元测试的生成，也提升了模型训练的效果。通过简化数据，我们能够更有效地
训练模型，使其能够更快地学习和适应。 
6. 数据配比打乱，确保数据来源多样性的关键步骤。由于数据往往是分块收集的，
我们需要混合这些数据，以实现多元学习的效果。这种混合数据能够提供更全面
的学习场景，进一步增强模型的性能。 
在代码简化方面，我们面临的一个主要挑战是业务代码中的业务语义非常多。这些
 
186 
热门演讲实录 | 落地和进化 
代码在字段命名、方法体命名以及变量命名上都具有很强的业务特定性，这对于模型来
说增加了额外的学习成本。为了解决这个问题，我们采用了一些来自业界和学术界的优
秀思路，制定了一个简化方案。 
我们的方案主要包括两个部分。首先，我们尝试从代码中抽取关键数据，保留函数
声明、函数调用语句、控制流和变量信息，以及 return 信息。这样，我们可以剔除那
些复杂的日志处理和不影响路径的无关内容，从而简化代码。其次，我们对那些业务语
义重的字段进行转译。例如，将“中国建设银行”这样的字段转译为 Y0、Y1、Y2 等，
这样做既保留了代码的原貌，又不会影响代码逻辑。通过这种方式，我们可以减轻模型
训练的负担。 
此外，我们还注意到不同编程语言中，如 Python、Go、Java 或 C++，这些语言对
应的 print 函数的实现方式各不相同。因此，我们对这些不同编程语言的差异进行了简
化，统一为模型可识别的 print 操作。在模型完成代码生成后，我们再根据之前的转译
规则将代码转换回原始形式，以获得更准确的结果。在代码简化之前，我们观察到 loss 
的下降和收敛效果非常慢。但在实施代码简化后，与之前未简化的代码训练过程相比，
loss 的下降趋势明显改善。这表明代码简化对于模型训练的效率和效果都有积极的影响。 
PE 工程及模型微调 
PE 工程路径规划覆盖了多个方面，包括对被测代码的理解、路径识别、入参理解，
以及 mock 规划，这些都是为了确定哪些 mock 会导致不同路径的执行。最终，我们
让模型学习并生成求解。在这个过程中，我们将工作细分为四个主要部分：路径提升、
参数补全、语法修正和断言修正。实验结果表明，不同的提示词方式对效果有显著差异。
例如，在参数补全方面，如果提供样本，模型可能会模仿这些样本来构造输出，这会影
响其在真实业务语义上的泛化效果。相反，使用 zero-shot 的方式，即不给模型任何样
本，有时会得到比预期更好的结果。 
我们目前主要处于模型微调阶段，我们还希望通过结合微调和奖励模型来进一步提
升效果，未来可能会涉及到 PPO，但鉴于资源和成本的考虑，我们尚未开始这一阶段。
我们的目标是生成一个策略模型实现持续的强化学习反馈和单测生成的动态决策。 
 
 
187 
InfoQ 架构师2025年第一季 
评测工程建设及效果 
我们针对评测集设计了不同阶段的专项评测集，以便模型进行更精确的评测。 
在评测工程建设方面，目前已经实施了一系列基础评估工作，这包括了之前提到的
人工评估，以及我们针对业务和产品制定的五个评测指标：编译通过率、覆盖率、断言
成功率、运行通过率，以及路径提升效果，我们通过这些指标来进行综合评估。 
目前，我们正处于基于 DPO 的多轮偏好学习提升阶段，这是我们期望通过持续优
化来改善的效果。在训练效果方面，与原始模型相比，我们在断言修复、语法修复和路
径提升方面取得了更好的成果。尽管在语法修正方面，我们的效果仍然略逊于 GPT 模
型，但我们正在不断优化数据和训练方案，以期提升这一方面的表现。 
效果度量及演示 
我们对度量工作进行了细致的拆分，主要关注两个方面的提升：覆盖率和场景支持。 
目前，我们在仓库级的覆盖率上已经取得了显著的成绩，平均覆盖率达到了 40%。
通过采用基于 DPO 的偏好方案进行重训后，部分仓库的覆盖率甚至提升到了 60%。这
是在我们已经生成过的仓库基础上，通过重新生成得到的成果。在断言通过率上，我们
也达到了一个较高的水平，单个方法的覆盖率提升到了 83.09%。 
在场景支持方面，我们结合工程实践来辅助模型，包括在指标统计和数据优化策略
上，都提供了策略上的支持。我们的目标是向业务研发交付一个无需二次介入修正的单
测生成产品。这意味着提供的单元测试在编译、运行、语法和断言上都不存在问题，业
务研发只需审查生成的单元测试是否符合预期即可。 
最关键的是，我们需要确保每个测试用例对真实路径的提升都是有价值的。我们模
拟了整个单元测试的生成流程。这包括流量获取、路径选择、单元测试生成、语法修正、
检测、断言修复以及路径提升识别。如果一个测试用例已经提升了路径，我们就认为这
个案例是成功的，并继续下一个路径的提升。在断言检测过程中，我们发现有些断言并
不符合预期，例如，预期为 false 但实际上生成的结果是 true，这也是需要我们持续优
化和矫正的场景 case。 
 
188 
热门演讲实录 | 落地和进化 
而对于业务的诉求而言，例如，我们向业务交付的仓库之前没有任何单元测试，这
也是他们的一个痛点。通过我们的生成，目前效果达到了 63.92%，生成了 2,627 个测
试用例。面对生成的 2,627 个单元测试用例，我们深知业务后续的维护工作将面临挑战，
因为维护这些测试的成本相对较高，而且逐个检查每个测试用例的难度也较大，不如自
己编写的代码容易理解。为了解决这一问题，我们已经规划并实施了一系列措施，将在
可维护性和可读性上为用户持续提升便利。 
总结及规划 
总结目前我们已经完成的工作。首先在基础层上我们完成的工作包括能力分析、数
据构建，以及环境问题的解决。这些基础工作为后续的测试维护提供了坚实的支撑。第
二层生成层，这涉及到整个单元测试框架的生成、路径提升以及数据转换过程。通过这
些优化，我们提高了测试生成的效率和质量。第三层修正层，这里我们实施了一些修正
方案，包括语法修正、运行修正以及断言修正。这些方案有助于确保生成的测试用例在
语法和逻辑上的正确性，减少后续的维护工作。最上层是统计 & 应用层，提供业务产
品化体验能力，360°可视化度量体系，提升可观测性。 
在规划方面，我们目前的工作重点包括模型的持续优化、用例召回分析、用例保鲜
机制以及产品多元化。这些是我们持续进行的工作，其中用例召回分析和用例保鲜机制
是我们目前特别关注的两个方面。其中， 
用例召回分析是指我们生成了许多测试用例后，如何检测这些用例的召回有效性是
否具有价值。这些用例会在业务的日常迭代的持续集成（CI）环节中运行。如果运行后
出现失败的用例，我们需要分析失败的原因。目前，我们的初步思路是结合模型来识别
关键日志和平台性数据中的异常，进行问题的深入分析。这个过程中，我们会进行降噪，
因为 CI 中的用例失败可能不仅仅是问题的发现，还可能因为环境因素或平台因素导致，
这些都需要我们进行降噪处理。 
用例保鲜则是指如何在日常业务研发中持续保证上述示例介绍的 2,000 多个用例的
有效性和正常通过率。这是一个我们正在努力的目标，目的是确保这些用例能够适应业
务的变化，保持其准确性和相关性。 
 
 
189 
InfoQ 架构师2025年第一季 
嘉宾介绍 
• 赵亮，13 年工作经验，先后在蚂蚁集团余额宝质量技术和研发效能任职，现就职于
字节跳动质量技术团队，担任质量内建智能化场景技术负责人，曾发表 4 篇国家技
术专利。在质量技术、程序分析以及智能化相关场景的应用上有丰富的项目经验和
落地成效。 
 
 
190 
热门演讲实录 | 落地和进化 
去哪儿网前端代码自动生成技术实践 
 
在 AI 时代，我们重新挖掘了前端开发的提效方向，从去哪儿网的三大业务场景：
机酒主流程业务、营销业务、后台服务三大业务场景，提出了 D2C 方案和基于 AI 生
成后台代码的方案。 
D2C 方案中，我们着重解决了代码的可用性，包括固定宽度、布局精准划分、代码
语义化等方面进行了代码健壮性和可维护性的提升，业务需求出码率达到 36%。 
基于 AI 生成代码的方案中，需求文档和接口 API 组成的 Prompt，依赖 GPT 生
成页面渲染和业务逻辑代码，重点解决了 Prompt 的高效生成以及复杂页面代码的有效
生成，业务需求代码出码率达到了 55%。 
分享嘉宾 姚佳梅  编辑 Kitty 
策划 QCon 全球软件开发大会 
 
191 
InfoQ 架构师2025年第一季 
在不久前举办的 QCon 全球软件开发大会 上，去哪儿网前端技术总监姚佳梅带来
了精彩的专题演讲“去哪儿网前端代码自动生成技术实践”，围绕去哪儿网三大业务场
景的前端代码生成的方案，涵盖了代码生成中的难点和解决思路、AI 的应用和业务的应
用提效等，希望能给大家带来一些帮助和思考。 
内容亮点 
• 设计稿生成代码在传统算法方案的基础上加入 AI 应用 
• 区分 C 端和 B 端两种场景方案，覆盖业务范围更广，有实际应用效果 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。 
我将从我们的实际业务开发角度出发，探讨我们是如何利用大型模型在大前端领域
实现代码生成，以提高开发效率。我目前负责的是去哪儿网国际机票的前端业务，同时
也涉足低代码和营销业务领域。我的工作重点之一就是从业务角度出发，辅助前端开发
提效，包括代码生成技术的应用。 
今天，我将围绕《去哪儿网前端代码自动生成技术实践》这一主题，展开我的演讲，
内容分为四个部分，首先，阐述一下我们的研发现状以及目标达成情况，接着，将分别
从 C 端和 B 端的角度探讨我们的解决方案，以及在实践过程中遇到的问题。最后，将
分享一下我们的未来规划。 
研发现状和目标达成 
C 端和 B 端的业务场景分析 
我们 C 端的业务场景主要分为两大块：主流程业务和营销活动。 
主流程业务涵盖了用户在使用我们的系统购买机票、酒店等各类产品时所接触的所
有页面。这些页面的 UI 复杂度极高，开发过程中，从 UI 编写到最终验收，需要不断
地调整细节，这个过程既繁琐又耗时。其次，C 端业务涉及的客户端类型多样，包括 
APP 端、小程序、PC 端和 H5 等。这些不同的客户端都需要我们去适配和开发，进一
步增加了工作的复杂性。 
 
192 
热门演讲实录 | 落地和进化 
营销活动的页面的 UI 复杂度也很高，并且很多页面涉及到复杂交互设计，如秒杀、
砍价等。在开发这些营销活动时，我们遇到的痛点主要有两个：一是时间紧迫，比如五
一、十一等节日活动，节前必须上线；二是活动量巨大，尽管我们已有低代码平台来解
决部分问题，但活动形式的不断更新，使得我们的开发工作量依然巨大。 
B 端业务主要指的是我们公司内部使用的各类后台管理系统。这些页面以大量的表
单和表格为特征，信息密度极高，内容丰富。随着公司业务的增长，人力资源变得紧张，
大量的需求积压。因此，我们开展这项代码生成工作的首要目标是提升前端开发效率。 
以往的工作流程 
在前端开发流程中，无论是 C 端还是 B 端，我们都是从需求 PRD（产品需求文档）
开始的。对于 C 端，我们通常会有 UI 设计稿，这为我们的开发流程提供了视觉参考。
在 C 端开发中，我们首先编写渲染代码，包括布局和样式。渲染代码的开发时间与逻
辑代码的开发时间大致相当，各占一半。 
 
B 端在大多数情况下，是依赖于需求文档和原型稿进行开发。在 B 端开发中，我
们可以使用现有的 UI 框架来实现样式，因此在渲染上花费的时间会比逻辑代码少，逻
辑代码的开发时间会占据更大的比例。 
AI 重塑后的开发流程 
在引入 AI 技术重塑我们的开发流程之后，我们的工作方式发生了显著变化。过去，
我们是从零开始编写代码，但现在，我们首先获取必要的物料，比如 UI 设计稿和需求
文档。接着，我们会进行代码生成，基于这些生成的代码，进行后续的开发工作。我们
会对生成的代码进行检查，确保功能没有遗漏，并且确定是否有 AI 无法生成的部分，
 
193 
InfoQ 架构师2025年第一季 
这些部分就需要我们进行二次开发。 
C 端代码生成出码率是 36%，B 端的出码率达到了 55%。出码率是指需求中被应
用的自动生成的代码行数占需求上线总代码行数的比例。例如，一个 C 端需求上线了 
1000 行代码，其中有 360 行是智能生成的。 
 
代码生成案例 
C 端以一个页面为例，这个页面包含了与后端的交互接口数据以及用户点击和不同
状态的交互逻辑。如果从头开始开发，这样的页面通常需要 1 到 2 天的时间。然而，
通过我们的 AI 代码生成系统，这个页面的开发可以在短短一个小时内完成。我们的系
统首先要求上传 UI 设计稿。上传后，系统大约几秒就能生成渲染代码，同时提供实时
预览效果。UI 设计稿在左侧展示，而右侧则是实时预览的效果。这样，我们就能直观
地看到生成的渲染代码与设计稿的匹配程度。接下来，我们结合产品需求文档生成一个
可交互的 Checklist，这个 Checklist 帮助我们对需求点进行详细分析，梳理成一个完善
的结构，为后续的代码生成打下基础。这份 Checklist 的数据会传递给 GPT，由它来生
成逻辑代码。这部分代码包括页面与后端的交互以及页面本身的交互逻辑，比如点击某
个按钮后的行为，或者页面的埋点信息等，基本上页面的大部分交互逻辑都能通过 AI 
生成。 
B 端的案例页面包含了表单、表格，以及一个树形结构和搜索框，页面的另一侧是
节点的详细内容。如果从头开始开发，这样的页面同样需要 1 到 2 天的时间，而我们
的 AI 代码生成系统也能在一个小时内完成开发。我们在一个系统中新建页面，然后将
 
194 
热门演讲实录 | 落地和进化 
页面的所有模块进行拆分。针对每个模块编写提示词，并将这些提示词发送给 GPT 以
生成代码。这个过程支持多轮对话，如果生成的代码不够好或者有功能遗漏，我们可以
继续与 GPT 对话，指导它进行修正。所有模块生成后，会跳转到我们的在线编辑页面，
左侧是编辑区，右侧是实时预览区。GPT 生成的实时预览页面的初版整体可用性已经非
常高，所有的操作，比如点击查询后表单的显示，以及表格的生成，都是从真实后端接
口数据中获取的，而不是一个静态的页面。 
C 端业务场景解决方案 
在 C 端业务场景中，我们对代码生成的要求主要有两个方面。首先，由于 UI 设
计稿的样式复杂，我们需要高还原度的代码以确保代码的可用性。其次，考虑到主流程
业务的更新迭代，我们对生成的代码质量有较高要求。为了进一步提升效率，我们还希
望生成业务和交互逻辑代码。 
在技术方案选型上，我们首先考虑了从设计稿到渲染代码的 D2C 方案。业界已经
有很多成熟且优秀的 D2C 方案，我们进行了调研和试验，这些方案的 UI 还原度普遍
在 90% 以上。然而，仅仅有高还原度是不够的，我们还需要考虑到代码的布局合理性、
避免固定宽度以及语义化等问题。这些代码在简单图文场景下或许足够，但对于需要后
续更新迭代的主流程代码来说还不能满足需求。此外，大多数优秀产品不开源，这意味
着我们无法基于它们进行优化。 
考虑到 AI 技术的流行趋势，我们也探索了现有的大模型是否能够从设计稿生成高 
UI 还原度的代码。我们尝试了多个案例，并不断优化提示词，但效果大同小异。在对 
UI 还原度要求极高的场景下，仅提供设计稿生成的效果往往达不到我们的要求，许多
细节模块可能缺失。同时，在业务需求上线前，我们缺乏对 UI 稿有效的脱敏手段，无
法将其直接提供给外部。 
我们进行了其他方向的探索。一方面，我们针对 D2C 方案中语义化不足的问题，
在大模型中进行了尝试，效果非常好。另一方面，我们利用大模型对自然语言处理的优
势，根据文字描述生成交互逻辑代码。最终，我们选择了一个综合方案：自研一套 D2C 
方案，重点解决布局不合理、固定宽度等问题，并结合 AI 优化渲染代码，同时利用 AI 
根据需求文档生成逻辑代码。这样的方案既满足了我们对 UI 还原度和代码质量的要求，
 
195 
InfoQ 架构师2025年第一季 
也提高了开发效率，并且能够适应后续需求的更新迭代。 
系统架构 
我们的代码生成架构是一个自上而下的流程，它包括用户输入、中间生成、以及最
终的目标代码输出。用户输入部分，我们目前支持 Sketch 和 Figma 这两种 UI 设计软
件的稿件，以及需求产品文档。 
从用户输入的 UI 稿件到中间的 D2C 描述及渲染代码的生成，整个过程是一个典
型的 D2C 转换。我们首先获取 UI 稿件，然后对其中大量的图层数据进行解析，生成
我们所需的 DSL 结构。在这个结构基础上，我们进行布局和样式的生成，最终得到包
含布局和样式的 DSL。之后，我们将这个 DSL 翻译成目标代码，目前我们支持的目标
代码包括 React Native、Shark（自研的跨端营销活动框架）、Taro 以及 ArkUI。在获得
渲染代码之后，我们利用 GPT 进行语义化处理，以及代码组件的拆分和列表数据的循
环。完成这些步骤后，我们再结合需求 PRD 和本地 SDK 代码库依赖 GPT 进行逻辑代
码的生成。所有代码生成完毕后，开发会进入检查和补充开发环节。需求经过测试上线
后，就会触发出码率的自动计算。 
 
本次分享，我将重点介绍 D2C 过程中遇到的难点以及解决方案，同时展示 AI 如
何提升代码质量，如何实现从 PRD 到代码的转换。 
 
196 
热门演讲实录 | 落地和进化 
D2C 难点与解决方案 
在 D2C 的过程中，我们遇到了以下几个难点， 
1. 布局切割不合理。例如，当前这个例子我们期望的布局是 5 组，其中图片和标
题应该是一组，并且能够用 list 数据循环渲染出来。但是 D2C 可能将它们分割
成不同的部分，图片和文字被分离，或者因为宽度和间距的原因，图片被分成两
组。这样的结构对于前端开发来说是不可接受的。 
2. 大量的 Absolute 定位问题。由于图层交叉，D2C 可能会产生大量的绝对定位，
这会导致后续布局难以编写。 
3. 固定宽度问题也是一个挑战。D2C 生成的模块是固定宽度，这在不同分辨率的
屏幕上可能会导致内容被截断或者铺满整个屏幕。 
4. Class 类名不语义化的问题。D2C 算法生成的类名，如 view0、view1、view2、
text0、text1 等，缺乏有效含义。 
针对这些问题，我们采取了以下解决方案： 
布局切割优化 
我们使用了业界常用的投影算法，依据侧投影进行水平和垂直两个方向的切割。结
合投影算法和一些算法规则，解决了大部分的布局切割不正确的问题。下面主要讲解三
类应用最多的算法和方法： 
1. 间距聚类，我们引入了 K-Means 聚类算法，在这个例子中，将水平切割产生的
模块的间距作为输入值，输出不被切开的间距，从而保持图片和文本作为一个整
体。 
2. 切割方向的选择：我们采用了轴向布局验证算法，选择切割方向的交叉轴方向，
查看所有节点的对齐方式是否一致，选择不对齐节点数最少的方向进行切割。 
3. 识别组件辅助精准进行切割，我们识别出实际的组件，进行布局的合理生成。例
如在这个例子中，我们识别出 Toast 组件并将其移除，然后针对剩余的组件进
行是否为列表的识别。识别为列表的判断依据包含：从 DSL 判断子元素的相似
度、是否等距以及面积是否相等。如果识别为列表，直接按照列表生成布局。 
 
197 
InfoQ 架构师2025年第一季 
Absolute 布局的优化 
在 Absolute 布局的优化时，我们面对的主要问题是图层交叉导致的绝对布局问题。
我们分析三个典型的案例，并为每个案例制定了相应的解决方案。 
1. 紧凑文本。从视觉上看，设计稿中的文本并不交叉，但如果我们查看图层的实际
高度，就会发现存在细微的交叉。如果整个页面都使用 Absolute 布局，代码的
健壮性会非常差，稍微长一点的字段就会遮盖后面的内容。针对这种情况，我们
的处理方式是调整行高，使得文本从交叉变为不交叉，从而能够正常地进行布局
切割。在完成正常布局后，再在属性上还原行高。 
2. 去除干扰元素。例如，一个浮标组件本身使用 Absolute 布局是没有问题的，但
由于图层交叉，使得下面的元素也变成了 Absolute 布局，这影响了我们的代码。
在这种情况下，我们会先移除浮标，让下面的元素正常参与布局，然后再以 Ab-
solute 布局将其还原回来。 
3. 背景交叉问题。这个例子中，由于背景图层是渐变色，与下面的入住信息模块产
生了交叉。我们通过拉伸背景图，将其从交叉关系转变为父子关系，这样就能进
行正常布局计算。 
固定宽度的优化 
在前端开发中，固定宽度的问题常常导致兼容性问题，尤其是在不同分辨率的屏幕
上。如果给元素设置了固定宽度，实际业务数据中的几个额外字符可能会导致内容换行
或挤压旁边的元素。即使我们生成了代码，这种固定宽度的问题也可能使得代码无法使
用，进而影响我们的出码率。 
为了解决固定宽度的问题，我们采用了弹性布局的方法。原本的布局生成依赖于元
素宽度固定值和 Margin 来维持元素间的间距，但这并不是我们想要的结果，因为实际
开发中我们不希望有过多的 Margin 或元素宽度固定值。因此，我们结合元素本身的信
息、它与兄弟节点和父节点的位置关系，计算在 Flex 布局下应该使用的属性。也会根
据一些条件判断元素的宽度是否确实需要保留。 
我们也对算法生成的样式代码进行了优化，包括提取公共样式和删除冗余样式，以
减少代码中的重复和冗余。 
 
198 
热门演讲实录 | 落地和进化 
AI 辅助代码优化 
在拿到渲染代码后，我们利用 GPT 进行了一系列 AI 辅助的代码优化。这些优化
主要包括两个部分：语义化和代码结构优化。 
语义化优化 
在编写 class name 时，开发者通常会选择像 main-container、ticket-section 这样具
有描述性的名称。而 D2C 算法生成的可能是 view008 这样的无意义标识符。GPT 帮助
我们将这些无意义的标识符转换成更易于理解和接受的语义化名称。实现这一过程，我
们输入了一个简化的 DSL，并通过编写提示词让 GPT 生成新旧类名的映射。在获得新
的类名后，我们会进行校验，确保没有重名或遗漏处理的类名，然后将这些回填到渲染
代码中。最终的语义化结果比较符合开发者命名习惯。 
AI 代码结构优化 
D2C 生成的代码在实际应用中还需要进一步的组件拆分。例如，一段代码可能需要
被拆分成一个 Tab 组件和一个列表循环。GPT 在这方面也提供了帮助，它不仅帮助我
们进行组件拆分，还识别出了列表（list）和列表项（list item）。GPT 生成的代码能够
根据 map 数据结构直接进行循环，并将业务数据与后端数据结合填充进去。与原始代
码相比，GPT 优化后的代码整体可用性大大提高。 
UI 还原度 DIFF 工具 
我们开发了一个 UI 还原度 DIFF 工具，旨在帮助开发人员更高效地检查和修正代
码的 UI 还原度。这个工具的推出基于两个主要考虑：首先，开发人员需要一个方法来
验证我们的 UI 还原度是否达到了预期；其次，由于生成的代码并非 100% 还原，开发
人员在需要进行修正时，手动比对会非常耗时。这个 DIFF 工具提供了一个整体的还原
度数值，这个数值是基于像素维度的对比得出的。此外，工具中还有一条可以拖拉的线
条，通过拖动这条线，我们可以看到重影部分，这些重影是我们生成的渲染效果与 UI 
设计稿效果的叠加。通过观察这些重影，我们可以判断出哪些元素没有对齐。点击这些
元素后，开发人员可以精确地知道每个元素偏差了多少像素，宽度少了多少像素，这样
也可能快速对样式进行修正。 
 
199 
InfoQ 架构师2025年第一季 
P2C：逻辑代码生成 
在渲染代码生成之后，我们需要让这份代码具有交互性和逻辑性，使其能够动态地
响应用户操作。这就是我们的 P2C（PRD to Code）部分所要解决的问题。让我们来看一
下整体的生成流程。 
首先，页面的所有交互行为和业务逻辑都源自于需求 PRD。我们首先对 PRD 进行
解析，然后利用 GPT 梳理优化生成一份结构化的数据后，导入到 Checklist 中。这份 
Checklist 数据将在本地知识库代码中检索出会用到的代码 SDK。结合刚刚生成的渲染代
码以及接口数据，构建整个 prompt，进行数据脱敏后，让 GPT 进行代码生成。 
在整个过程中，我们分析了两个主要的难点。第一个难点是如何有效地处理需求 
PRD，提取信息并将其结构化。PRD 的功能描述是否完整是一个挑战。第二个难点是如
何从需求 PRD 匹配到现有的私有代码库的 SDK。由于 SDK 数量众多，可能多达几百
个，如果全部提供给 GPT，将会造成 token 的大量浪费。SDK 举例：登录校验、接口
调用封装、分享、埋点等。 
 
需求 PRD 生成 Checklist 
我们的需求 PRD 生成 Checklist 的过程开始于 Wiki 平台，我们会读取 Wiki 上的
 
200 
热门演讲实录 | 落地和进化 
文档，从上到下解析整个文档，提取出我们需要的内容。由于我们的需求 PRD 遵循一
定的格式，我们可以准确地提取出所需的部分。提取出来后，我们会将其格式化为我们
自己制定的格式，并进行数据脱敏。 
接下来，我们需要编写 prompt，引导 GPT 产出一份 Checklist 所需的数据。这个 
prompt 过程实际上也是一个 COT 过程，我们要求 GPT 进行步骤拆解，构造出多层级
结构，并确保每个步骤都是独立且完整的。根据我们的需求，GPT 会生成包含模块名、
case 名、步骤和结果的格式化数据。 
得到的这份结构化数据随后被输入到我们的 Checklist 系统中，生成一个可交互的
结构。这个可交互的结构允许开发人员手动进行修正，比如通过拖拽调整条目的位置，
或者编辑补充缺失的内容。这个界面的设计是为了提高后续代码生成的准确性，而且这
份 Checklist 也能用于后续的测试用例生成。 
SDK 动态匹配 
这里是 RAG 的应用，它主要分为三个核心部分。 
首先是知识库存储。由于 SDK 的数量较大，每个 SDK 都包含描述其功能的文本和
示例代码。为了解决分块时 SDK 被截断的问题，我们将 SDK 的描述单独分块并进行向
量化存储。 
其次，如何从需求 PRD 中精准匹配到需要调用的 SDK。由于 Checklist 中包含了
大量的业务描述，这些描述对于 SDK 的匹配很多干扰项。因此，我们让 GPT 对 check-
list 的数据进行拆解和任务识别，识别出类似于接口调用、环境判断和埋点等功能点，
将需求文档转化为与  SDK 更匹配的功能描述，形成  query list。然后，我们用每个 
query 去向量数据库中匹配，找出最相关的 TOP5 结果。 
最后，由于 TOP5 结果对后续代码生成仍有干扰，我们引入了 BGE Reranker large 
model 进行进一步排序，获取最匹配的结果。所有 query 完成这一步后，我们再让 
GPT 帮助我们进行去重和匹配度确认，最终输出一个相关度较高的 query 和 sdk 集合。 
将这份集合与前面的需求 Checklist 数据、渲染代码以及后端接口 API 数据结合起
来，我们就可以进行整个逻辑代码的生成。GPT 生成的结果包括私有 SDK 引用、接口
 
201 
InfoQ 架构师2025年第一季 
调用、交互行为（如 Tab 切换、点击后的页面跳转等），以及渲染部分。 
小结 
在 C 端的应用效果方面，我们可以看到出码率的计算结果有所不同，这取决于各
类业务场景。一些场景的出码率较高，这些通常包括样式规整的页面，比如列表循环等。
这些页面的结构和样式较为标准化，因此更容易被自动化工具识别和生成。 
相对而言，出码率较低的场景可能涉及到复杂的图表、隐藏的业务逻辑，或者动画。
例如，一些页面可能包含复杂的动画效果，如“拍一拍”按钮的动态效果，或者轮播图
等。这些动画效果目前不在我们的代码生成范围内，因此这些页面的出码率相对较低。 
B 端业务场景解决方案 
从系统架构图可以看出，我们选择了利用 AI 技术来进行代码生成。我们构建了一
整套系统，这套系统不仅包括代码生成，还涉及到从项目维度对代码进行管理，包括项
目中包含的页面以及这些页面的所有代码。 
我们的系统覆盖了项目的整个生命周期，从代码生成到项目的整体发布，包括 Beta 
测试、部署、线上部署，以及图片的 CDN 上传和代码的回滚等一整套解决方案。这些
功能都在我们的系统中得到了集成和管理。篇幅有限，这里主要关注代码生成这一部分。 
 
 
 
202 
热门演讲实录 | 落地和进化 
B 端代码生成 
页面提示词的编写 
在 B 端代码生成，比如生成这样一个包含表单和表格的页面，我们提示词是如何
编写的呢。 
首先，我们会编写一些系统预设，这些预设包括设定角色为资深前端开发工程师，
使用的框架可能是 Ant Design V4、Mobx 等。我们还会设定一些限制，比如生成注释的
要求、生成文件要求、代码格式规范，以及 workflows 的输入输出规范。这些预设可以
根据需求做调整。 
用户 prompt 部分，是我们对需求的描述。以这个案例为例，我们首先进行业务抽
象，然后结合接口信息用 Markdown 格式描述整个功能模块。例如，如果页面中有一
个表单，我们就直接描述它是一个表单，并详细说明表单的元素，如角色名称的 key 
和类型是什么。如果有一个按钮，我们描述按钮的点击交互，点击后调用的接口是什么，
接口的地址、参数和返回数据是什么。拿到接口数据后，我们如何渲染表格，表格是否
需要分页，列名是什么，对应的字段又是什么。 
实际的需求应用过程中，我们会遇到下面这些问题。 
1. 提示词编写成本高。如果让一个新同学去编写提示词，他可能不知道应该使用什
么格式或者方式来正确描述需求，导致提示词的编写成本很高。提示词的质量与
最终代码生成的质量密切相关。因此，我们需要解决提示词编写成本的问题。 
2. 无法使用私有代码库。B 端系统需要使用我们自己的业务代码库，我们需要让 
GPT 在生成的代码中使用这些代码库。 
3. 复杂页面生成代码质量差。我们需要找到一种有效方式来描述功能复杂的页面，
以便 GPT 能够准确地生成所需的代码。 
提示词编写效率提升 
为了提升提示词编写的效率，我们提供了两种方案。 
第一种方案是根据接口 API 自动生成提示词，主要针对表单和表格这两种场景。
 
203 
InfoQ 架构师2025年第一季 
我们会提供一份提示词的示例作为上下文，并结合用户输入的接口信息来自动生成提示
词。选择表单和表格是因为请求参数往往来源于表单数据，而返回数据中的 list 结构通
常用于渲染表格，所以我们可以根据接口信息自动生成相应的提示词。开发人员只需将
自动生成的提示词与自己的需求进行比对，如有需要，进行增删改，这样的修改成本相
比从头开始写可以降低至少 60%。 
第二种方案是提供一些常见的提示词模板。这些模板适用于复杂的表单、表格、树
形结构、图表等场景。开发人员可以直接使用这些模板，并根据具体需求进行一些修改。
提示词模板也能给开发编写提示词带来极大提效。 
私有组件代码库应用 
在我们的业务场景中，私有组件库的应用采用了 few-shot prompting 的方案。我们
会向用户提供一个代码库列表，用户选择代码库后，我们会将所选代码库的示例作为提
示词的上下文提供给 GPT，以便 GPT 能够根据这些信息生成代码。采用这种方法主要
是因为目前在我们的 B 端业务场景中，代码库的组件数不多，如果后续代码组件数量
非常多，我们也会考虑使用 RAG 方案。 
复杂页面生成 
在处理复杂页面的代码生成时，我们采取了一种化繁为简的策略。比如这个页面，
包含 6 个功能模块和 4 个后端接口。如果在一个 prompt 中描述这 6 个模块的内容、
模块间的调用关系以及与接口的交互，prompt 会非常长，这样长的 prompt 给到 GPT，
生成的代码质量并不理想。为了解决这个问题，我们的解决方案是将页面拆分成单独的
模块，通过编排的交互方式，描述模块间的引用关系，然后对每个模块单独进行代码生
成。这种方法比整体生成的质量要高很多，因为我们更精细地控制每个模块的生成过程。 
组件编排 
我们开发的编排界面是实现组件编排的核心工具。在这个过程中，需要人工手动拆
分模块，识别并定义模块间的调用关系。我们会先对子模块进行代码生成，然后将子模
块的代码作为上下文，结合父组件的提示词来生成父组件的代码。这种逐个模块的代码
生成方法确保了每个模块的代码都是高质量的。 
 
204 
热门演讲实录 | 落地和进化 
在所有模块的代码生成完成后，我们会进入在线页面编辑器，这里包含了所有模块
的文件，每个文件都已经完成了拆分。左侧是在线编辑器，右侧是实时预览沙箱。这个
例子里面的页面在 GPT 生成代码后，无需任何修改即可运行，可用度非常高。在这个
页面上，我们对比需求 PRD，检查功能是否有遗漏，或者是否有业务逻辑偏差。功能补
充完整后，我们可以提交代码进行 Beta 发布和测试，并在测试完成后直接在系统上进
行线上发布。 
小结 
从 2 月到现在，我们 B 端的出码率整体呈上升趋势，已经有 100 多个页面上线，
代码量达到几万行。出码率的提升得益于我们对提示词的优化、交互的优化，以及大模
型的升级。 
未来规划 
代码生成的持续发展上，主要包含以下三个方面： 
1. 制定 UI 标准范式。开发与 UI 设计的协同工作对于生成高质量代码至关重要。 
2. 逻辑代码生成方面，我们之前讨论的主要是营销活动的逻辑代码。对于主流程，
由于其历史业务逻辑较为复杂，我们仍在开发和探索过程中。 
3. C2C（Code to Code），包含老旧系统重构和多端代码生成。 
系统的智能化提升，主要包含下面两点： 
1. 我们将持续优化系统，提升系统的自动化能力以及交互模式，使其更加智能，降
低对人工操作的依赖。 
2. 多模态智能生成的应用。在 C 端，我们比较期待随着大模型的发展，后续支持
用 UI 设计稿直接生成还原度高且质量高的代码。对于 B 端，目前除了提示词
外，还支持根据原型图生成页面，我们期待未来可以通过语音等多模态输入直接
生成页面，实现更高效、更智能的代码生成，从而进一步提升开发效率。 
 
 
 
205 
InfoQ 架构师2025年第一季 
嘉宾介绍 
• 姚佳梅，在去哪儿网工作了 11 年，目前担任前端开发技术总监，主要负责国际机
票前端开发业务、营销业务和一些公司基建项目。在公司期间，参与了公司营销和
中后台低代码平台、Serverless 平台的建设、前端统一 UI 组件库建设、多端统一代
码方案建设、国际机票 APP 端用户性能提升等项目，目前负责大前端代码生成项目，
结合传统算法和 AI，打造了代码生成一体化平台，着力于公司前端开发的提效工作。 
 
 
206 
热门演讲实录 | 落地和进化 
从中国出发、走向全球，网易有道 AI 创新应
用的商业化实践 
 
在 InfoQ 举办的 AICon 全球人工智能开发与应用大会上网易有道国际 App 产品部
业务负责人赵越做了专题演讲“AI 创新应用 C 端 B 端商业化实践，从中国走向全
球”，演讲围绕 AI 技术在不同市场中的应用和变现策略。 
首先，将探讨网易有道是如何利用 AI 技术开发出具有千万级日活跃用户的翻译应
用，并分享在北美市场成功占据高比例语音转写市场份额的经验。随后，我会介绍在音
乐娱乐和图像娱乐领域的实践，讲述如何在 AI 翻唱领域开发出全球排名第一的应用，
以及如何通过创新和技术优势打造图像娱乐类产品，从而在全球市场中获取竞争优势。 
最后，我将详细阐述 AI 及大模型技术在 B 端市场中的应用，展示如何有效实现
作者 李忠良 
 
207 
InfoQ 架构师2025年第一季 
全面覆盖算法研发和销售成本的目标。 
通过这些实践内容，我希望展示 AI 技术如何在 C 端和 B 端市场中实现高用户活
跃度和高盈利能力。 
内容亮点 
• 了解如何基于大模型和 AI 技术，开发出具有高 DAU 和高利润空间的 C 端 
App。 
• 了解如何将大模型和 AI 技术应用于 B 端场景，覆盖所有成本并实现公司整体
盈利。 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。 
我今天分享的主题是“AI 创新应用商业化实践”。这个主题的出发点其实是从“全
球走向中国”。 
因为我一开始从事的是海外相关业务，之后才逐渐转向国内的产品开发。 
首先做一个简单的自我介绍。我目前所在的部门是国际 APP 产品部。我在 2014 
年硕士毕业，当时的研究方向是机器学习和人工智能。但在经过三年的硕士学习之后，
我意识到自己在学术研究方向上可能并没有太大的天赋，于是毕业后就开始转向 AI 技
术的应用落地方向。 
从 2016 年开始，我陆续做了多款 AI 应用产品。最开始做的是国际版的有道词典，
这是一个相对成功的项目，最初是在印度上线，最高的时候日活用户接近一千万，在全
球几十个国家的教育类应用榜单上都能排名第一。 
从 2021 年开始，我开始开发多个同传类应用。一开始这些产品主要在北美市场上
线，之后才开始拓展到国内市场。其中一个产品叫 Lectmate，是一款专门为中国留学生
开发的“留学听课助手”。 
因为很多留学生在国外上课时可能面临语言障碍，我们希望通过转写和翻译功能，
帮助他们更好地理解课堂内容。这个产品目前在国内同类产品中也可以排到前列。 
 
208 
热门演讲实录 | 落地和进化 
除了教育方向，我们还做过一款 AI 翻唱类娱乐产品，也是在北美市场推出，在该
类应用中也曾排名第一。最近，我们也开始在 To B 方向出海，并开展了一些相关的研
究。 
我之所以选择“商业化实践”作为分享主题，是因为商业化本质上是大多数公司，
甚至可以说是所有公司的最终目标。当然，一开始我也有过困惑。比如在我最初做国际
版有道词典的时候，我们的团队一年辛苦下来，虽然产品取得了用户增长，实际上却并
未给公司带来利润。 
那时候我确实比较迷茫。不过，随着工作经验的积累，我逐渐认识到：所有的工作
最终都是为了商业化，只不过每个项目的商业化路径不太一样。 
一种是直接商业化，比如通过向用户收费、广告投放、会员订阅等方式来获得收入；
另一种是间接商业化，即先提升产品的用户规模（日活），在当前或未来再通过其它方
式实现变现。比如现在很多 Chatbot（聊天机器人）应用目前是免费的，但等它们积累
了足够的用户后，就会开始推出收费功能，或者通过产品影响力带来其他变现机会。这
种模式也是近年来，尤其是在资本市场驱动下，非常流行的路径。 
我认为，一个产品或者一项工作的开展，首要问题是要搞清楚我们走的是哪一条商
业化路径。如果这个问题搞不清楚，就很容易陷入内耗。因为你可能每天都在很努力地
工作，但回头看，却发现公司账面上是亏损的，这就会对自己的价值产生怀疑。 
新产品取得成功的三种情况 
我们团队在过去几年中做了很多产品上的尝试。回顾这些经历，我总结出一个新产
品在市场上取得成功的三种主要路径。 
一、入局足够早 
我们发现，很多现在市场上已经成功、甚至可以稳定盈利的产品，往往是在十几年
前就已经入局，在该领域长期积累、持续打磨。我们也曾尝试将我们在国内做得非常成
功的翻译类产品，扩展到 T1 国家市场（如美国、加拿大、英国等）。但我们很快发现
这是一件极其困难的事。虽然我们在 T2/T3 国家 推出翻译类产品时仍然可以获得一定
 
209 
InfoQ 架构师2025年第一季 
的收入，但一旦进入 T1 国家市场，情况就完全不同了。 
我们在这些市场投入了大量资源，但始终难以撼动头部产品，最终赔了不少钱。回
头复盘，主要原因在于： 
1. 头部竞品入局早，已经建立起完善的流量结构。 
2. T1 国家头部产品的流量来源一般分为三部分： 
○ 买量（付费广告投放）：大家在这一块的效率可能相近； 
○ 关键词自然流量：因为他们入局早，早已在应用商店中占据了大量高价值关
键词，这些词每天带来的搜索流量对他们来说是免费的自然量； 
○ 品牌认知流量：长时间的市场积累让用户对其产品形成了品牌认知，也带来
了稳定的自然流量。 
因此，即使我们在买量环节与其效率相当，但对方有额外的 2/3 自然流量作为
“补贴”，使得他们可以亏着买、持续投放。而我们完全没有这些“红利”，就意味着
再怎么投，也都是亏的，根本无法进行长期竞争。所以我认为，“入局早”本身就是一
项重要的战略优势，很多成功案例的底层逻辑，就是他们在合适的时间点进入市场，并
持续积累。 
二、产品明显更好 
如果我们不是最早入局者，那么第二种可能成功的路径就是：产品足够优秀，甚至
明显优于竞品。 
举一个典型的例子：在北美市场，用户家中普遍拥有庭院，种植各种植物，植物也
常常容易生病，比如叶子发黄、掉落等问题。因此，早在 2017 或 2018 年之前，市场
上已经出现了识别植物病症的产品，比如 PlantSnap。后来，来自杭州的 瑞奇软件推出
了 PictureThis，也进入了北美市场。 
结果是，PictureThis 逐步替代了 PlantSnap，拿下了该领域的大部分市场份额。我
们深入研究后发现，PictureThis 在产品识别准确率、体验流程等维度上，整体水平远高
于原有竞品。这说明，如果产品体验明显优于市场上已有的选择，就有可能打破原有格
局，获得用户青睐。 
 
210 
热门演讲实录 | 落地和进化 
这里需要强调的是：“稍微好一点”远远不够。我们曾经也有过类似的尝试，产品
在某些维度可能比竞品略好，但没有形成“明显优势”，最终仍旧无法撼动头部地位。 
所以，第二个成功路径的前提是：产品在核心能力上必须具备显著提升，而不是小
幅优化。 
三、新的技术要素的出现 
第三种情形是“有新的技术要素”出现，带来了新的机会窗口。这是近两年来我们
特别强烈感受到的一点。比如随着 Stable Diffusion 和 ChatGPT 等前沿技术的出现，带
动了一大批具有极强商业潜力的应用爆发。这类技术的出现本身，就在用户体验层面和
应用形态上带来了巨大革新，进而重塑了整个产品竞争格局。 
在海外市场，我们观察到许多成功的产品，比如： 
• 基于 Stable Diffusion 的 图片生成类应用； 
• 基于 GPT 的 聊天机器人（Chatbot）类应用。 
这些产品每天都能产生非常可观的收入。这说明，一旦有新的底层技术出现，便可
能带来新一轮“窗口期”，谁能在第一时间内结合技术优势推出高质量产品，谁就有可
能抓住这个时代的新红利。 
面向翻译类、语音处理类等 C 端需求的实践 
我想先简单介绍一下我们此前做的产品——U-Dictionary，也就是有道词典的国际
版，主要面向 南亚市场，特别是印度等 T2、T3 国家的用户群体。 
一、产品在印度的成绩与成功要素 
右边这张图是我们当时在印度教育类 App 榜单中排名第一的截图。回顾这款产品
在印度等市场取得成功的原因，我认为主要有两个： 
• 入局早：我们是较早进入这一细分市场的团队，抢占了先发优势； 
• 产品质量相对较好：这里“好”是一个相对的概念。印度本地的多数互联网公司
在技术能力和产品设计能力方面相比国内仍存在一定差距。因此，当我们以国内
 
211 
InfoQ 架构师2025年第一季 
成熟的 AI 和语言技术进入当地市场时，能够明显形成优势。 
 
二、核心产品能力与技术优势 
我们出海时主推了两大核心能力。 
• 查词与翻译能力 
我们依托国内有道词典多年积累的释义自动挖掘算法和小语种翻译引擎，将这些能
力迁移到国际版产品中。虽然需要做一些本地语料收集的工作，但整体翻译效果已经能
够与 Google 翻译相媲美——甚至在某些小语种场景中更优。这是因为 Google 往往是
基于一个统一模型进行泛化训练，而我们针对不同语种做了精细化调优。 
• 亮点功能：OCR 技术 
我们的 OCR 技术支持多种字符集，尤其适配了东南亚等地的特殊语言和字符结构，
可以覆盖一些非常“稀奇古怪”的字体。这一功能也非常适合用于网红营销，因为它具
有很强的视觉冲击力和传播属性。例如，我们支持图像翻译，拍照后自动识别并用翻译
内容“盖住”原文，整体效果尽量贴近原图视觉。我们还推出了实时翻译功能（AI 所
见即所译），在交互效率和用户体验上都取得了不错的反馈。 
 
 
212 
热门演讲实录 | 落地和进化 
三、OCR 技术的多语言和复杂场景适配 
OCR 是我们特别下功夫优化的技术模块，因为它不仅能服务国内用户，也能很好支
持海外多语种环境。我们做了以下几个方向的优化： 
• 多语种混排识别：如同时识别包含阿拉伯文、泰文、印地语等的图像； 
• 多角度文字识别：如日语的横写与竖写混杂排版； 
• 特殊场景训练： 
• 手写体识别； 
• 公式识别； 
• 超模糊图像处理——第三世界国家的手机拍照硬件和光照条件较差，我们专门训
练了模型以适应昏暗、模糊的拍摄环境，从而显著提升用户体验。 
 
四、海外推广策略：买量 + 网红营销 
在海外推广方面，我们主要采用了两种方式： 
• 买量 
主要渠道为 Google 和 Facebook，这是一种标准打法。 
• 网红营销 
我们在这一块也走在了较前的位置，早在 2016 年就开始在 YouTube 上尝试。当
 
213 
InfoQ 架构师2025年第一季 
时做网红合作的厂商还非常稀少。我们发现，如果你的产品是面向大众用户的，那就非
常适合做网红营销。我们总结了网红营销的执行经验： 
• 关键是要有“吸量功能”，即视觉上或功能上“眼前一亮”的特性，类似广告素
材，能调动观众的感官和情绪； 
• 推广节奏上的技巧：我们采用“KOL + 投流”的模式。即先找一批网红在某个时
间点统一发布内容，后续对表现最好的网红进行重点投流，从而实现 ROI 最大
化，降低 CPI。 
五、现实挑战：商业化困境与用户付费能力 
尽管我们在很多国家都做到了不错的成绩，甚至在一些国家成为了品类第一，拥有
品牌关键词、良好的用户认知，但始终面临一个核心问题——商业化能力薄弱。这些国
家和地区的用户，尤其是我们的主力用户群体——学生（K12），实在是没有支付能力。
他们的收入结构决定了他们无法为教育类产品支付较高的费用。我记得看到一句话特别
有共鸣，是查理·芒格说的： 
“钓鱼的第一条规则是在有鱼的地方钓鱼；第二条是永远不要忘记第一条。” 
印度、埃及这样的市场，“鱼实在太少”。我们即使投入了大量资源，做到了品牌
领先和产品领先，最后还是挣不到钱。公司也给我们下了明确的商业化指标：如果不能
变现，就“你们自己看着办”。于是，我们开始探索新的变现方式，希望打破单一盈利
路径的困境。 
在 2021 年，我们启动了一个新的产品项目，叫做 iRecord。我们当时有一个非常
明确的判断：印度用户没钱，美国用户有钱。所以我们决定将目标市场转向北美，开发
面向美国用户的录音转写类产品，主要应用场景包括： 
• 会议记录 
• 课堂笔记 
• 教堂讲道记录 
这个产品的发展路径，实际上也符合我们此前总结的新产品在市场上取得成功的三
种情况： 
 
214 
热门演讲实录 | 落地和进化 
• 入局早：当时北美市场上在这个赛道的竞品还不多，我们算是较早一批进入者； 
• 竞品不强：现有产品技术能力不突出，产品形态也比较老旧； 
• 技术优势明显：我们拥有稳定且先进的语音识别、音频处理等底层技术； 
• 针对用户做了定制优化：在理解场景的基础上，做了本地化适配和用户体验提升。 
这些因素的叠加，帮助我们在相对短的时间内拿下了北美市场大约 40%～50% 的
份额。 
我们在 iRecord 中实现了一系列核心功能与技术特性： 
• 多语种支持：覆盖主流及部分小众语种； 
• 音色识别与声音定位：用于识别不同说话人，以及判断其在空间中的位置； 
• 预处理增强：在音频识别前对原始语音进行预处理，使声音更饱满、清晰，提高
识别准确率； 
• 人声分离：特别针对会议场景优化，可将多位发言人各自的声音分离开来，提升
内容结构化程度； 
• 背景音去除：借鉴音乐处理中的降噪技术，我们也做了一些如同“分离一首歌”
那样的处理方式，有效去除背景杂音，增强人声识别效果。 
 
在这一产品中，我们也成功实现了商业化变现，相比此前在发展中国家的产品有了
明显提升。得益于美国用户更高的支付能力和对效率工具的付费习惯，iRecord 成为了
我们出海战略中，第一个“跑通技术→市场→变现”的闭环产品。 
 
215 
InfoQ 架构师2025年第一季 
在 iRecord 取得阶段性成功之后，我们也开始尝试更多新的产品方向，并继续秉持
“找准市场、有技术优势、快速落地验证”的理念，不断探索海外的创新机会。 
面向音乐娱乐类、图像娱乐类 C 端需求实践 
AI 娱乐是一个在全球范围内增长非常迅速且潜力巨大的赛道。头部公司每月营收可
以达到数千万美元，日收入在 40 万美元左右。据我们掌握的一些内部消息，这类产品
的利润率大约在 30%，也就是说，一家公司每天可以净赚约 10 万美元的利润。这个体
量对任何一家创业公司来说，都是非常可观的。 
我们注意到国内的 AI 孙燕姿翻唱功能火爆之后，立即将这一能力产品化，并迁移
到海外市场，让用户可以自定义训练任意一个他们喜欢的音色，让这个音色去翻唱任意
一首歌。这个产品在海外也取得了不错的反响，可以看作是一个从国内场景出发、通过
“时光机效应”在国外市场重现成功的案例。“时光机理论”依然在发挥作用。类似的
例子还有妙鸭相机。 
它上线之前，海外市场就已经有非常相似的产品——Lensa。Lensa 在高峰期的日营
收能达到 200 多万美元。这个趋势验证了一个现象：一些在国内成熟并成功的产品形
态，往往可以在海外市场再次爆发，仿佛历史总是在一遍一遍地重演。 
在已有产品基础上，我们还持续进行了功能上的扩展。例如，AI 翻唱产品中加入了
伴奏替换功能，一首歌可以切换为爵士、摇滚等不同风格的伴奏；我们也开发了超拟人
的 TTS 技术，不仅用于娱乐类产品中角色语音的生成，也作为功能亮点用于广告素材
制作。这些 TTS 引擎的生成效果已经几乎无法与真人语音区分，因此在用户体验上极
具吸引力。我 
们还尝试了将这些 TTS 技术用于广播剧的制作，虽然技术实现上没有问题，但从
市场反馈来看，这个细分场景的规模有限，变现能力也比较有限，不太可能成为核心的
营收来源。 
综合来看，我们认为海外娱乐类  App 会长期存在并持续涌现机会。即使像 
ChatGPT 这样的大模型占据了市场主导地位，其它中小型的 Chatbot 仍然可以拥有可观
的收入。与国内不同的是，在海外，绝大多数 Chatbot 都是付费产品。在这个市场上，
 
216 
热门演讲实录 | 落地和进化 
最核心的逻辑只有一条：只要你的产品单元经济模型能跑正，就能持续扩张规模。 
而不像国内，很多公司采用烧钱补贴的方式来争夺用户。图像类产品在海外也同样
具备非常强的商业化能力。例如美图类的工具型产品，依然能带来稳定且可观的营收规
模。这也说明，在 AI 娱乐和视觉创意类方向上，海外市场还远未饱和，依旧充满机会。 
 
面向用户的变现实践 
我们如何找到新的方向并起步？ 
我们在寻找新方向和启动新项目时，其实采用的方法和许多团队类似，首先会搭建
一个情报系统，用来追踪并遍历市场上新近出现、且增长势头较快的产品。在使用情报
系统时，有一个非常关键的判断要做：这个产品究竟是真的具备盈利能力，还是在依赖
资本进行补贴式增长。这个判断非常重要，如果分辨不清，就很容易偏离自己的目标和
资源能力，导致方向跑偏。 
在确定一个方向是否值得尝试时，我们强调的是与竞品之间的相对优势。只要我们
具备明确的优势，就有可能做得起来；反之，如果优势不明显，进入这个市场的性价比
就会很低。我们内部有一个明确的原则，叫做“先射子弹，再射炮弹”。 
也就是说，先做一个基础版本的产品，甚至这个产品可能都还达不到 75 分，先推
向市场测试一轮。如果市场反馈好，我们再加大投入；如果反馈一般或者不行，那我们
 
217 
InfoQ 架构师2025年第一季 
就会及时止损，转而寻找下一个机会。 
不过归根结底，到最后比拼的还是核心能力。无论你是做 B 端产品，还是内容型
产品，还是 AI 驱动的产品，最终决定竞争力和可持续性的，是背后的技术、算法、资
源、商业理解等方面的综合能力。前期的技术红利或打法差异可能会带来一时的领先，
但从长期来看，所有表面的差异都会被拉平，剩下的就是硬实力的对抗。 
如何获得竞争优势获得利润 
我们在评估一个新产品能否建立竞争优势时，通常会从五个维度进行判断。 
• 第一，是否具有核心能力。我们会审视自己的核心技术能力是否在这个领域中有
明确的优势，是否能做到比竞争对手更好。 
• 第二是产品功能本身，是否真正能满足用户需求，以及是否具备差异化。 
• 第三是商业化策略。很多产品虽然功能强大，但在定价模型、付费路径设计或者
用户转化策略上存在明显短板，结果也难以实现盈利。所以商业化策略的对比和
优化非常关键。 
• 第四是硬件成本的考虑。尤其是 AI 类产品，对计算资源的依赖极强。如果一个
大公司进入这个市场，他们在服务器租赁、推理成本等方面的资源可能是小公司
的几分之一，这对竞争格局影响很大。我们也需要评估自己在这方面是否具备足
够的资源支撑。 
• 第五是投放能力。投放不仅仅是渠道覆盖的广度，更是对流量转化效率、素材优
化能力、投放节奏管理等综合能力的考验。我们看到有些产品本身质量很高，但
因为投放能力不足，始终起不来。而这恰恰是我们作为大公司的优势，比如网易
本身就有大量直客资源和投放渠道的支持。 
我们会以这五个维度为标准进行全面评估：只要在这些核心要素上能形成足够竞争
力，我们就会坚定投入；如果评估下来不具备优势，我们就果断转向，不做无效投入。 
如何抓住机会 
第一点，我认为最重要的就是多看。移动互联网已经发展了十几年，准确地说已经
是第 11 年了。大机会基本都已被占据，小机会也大多被人发现并入局。不过，小机会
 
218 
热门演讲实录 | 落地和进化 
虽已存在参与者，头部竞品的能力可能并不强，还有可操作的空间。与此同时，技术的
持续演进也会不断带来新的“窗口期”。比如我们刚刚提到的 Lensa，它的母公司其实
是一家小公司，但依靠一波技术浪潮依然在这个市场中赚到了钱。 
第二点是“慢慢跟、慢慢抄”。新技术并不是每天都诞生的，如果没有新技术，就
很难拿到新机会。但这不意味着什么都不做。平时我们要做的就是练内功：在已有的成
熟产品上，打磨自己的投放方法、商业化策略、产品功能，持续总结成功的方法。对标
竞品抄是一个非常直接有效的方式。不是简单照搬，而是理解其路径和打法，优化为自
己的方案。 
第三点是等待。我们在能力打磨好之后，要有耐心等待新的技术或新机会的出现。
一旦技术浪潮来了，准备好的人才有可能第一时间抓住它。 
最后一点，我想讲的是先做中小机会。中小机会通常门槛较低、落地难度也相对较
小，适合作为团队的“基本盘”。一方面，它可以稳定现金流和团队状态；另一方面，
也能为后续搏击更大机会打下基础。大的机会当然值得追，但当下能拿下的先拿下来，
大的拿不下也没关系，等时间够长，总会轮到你。 
这一点我们团队体会特别深。过去公司老板经常 push 我去研究一些特别大的方向。
但我的经验是：如果一开始就盯着所谓的“大机会”，我们团队早就“死好几回”了。
所以我们并不刻意去追逐“宏大叙事”，而是选择了更踏实的打法。 
B 端机会观察 
我们团队主要做的是 AI 的 C 端产品，这些产品成熟后，相关技术也会向 B 端输
出。反正技术已经沉淀好了，多卖一次就是多一份收入。但我们也清楚，在国内做 B 
端公司是很难的。我们系统性地研究了多个 B 端公司的财报，发现几乎都处于亏损状
态，而且这种亏损并没有好转趋势。 
相比之下，英伟达提供了一个更具启发性的 B 端公司模式。它并不直接提供产品
解决方案，而是提供那些大众产品不可或缺的能力——显卡。显卡一旦交付出去，客户
就可以基于它去训练模型、提升估值、甚至“挖矿”、开展商业应用。 
也正因如此，英伟达的产品始终处于供不应求的状态。这种做法给了我们启示：如
 
219 
InfoQ 架构师2025年第一季 
果我们能帮助 B 端客户赚钱，或成为他们产品里的关键能力，就可能建立起更稳定的
商业模式。 
此外，我们还看到了另一个值得关注的机会：国内 AI 能力“质优价廉”，在海外
市场具有竞争力。以语音识别（ASR）为例，我们发现谷歌、微软等海外头部厂商的平
均价格在 1 美金每小时，约合 7 到 8 元人民币，而国内的 ASR 厂商标价通常只需 2 
到 4 元人民币，价格优势非常明显。因此，我们最近也在积极推动国内 AI 技术向海
外市场的输出。 
嘉宾介绍 
• 赵越，网易有道国际 App 产品部负责人，成功研发了多款广受欢迎的 AI 应用，包
括有道词典国际版 U-Dictionary、iRecord 和 LectMate。其中 U-Dictionary 在全球教
育类应用下载量中名列前十，iRecord 和 LectMate 分别在北美和中国大陆的语音转
写领域排名前列。此外，在 AI 音乐和 AI 图像处理方面也取得了多项成功案例。 
 
 
220 
热门演讲实录 | 落地和进化 
大模型赋能电商 B 端，快手电商技术实践深
度揭秘 
 
随着 AI 应用在商业领域的渗透及普及，电商 B 端商家经营及商家运营场景也进
入了百花齐放的阶段，带来了前所未有的变化与机遇。 
在 InfoQ 举办的 AICon 全球人工智能与开发大会上快手电商运营平台 / 研发负责
人袁首超为我们带来了精彩专题演讲“大模型在电商 B 端的应用实践”，分享将从快
手电商 B 端实际业务场景出发，结合真实线上应用案例 (规模化运营、运营提效等)，
讲述大模型在快手电商 B 端应用的技术方案，如何通过技术架构建设，实现大模型应
用的快速交付并兼顾各场景差异化的诉求；并通过体系化技术建设，实现大模型从模型
到应用层面的准确度提升；其次也会从实际生产出发，分享针对大模型应用场景差异化
的稳定性保障手段能力。最后结合业务应用情况，分享未来电商技术大模型应用技术的
分享嘉宾 袁首超  审校 李忠良 
策划 AICon 全球人工智能与开发大会 
 
221 
InfoQ 架构师2025年第一季 
未来发展方向及展望。 
内容亮点 
• 了解快手电商 B 端在大模型应用上的实践经验 
• 了解大模型应用前沿技术及案例 
• 了解前沿技术如何与业务结合的实践及思考 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。 
B 端大模型应用建设背景 
在电商领域，从角色和核心领域相互关系来看，除了日常接触较多的消费者，电商
的绝大部分参与角色都在 B 端，像在平台卖货赚钱的商家、达人，还有类似公司机构
的服务商、MCN 机构、ISV 等。从平台视角，运营角色也很多，除了常见的产运、用户
运营，电商行业还有商家运营，直播电商场景下还有达人运营。这些众多角色相互合作、
联系，结合一些领域共同构成了复杂的电商生态。 
 
从 B 端视角来看，商家从入驻开始，到卖货、发品、交易、履约，再到最后结算、
清退，整个过程中要处理大量流程，涉及众多领域，还会与达人、服务商、MCN 等产
生合作与联系。我们统计发现，B 端给商家的 PC 端工作台页面大概有 200 多个，商
家若想经营得好，至少要了解一两百个产品功能。尤其在直播电商场景下，新引入的内
 
222 
热门演讲实录 | 落地和进化 
容场域是主要交易载体，内容生产给商家的经营成本和经营门槛带来了较大挑战。 
 
从运营端来看，电商大促是主要业务流程，每次大促基本全员参与，且涉及较多外
部协作，如与商业化、主站体验等协作，同时还会用到众多产品功能，包括协作团队的
产品功能。现在电商环境竞争激烈，大促日常化，运营同学或小二工作状态繁琐，除了
大促还要处理其他专项或业务流程。 
 
直播电商 B 端的业务场景挑战如下： 
1. 场景多样性：平台内包含商家、团长、达人共计 10 余种角色，形成上百种业务
 
223 
InfoQ 架构师2025年第一季 
组合，涉及到的业务环境也越来越多。 
2. 业务复杂度高：平台内商家、达人、场景落地复杂的商业模式，每个场景都环环
相扣，单一解决方案难以应对。 
3. 内容创作要求高：与传统货架电商相比，内容生产对商家经营的重要性越来越高，
整体成本投入也越来越大。 
4. 运营复杂度高：除了基本的客情维护、商品、履约之外，还需要掌握比较多的运
营诊断、政策动向等，以便更好地吸引商家。 
B 端大模型的技术实践 
电商大模型基座建设 
在重构电商 B 端的过程中，我们的主要思路集中在三个方面：通用性、效率和可
靠性。 
 
通用性是我们建设大模型能力的核心，我们希望这个能力能够适配电商 B 端的各
个业务场景，并解决相应的问题。从行业角度来看，大约从 2020 年开始，大模型进入
了快速发展阶段，无论是国外的 OpenAI、谷歌、Meta，还是国内的阿里、百度、字节
等公司，都在积极发展。国内生成式大模型的数量已经达到了 1900 多个。 
然而，通用大模型在解决行业专业问题时，由于领域知识的局限性，往往难以很好
 
224 
热门演讲实录 | 落地和进化 
地解决一些相对专业的问题。因此，各行各业开始建设自己的大模型，如医疗、汽车、
金融等，电商 B 端的大模型也是按照这个思路建设的垂直大模型。 
在建设大模型时，我们会遇到一些电商特有的典型问题。以商品为例，我们面临两
个相对典型的问题。第一个是关于商品理解的问题，我们需要对商品有深入的理解，并
生成相应的标签，这些标签将用于各种场域的分发、推荐或商达撮合过程中。然而，传
统小模型存在局限性，只能生成预设的标签，无法挖掘出更细粒度的信息。 
通用大模型虽然能挖掘更多信息，但生成的内容相对随意，可能不适用于推荐或分
发场域。我们更期望的是能够识别出商品的具体属性和适用场景，如法式女帽适合夏天
海边出游，以便在用户搜索时直接推荐。第二个挑战是商品文案的生成。 
在内容场域，除了商详页的文案外，我们还需要生成大量的台本，即主播在直播间
介绍商品时的稿子。我们希望生成的文案自然流畅，能够让主播直接在直播间使用。传
统小模型可能只能进行图像识别和文案提取，生成的内容生硬且无法使用。通用大模型
虽然能生成流畅的文案，但可能会出现严重的事实错误，这在直播电商中可能导致虚假
宣传问题，甚至可能导致封号。 
针对商品理解和电商创作事实性问题，我们采取了相应的应对措施。对于提高商品
理解能力的问题，我们引入了生成和理解联合建模的大模型训练方法。在训练过程中，
我们不仅保留了大模型的开放性生成能力，还结合了多种多维的电商理解任务进行联合
建模。 
这样，大模型在进行商品理解时，能够明确从哪些维度去分析商品，例如风格、同
类目、同品牌等。对于电商创作中的事实性问题，我们主要采用了基于课程学习的多粒
度知识注入范式。通过这种方法，大模型能够学习从简单到复杂的多层级电商领域知识。 
在这个过程中，我们积累了超过 1 亿的基础数据，并在进行推断理解时，结合了
超过 1000 万的知识图谱来确保生成内容的准确性和可靠性。 
电商 B 端大模型基座建设由四个层次构成，最上层的应用层直接面向用户，提供
具体的 AI 应用解决方案，包括电商商品理解、电商创作工具和电商对话助手。 
这些解决方案利用大模型的能力，旨在提升电商业务的效率和效果。在应用层之下
 
225 
InfoQ 架构师2025年第一季 
是能力层，它提供细粒度理解、可控生成和智能体交互等多方向的大模型能力，为应用
层提供支持。 
再往下是方案层，它基于电商 B 端的业务场景沉淀了通用的解决方案，如数据解
决方案、算法解决方案和评估解决方案。最底层是架构层，负责高性能大模型基础建设，
包括训练、推理、评估等环节，以及 ROCE 多轨网络、混合并行、计算优化等技术。这
四个层次相互协作，共同构建了一个全面、高效、可靠的技术支持体系，以满足电商 B 
端的业务需求。 
 
大模型工程体系建设 
开发引擎 
在大模型工程体系建设方面，我们面临的挑战是如何将算法有效地应用于实际业务
中，开发出供用户使用的产品功能。我们的场景非常多样，多达 20 多个，我们从中筛
选出了 P0 阶段的业务场景。 
然而，我们的工程开发团队在大模型开发方面缺乏经验，从熟悉到搭建一个简单的
大模型应用可能需要十几天，这对于我们众多的场景来说是不可接受的。因此，构建一
个高效且低成本的技术体系来支撑业务场景是我们的首要任务。 
 
226 
热门演讲实录 | 落地和进化 
 
我们的技术体系建设思路与行业大致相同，从基础的大模型能力开始，如提示词、
工具调用，最终形成一个垂直领域的 Agent 来解决复杂场景。我们通过垂直领域的 
Agent 组合来解决一些非常复杂的业务场景。 
我们的基础能力建设，即工具链部分，包括开发框架的建设，我们将其定位为大模
型领域的 Spring 框架。我们希望通过它降低普通工程研发的开发门槛，使他们只需编
写几行代码或进行配置化操作，就能实现大模型应用。除了集成基础能力，我们还从三
个维度进行建设：扩展性、易接入和协作模式。 
在扩展性方面，我们保留了很好的扩展性来支撑各种差异化场景的定制，提供了 9 
种扩展点，每种扩展点内置一到两个默认方案，共有 20 多个默认方案，如知识库、绘
画等，都是可配置的，用户可以基于这些实现自己的扩展。 
在易接入方面，我们建设了一个领域能力市场，与公司的服务发现能力打通，只需
简单配置即可注册到领域能力市场，并直接被引擎使用。 
在协作模式方面，由于涉及众多业务场景和研发团队，我们采用开源方式推进，建
立了一个 git 仓库，任何人都可以使用这个仓库，实现自己的扩展，并将迭代增量建设
的能力贡献回代码仓库，实现能力的升级。这种模式不仅在电商 B 端有用，也影响了
电商事业部之外的团队，如本地和商业化团队。 
 
 
227 
InfoQ 架构师2025年第一季 
Agent 平台 
在大模型工程体系建设中，我们不仅开发了框架，还进一步推出了 Agent 平台，
以实现更高效的应用交付。我们的 Agent 平台名为“千机”，其定位是实现无代码配
置化交付大模型应用。 
千机平台主要针对三类场景：一是典型的通用场景，如会话助手，用户无需自行开
发，通过配置即可使用；二是小流量场景，这些场景不需要单独的服务，可以通过配置
化快速部署；三是快速验证和试点场景，用户可以配置化尝试，以评估效果。我们希望
将大模型交付从研发扩展到产运，使产运人员能够自行配置和迭代大模型应用。千机平
台的主要增量建设包括租户隔离、模型切换和实时反馈等运营向能力。 
 
RAG 
在建设工程体系框架的过程中，我们也遇到了一些典型问题，如 RAG。RAG 是所
有大模型应用都无法避免的挑战，我们在智能助手中深度使用 RAG，并通过离线和在线
两套流程进行优化。 
离线流程中，我们优化了 Embedding 算法，并构建了向量索引和倒排索引，同时
定期将知识库和聊天记录输入大模型进行微调。在线流程中，除了传统的 Query 改写，
我们还使用了多路召回能力，进行精准匹配、模糊匹配和向量匹配，最终进行重排，将
知识交给大模型进行文案优化输出。 
 
228 
热门演讲实录 | 落地和进化 
以智能客服场景为例，通过这套体系，我们的准确率提升了大约 17%。如果将这一
提升折算成商家经营成本，我们每天可以为商家降低几十万到一百万左右的经营成本。 
 
多 Agent 合作 
我们面临电商复杂业务场景的挑战，这些场景涉及众多业务领域，如售前、售中、
售后以及政策咨询等。单一的 Agent 难以应对多样化的需求，而增加 Agent 数量又会
导致资源消耗和响应时长增加，影响成本和用户体验。 
因此，我们采用了多 Agent 合作的多轮会话解决方案，沉淀了多个垂直领域的 
Agent 集合，并建立了一个路由层来识别用户意图，将请求分配给相应的 Agent 处理，
以提高回复的准确率和可靠性。 
测评平台 
在大模型工程体系建设完成后，我们面临的下一个重要任务是验证大模型应用的可
用性和可靠性。为此，我们构建了一个评测平台。这个平台对于 B 端业务场景尤为重
要，因为 B 端对内容的准确性和合规性有严格的要求。 
例如，在商品文案生成中，任何事实性错误或违规词汇都可能对商家造成严重影响，
如违反广告法可能导致店铺被关闭或封号。此外，如果大模型给出的建议导致商家亏损，
商家可能会要求赔偿。因此，我们的评估必须符合风控逻辑，确保可靠性。 
 
229 
InfoQ 架构师2025年第一季 
 
上线后，我们面临的另一个挑战是如何赢得用户对大模型应用的信任。我们建立了
一个三层评估体系，包括规则评估、大模型二次评估和人工评估，以确保交付的大模型
应用是可靠的。 
在知识质检方面，我们首先进行简单的规则校验，如错别字和领域知识纠错，然后
由大模型判断知识的可用性、伦理合规性以及对用户的帮助程度，最后由审核团队进行
标注，输出知识评估。为了解决上线后的信任问题，我们正在探索如何将用户对大模型
的应用数据量化，以建立信任。 
我们希望将用户的反馈和客观指标（如 ROI、转化率）结合起来，形成一个可量化
的数字，使用户相信大模型应用的可靠性。例如，滴滴司机的评分系统就是基于用户评
价和司机表现得出的分数，用户看到这样的分数后，会认为它是可靠的。我们也希望在
大模型产品上实现类似的量化信任。 
 
230 
热门演讲实录 | 落地和进化 
 
通过这些建设，我们完成了评测体系的建设。在上线前，我们利用三层评估能力对
交付结果进行评估，并建立链路归因分析，以确定哪些改动影响了大模型的效果。上线
后，我们收集产品功能数据和用户反馈，将其作为基础因子交给产运团队，以便他们进
行整体评估和迭代，并将这些因子量化，转化为白盒化的产品，以提高用户信任和产品
优化。 
整体技术体系 
通过上述建设，我们已经完成了整个大模型技术体系的构建。体系的最底层依托于
快手的基础设施，建立了电商大模型基座。在此基础上，我们通过智灵引擎和千机平台
来支持业务场景的具体落地实施。智林引擎作为一个开发框架，旨在降低工程研发的门
槛，使得开发人员能够通过简单的代码编写或配置化操作来实现大模型应用。 
千机平台则进一步简化了这一过程，它允许用户通过无代码配置化的方式直接交付
大模型应用，从而覆盖了如会话助手、小流量场景和快速验证试点等多样化的业务需求
最后，我们通过鸿儒平台，也就是我们所说的评测平台，对整体效果进行监控和迭代优
化。鸿儒平台的建设是为了确保大模型应用的可靠性和合规性，它通过规则评估、大模
型二次评估和人工评估三层评估体系来保障交付应用的质量。 
 
231 
InfoQ 架构师2025年第一季 
 
我们在三个主要方向上进行了工作：首先是理解领域，包括商品理解和内容理解等；
其次是创作领域，涵盖台本创作和内容生产等场景；最后是绘画领域，即各种助手类应
用，到目前为止已经累计交付了十几个场景。这样的技术体系不仅提高了开发效率，还
确保了应用的可靠性和用户的信任度，为电商 B 端提供了强有力的技术支持。 
B 端大模型的未来展望 
我们分享一个故事，其中一位内容创作者担心 AI 会取代他的工作。对此，有人建
议他不应将 AI 视为敌人，而应将其视为朋友，利用 AI 来帮助自己创作出更好的作品。
这个故事同样适用于我们的业务。我们期望 AI 能够成为我们用户的伙伴，助力升级电
商服务和生态。我们希望 AI 能够成为消费者的贴心朋友，帮助他们更高效地购买到满
意的商品。同时，我们也希望 AI 能够成为商家和达人的助手，帮助他们降低经营成本，
增加销售量。 
 
232 
热门演讲实录 | 落地和进化 
 
此外，我们还希望 AI 能够成为运营人员的助手，帮助他们处理繁琐事务，提高运
营效率。为此，我们将构建 AI 数字员工，接手运营人员不愿意做的重复性工作。通过
不断提升 AI 能力，并在电商的各个环节分层级、分阶段地应用，实现电商服务和体验
的整体升级。这不仅将优化用户体验，也将为商家和运营人员带来实质性的帮助，推动
整个电商行业的创新和发展。 
 
 
233 
InfoQ 架构师2025年第一季 
Databricks × Snowflake 纷纷下注，
PostgreSQL 成 AI 时代数据库标准？ 
 
本文内容整理自 ProtonBase CEO 王绍翾在 AICon 的主题演讲《Data Warebase: 
Instant Ingest-Transform-Explore-Retrieve for AI Applications》。作者的职业经历贯穿
了 AI 1.0、2.0 和 3.0 的时代，从搜索推荐，到视觉 / 语音 / NLP 智能，再到当
前正全力投入的大模型 AI 浪潮，本文将结合其多年来对数据基础设施的实践与
反思，深入探讨生成式 AI 时代对数据系统提出的全新挑战与潜在机遇。 
文章结构： 
• Trending：数据基础设施在 AI 时代的新趋势 
• Introducing Data Warebase：什么是 Data Warebase 
作者 王绍翾 @ProtonBase 
 
234 
热门演讲实录 | 落地和进化 
• Data Warebase for AI Workload：如何支撑 AI 工作负载 
• Use Cases of Data Warebase：典型落地场景 
• The Difference Between Data Warebase and Other Technologies：与现
有技术的差异与优势 
Trending：数据基础设施在 AI 时代的新趋势 
未来的所有应用，将主要对接两个接口：一个 Data API，一个 AI API。 
首先，回顾一下近期在数据领域以及 Data for AI 领域的相关思考。这段时间里，有
三条重大新闻格外引人注目： 
 
第一，Neon：Databricks 以 10 亿美元收购 Neon 的举措在行业内引发了广泛关
注。目前，全球最具影响力的数据公司无疑是 Snowflake 和 Databricks——它们不仅在
数据基础设施领域占据核心地位，也正成为众多企业构建 AI 能力的关键平台。 
第二，Supabase 在 4 月底宣布完成新一轮融资，金额高达 2 亿美元，估值也随
之攀升至 20 亿美元。与此同时，市场上传出有多家科技巨头有意收购 Supabase 的消
息，无疑为数据基础设施领域注入了新的活力与关注度。 
第三，ClickHouse 也完成了最新一轮融资，估值已超 60 亿美元。从其对外宣称
的目标来看，ClickHouse 似乎已经准备好向 Snowflake 发起挑战。 
 
235 
InfoQ 架构师2025年第一季 
接下来，我将分享我对这三家公司近期为何备受资本青睐、频频获得投资、收购关
注的几点观察与思考。 
趋势一：大语言模型的出现正在颠覆传统范式 
在我离开达摩院之前，尽管其在语音识别和自然语言处理（NLP）等领域已采用了
大语言模型（LLM）的技术路线，但当时尚未尝试使用 LLM 对全网数据进行统一训练。
直到 OpenAI 的成功落地，整个行业才真正意识到这种方式的可行性与革命性。随之而
来的是，几乎所有技术公司都开始拥抱大语言模型，将海量数据汇聚在一起，借助大语
言模型的能力为每个人回答日常问题，重构人机交互体验。 
但从趋势来看，未来具备能力训练大模型的企业将是极少数。AI 工程之后的重点，
将逐步从基础模型的训练转向应用层的落地与价值释放。而 AI 应用层的两个关键支点
正是： 
• Inference（推理）：如何以高效、低成本的方式透出模型能力； 
• Database for Application（面向 AI 应用的数据库系统）：如何支撑上下文管
理、向量检索、数据调用与语义理解等数据层能力。 
根据美国市场调研数据，已有约 70% 的企业已在实际生产业务中使用 AI 相关的
能力，说明这场范式转变已迅速从前沿技术走向主流实践。 
趋势二：Agent 数量快速增长，数据底座成核心支撑 
在前文提到的三家公司中，前两家均专注于构建基于 PostgreSQL 数据库的智能
代理（Agent）服务，而第三家则聚焦于通过提供数据仓库的能力为 AI 应用提供数据
分析的能力。这一趋势显示出，大模型 Agent 的生态正快速繁荣，背后对高效、高可
用的数据基础设施的需求也在同步升级。未来，Agent 的数量会越来越多，谁能够提供
真正适配 AI Agent 的数据系统，将成为基础设施竞争的核心关键。 
Neon 
首先我们先来看 Neon 是什么。 
 
236 
热门演讲实录 | 落地和进化 
 
Neon 是一个基于开源 PostgreSQL 构建的云原生数据库，它做了几件非常关键、适
合于 AI 应用开发者的事情： 
第一，它将传统的单机数据库架构转变为存算分离的云架构。 
这一点使得数据库具备了更强的弹性与可扩展性，也为其后续的一些创新能力打下
了基础。 
第二，在产品设计上，Neon 有两个非常突出的亮点： 
• Scale to Zero（按需弹性，空闲即释放） 
Neon 官网强调其核心优势之一在于 Scale to Zero，也就是说，当你不使用它时，它
能够将计算资源完全释放，真正做到“用多少，付多少”，这对于资源敏感型应用尤其
重要。 
• Branching（数据库分支管理） 
另一个亮点是 Branching 概念。就像我们使用 Git 一样，Neon 支持数据库级别的
“分支”操作。为什么需要这个？ 
因为在 AI Agent 开发过程中，越来越多的场景涉及大量试验、多人协作、并行工
作——允许开发者快速创建、管理和切换数据库的独立副本（分支），极大提升了开发、
 
237 
InfoQ 架构师2025年第一季 
测试和数据管理的灵活性。Neon 将数据库转变为一个支持敏捷协作的开发平台，为 AI 
和数据工程打开了全新的范式。 
一个有趣的观察：AI Agent 正在大量创建数据库 
Neon 团队也观察到一个显著趋势：AI Agent 正在以前所未有的速度创建数据库实
例。 
从 2024 年 10 月到 2025 年 5 月，短短 7 个月时间，数据库创建量出现了爆发
式增长。 
从 Neon 发布的柱状图中可以看到，绿色部分代表由 AI 自动创建的数据库，相较
于人工创建的实例占比显著提升，这说明 AI Agent 正在成为数据库使用的新主力，数
据库平台也必须为这种新型工作负载做好准备。 
 
Supabase 
Supabase 同样是构建在 PostgreSQL 之上的数据库平台，它与 Neon 构成了直接的
竞争关系。但与 Neon 相比，Supabase 提供了更为丰富的功能集，包括身份验证、对
象存储、实时订阅、边缘函数等服务，几乎可以看作是“开源版的 Firebase”，定位为
开发者的一站式后端服务平台。 
 
238 
热门演讲实录 | 落地和进化 
 
为什么这些公司在最近备受关注？ 
这背后有一个非常清晰的趋势判断：大模型训练的红利期正在接近尾声。虽然业界
尚未正式宣布“训练时代”的终结，但从资本和技术动向来看，未来再去投资新的基础
模型公司已不再是主流。相反，所有人的注意力都在向“应用层”聚焦——这就是我们
观察到的第一个重要现象： 
Inference（推理）和数据应用正在成为新焦点。 
无论是 Neon、Supabase，还是其他新兴的数据基础设施项目，本质上都在围绕这
个趋势进行布局。 
PostgreSQL：新兴数据库的共识基石 
几乎所有的新型数据库项目都选择基于 PostgreSQL 构建。我们刚才提到的 Neon 
和  Supabase 只是其中的两个代表，实际上，全球近几年新出现的数据库产品，
CockroachDB，YugabyteDB，和  DuckDB 也都无一例外的选择了 PostgreSQL 作为查询 
API。 
PostgreSQL 靠其强大的可扩展性和生态，赢得了全球所有新兴数据库的青睐。 
为什么 PostgreSQL 会成为事实上的行业标准？ 
 
239 
InfoQ 架构师2025年第一季 
原因很简单： 
• PostgreSQL 非常标准和规范，除了 SQL 本身就覆盖了 OLTP 和 OLAP 的需求
外，其最大的优点就是有强大的可扩展性。它允许用户通过扩展（Extensions）
来增强数据库功能（全文检索，向量检索，地理信息检索，时序处理等等），
而无需修改核心代码。 
• PostgreSQL 已形成强大的社区生态和工具支持。 
以向量检索为例： 
PostgreSQL 提供了原生的 pgvector 扩展，可以直接支持向量数据的存储与检索；
而在 MySQL 标准中，缺乏可扩展性接口与生态，MySQL 数据库系统往往需要自行定义
向量数据存储和检索的 API，导致实现千差万别，缺乏标准。这也是为什么越来越多的 
AI 公司，特别是 OpenAI、Anthropic、Notion 等大型 AI 初创项目，都选择 PostgreSQL 
作为其核心数据引擎。 
我曾看到一则非官方的报道：OpenAI 内部的一个 PostgreSQL 只读从库就部署
了近 50 个实例。虽然目前 OpenAI 尚未采用分布式数据库架构，但随着业务规模的持
续扩张，这或将成为其未来必须应对的架构挑战。 
Agent Talk to MCP：PostgreSQL 是默认选项之一 
我即将介绍的一个概念是“Agent Talk to MCP（Model Context Protocol）”。这个概
念最早由 Anthropic 提出，而在其官方文档中，明确列出的第二个支持平台就是 Post-
greSQL。 
这进一步印证了 PostgreSQL 在 AI 应用工作负载中的关键作用——它不仅是一种
数据库，更是 AI 系统与数据交互的中枢平台。 
ClickHouse 的定位演变与多模数据库的崛起 
相比 Neon 和 Supabase，ClickHouse 的定位其实有所不同。它本质上是一款数据
仓库。此前，在它的多轮对外宣传中，一直强调自身是一个 Real-time Data Warehouse
 
240 
热门演讲实录 | 落地和进化 
（实时数仓）。但最近我再次打开 ClickHouse 的官网，发现它也开始称自己为 Data-
base（数据库）了（ClickHouse 确实一直在开发 OLTP 的能力，只是一直还没有正式发
布）。这背后反映出一个趋势：未来 AI 应用层将越来越依赖数据库，尤其是多模态数
据库将成为核心基础设施。 
 
举个例子： 
• 如果你正在开发一个基于 AI 的 Agent，它势必需要与各种数据系统和应用系统
交互。按照传统架构的分工模式：事务性数据放在关系型数据库中； 
• 数据的横向水平分布式扩展用 MongoDB 或 HBase。 
• 搜索功能用 Elasticsearch（ES）实现； 
• 分析需求用 ClickHouse 支撑； 
这意味着，一个企业仅在数据底层就要维护至少 4 个不同的 MCP（Model Context 
Protocol ）服务。这对大模型来说是个巨大的挑战。理论上它可以理解这些差异化的服
务，但实际运作中非常复杂，对其“智力”构成高强度负荷。能对接一个 MCP，谁还
要对接 4 个呢？这也正是为什么越来越多的 AI 初创公司选择 PostgreSQL，而未来
大型企业在面向 AI 场景进行数据库选型时，也一定会倾向选择支持多模态的数据库平
台。 
虽然我们刚才提到训练的时代接近尾声，但训练本身的问题依然存在，尤其是在存
 
241 
InfoQ 架构师2025年第一季 
储层面。我们曾有一句行业共识：“AI 的瓶颈在计算，计算的瓶颈在存储。”这句话主
要是针对模型训练阶段而言的。而我们以后更关注的将是 AI 应用和 Workflow 的执
行效率。 
当前，大模型并不能完全替用户整理好所有数据，配合大模型一起工作的 AI work-
flow 主要集中在四个关键环节： 
• Ingestion（数据摄取） 
• Transform（数据加工） 
• Explore（探索分析） 
• Retrieve（查询检索） 
训练的瓶颈仍然存在，但重点正在转向 AI 应用流程（AI Workflow） 
虽然我们刚才提到训练的时代接近尾声，但训练本身的问题依然存在，尤其是在存
储层面。我们曾有一句行业共识：“AI 的瓶颈在计算，计算的瓶颈在存储。”这句话主
要是针对训练阶段而言的。而我们现在更关注的是 AI 应用和 Workflow 的执行效率。 
当前，大模型并不能完全替你整理好所有数据，尤其在真实生产环境中，它也不会
自动创建数据库。它能做的，主要集中在我们前面提到的四个关键环节： 
• Ingestion（数据摄取） 
• Transform（数据加工） 
• Explore（探索分析） 
• Retrieve（查询检索） 
 
242 
热门演讲实录 | 落地和进化 
 
AI workflow 从数据库、应用日志、埋点系统等来源收集数据；随后通过数据清洗与
转换进行加工；加工后的数据可能进入 Feature Store，然后由数据工程师或算法专家进
行探索与分析，做出参数调整等关键决策。当这些数据准备充分后，结合大模型的能力，
便可实现下一阶段的重要能力。 
Multi-Modal Retrieval：下一代智能检索范式 
什么是 Multi-Modal Retrieval？它的核心含义是：在数据检索过程中，不再局限
于某一种查询方式，而是融合结构化、半结构化、非结构化以及向量检索等多种方式，
实现更智能、更全面的查询体验。这项能力对于 AI 应用尤其重要。因为 Agent 面对的
问题往往不是“查一条信息或者一个向量”，而是需要对多个模态、多维数据进行理解、
关联和调用——这就需要底层数据库具备原生的多模处理能力。 
以“智能城市”为例，如果我们需要在监控系统中搜索某辆车或某个人，最基础的
方式可能仅涉及向量检索——比如通过图片或视频帧进行相似度匹配。但一旦我们引入
更具体的查询条件，比如“某个十字路口”“某个下雨天”“某个时间段”，“和某个
车的图片相似”的场景就会涉及到更多模态的信息： 
• “十字路口”是位置标签； 
• “下雨天”是环境标签； 
• “时间段”是结构化数据； 
 
243 
InfoQ 架构师2025年第一季 
• “车的图片”会被 embedding 成向量数据； 
这类查询就不再是单一模态的检索，而是需要同时融合结构化数据 + 标签信息 + 
向量检索的 Multi-Modal Retrieval（多模态检索）。 
再比如在社交推荐场景中，人与人之间的推荐可能通过 Embedding 大部分特征成
为向量，再靠向量相似度检索来实现。但如果用户添加了“同一个城市”或“同一活动”
的过滤条件，就引入了地理位置或事件标签，从而升级为真正的多模态检索任务。 
多模态检索对架构提出了更高要求 
实现 Multi-Modal Retrieval，意味着系统必须同时处理： 
• 结构化数据； 
• 半结构化数据（如 JSON）； 
• 向量数据。 
在传统架构中，不同类型的数据往往被存储在不同的系统中： 
• 结构化数据用关系数据库或数仓； 
• 半结构化数据的存储和检索用 NoSQL； 
• 向量检索用向量数据库。 
这样的问题是当我们要执行一个 Top 100 推荐任务时，分布在多个系统中的结果很
难直接进行 Join 操作，因为性能很差。于是，我们只能尝试从每个系统中提取大量结
果（如 Top 100 万），再在应用层合并关联处理。这个过程不仅开销极大，而且也从理
论上无法保障拿到最后正确的 Top 100。这正是 Hybrid Database（混合型数据库）登
场的理由： 
将多种模态数据统一存储与检索，消除系统间的分割，让多模态查询变得自然、实
时且可扩展。 
AI Workflow 的五个关键需求 
为了支撑真正的 AI 工作流，从数据获取到结果交付，系统必须满足以下五大核心
 
244 
热门演讲实录 | 落地和进化 
能力： 
1. Fresh Data（数据新鲜性）模型必须基于最新的数据进行推理，数据滞后将严
重影响 AI 产出质量。 
2. Instant Retrieval（即时检索）需要毫秒级的数据访问能力，以满足实时响应和
推荐需求。 
3. High Concurrency（高并发）特别是在面向 C 端或 Agent 场景中，系统需能
支撑成千上万用户同时访问，具备高吞吐能力。 
4. Fast Analytics（快速分析）不仅要能存储数据，还要能快速完成聚合、过滤、
排序等分析任务，为 AI 决策提供支持。 
5. Simplicity（易用性）整个系统要具备良好的开发者体验和管理简洁性，避免多
工具链、多平台切换带来的复杂性。 
这些能力构成了现代 AI 应用工作流的基础保障。只有构建一个满足实时性、融合
性、高并发与易用性的数据平台，才能真正释放大模型和 Agent 的智能潜力。 
 
为什么传统数据库和数据仓库难以满足 AI Workflow 的全部需求？ 
前面提到的那些产品之所以备受欢迎，本质上是它们各自解决了 AI 工作流中的关
键痛点，但仍存在明显局限： 
• 数据库：擅长处理 Fresh Data（数据新鲜性） 和 Instant Retrieval（即时检索），
 
245 
InfoQ 架构师2025年第一季 
适用于实时写入和快速查询场景。但其大多基于单机或简单主从架构，难以支撑
大规模的高并发访问。 
• 数据仓库（如  ClickHouse）：在  分析性能（Fast Analytics）  和  使用简洁性
（Simplicity） 方面表现出色，但它们普遍不适合高频写入或低延迟响应场景。 
换句话说，没有一个系统能够同时兼顾 AI 应用的五大关键诉求。 
 
Introducing Data Warebase：什么是 Data Warebase 
因此，我们提出了 Data Warebase 的概念——将 Data Warehouse 与 Data-
base 融合于一体，构建统一的数据底座，以全面支撑 AI 工作流中从数据采集、加工、
分析到检索的全过程。 
根据我们前文的架构模型，任何一家公司在构建数据系统时，都会面临如下几类核
心需求： 
• 事务型数据库：用于实时写入与查询（如订单、行为日志） 
• 文本搜索引擎：处理非结构化关键词匹配（如全文搜索） 
• 向量搜索引擎：支撑语义检索 
• 分析引擎：进行数据分析（如行情分析、指标监控、报表） 
传统做法是将这些功能拆分成多个独立组件，组成所谓的“多引擎架构”，例如： 
 
246 
热门演讲实录 | 落地和进化 
• 使用 MongoDB 或 HBase 做分布式存储； 
• 用 Elasticsearch 处理全文检索； 
• 用向量数据库做 vector 检索； 
• 用 ClickHouse 或 Snowflake 执行分析任务。 
这种架构虽然功能齐全，但存在三大问题： 
• 系统运维复杂：需管理多个技术栈，版本依赖、部署、运维压力大； 
• 数据割裂严重：数据需在多个系统间反复同步、复制，口径难统一； 
• 性能和响应链路长：查询需跨系统拼接，影响响应时间和稳定性。 
我们将这种架构称为典型的 Legacy Data Architecture（传统数据架构）。它已
经难以适配 AI 时代日益增长的实时性、统一性和智能化需求。 
 
Data Warebase 的目标，正是通过统一架构，将多模数据能力集成于一个平台之
上，以更简洁的方式支持复杂 AI Workflow。它不是将多个引擎简单拼装，而是从底层
架构开始融合事务处理、搜索引擎、向量检索和实时分析，真正做到“一个系统、全场
景覆盖”。 
Data Warebase 本质上是一个多模数据库 
正如之前讨论的，几乎所有的数据问题理应由一个统一的数据系统解决，而这个系
 
247 
InfoQ 架构师2025年第一季 
统必须对 AI 友好。AI Agent 需要一个多模数据库来处理多种数据类型和任务，这一点
我们之前已经讲过。 
当客户问到如何实现这个目标时，最初他们往往难以相信一个系统能集成如此多的
功能，因为挑战确实很大。简单来说，如果数据量只有 100 行，实现之前提到的功能
并不难，大多数单机数据库都能轻松胜任。但当数据量达到 1 亿、10 亿甚至 100 亿
行时，挑战才真正开始。 
 
因此，Data Warebase 的核心竞争力在于支持行列混存且具有分布式横向水平扩
展的能力。这种能力主要依赖三个关键技术支撑：存储、索引和存算分离。 
打造 Data Warebase 的核心三要素：存储、索引和存算分离 
1. 存储架构：灵活多样，兼顾 OLTP/ 搜索 /OLAP 的需求 
无论是传统数据库还是大数据系统，都通过行存储支持点查或高速查询，通过列存
储支持分析和搜索。Data Warebase 系统中任何一张表支持三种存储模式：行存表、列
存表和行列混存表。 
• 行存：适用于键值查询（KV）场景，支持快速单行访问。 
• 列存：适合分析和倒排索引，支持高效压缩和列级扫描。 
• 行列混存：在不确定负载特性时，自动兼顾行存与列存的优势。 
 
248 
热门演讲实录 | 落地和进化 
 
2. 索引体系：全面 / 完整 / 正交 
Data Warebase 实现了多种索引机制，包括： 
• OLTP 的全局二级索引：支持跨节点的数据定位。 
• 倒排索引：满足文本搜索需求。 
• 列存索引：优化分析查询。 
• JSON 索引：支持半结构化数据的高效访问。 
有了这些索引，结合智能查询优化器，系统能够动态选择最优执行路径，实现复杂
查询的低延迟响应。从理论上讲，这些技术在以前各种数据库和大数据系统都分别实现
了，我们只是把这些索引能力放在了一个数据库中并把它落地成为了现实。 
 
249 
InfoQ 架构师2025年第一季 
 
3. 存算分离：数据库的云原生创新 
Data Warebase 采用云原生架构设计，将存储与计算资源解耦： 
• 计算层：灵活弹性，支持按需扩展。 
• 热存储层：保证实时和近实时数据访问的低延迟。 
• 冷存储层：经济高效，满足海量历史数据存储，并且支持直接查询冷存上的数据
（通过一些架构的优化，冷存上的查询延迟可以做到接近热存，但是吞吐会远低
于热存）。 
 
 
250 
热门演讲实录 | 落地和进化 
不同于传统大数据存算分离直接使用云上高可用的对象存储，Data Warebase 在块
存储云盘上自主设计了高性能分布式文件系统，实现了在线数据库级别的存算分离，这
个挑战要比大数据系统的存算分离难一个数量级。 
 
同时，存算分离架构带来的秒级弹性（infinite scale & scale to zero），负载隔离，和
数据克隆（Branching）的能力，是实现 AI Agent 灵活工作流和多场景并发计算的关键。 
 
 
 
 
251 
InfoQ 架构师2025年第一季 
4. 其他关键能力 
 
• 数据分区（Partitioning）：细粒度数据划分，方便管理数据，在某些场景下可
提升查询性能。 
• 实时增量物化视图：突破传统物化视图“全量重计算”的瓶颈，实现 Subsecond 
级别的增量更新，极大简化实时 Transform 流程。 
• 时间旅行（Time Travel）功能：支持基于时间维度的数据版本管理，满足 AI 
训练过程中的特征追踪与历史数据回溯需求。 
总结一下，Data Warebase 的诞生之初就预见到未来的所有应用系统将 build 在
两个 API 之上：一个是 Data API，另一个是 AI API。我们专注于做好 Data API，而
它恰好在 AI 领域也能满足 AI Workflow 的所有需求。我们下面来看看它是如何满足这
些需求的。 
 
252 
热门演讲实录 | 落地和进化 
 
Data Warebase for AI Workload：如何支撑 AI 工作负载 
为了满足 AI workload 需求，Data Warebase 需要完成数据接入（Ingestion）、转
换（Transform）、探索（Explore）和检索（Retrieve）。我们分别来看这几个环节： 
 
1. Ingestion 
数据进来时，首先需要能够快速地导入。Data Warebase 能够支持数据库级别的即
时增删改查操作，保障了数据“写入即可见”，同时它支持通过 Foreign Table 直接从 
Data Lake 中读取数据。此外，作为一个数据库，它还支持 CDC 输出，而许多大数据系
 
253 
InfoQ 架构师2025年第一季 
统并不支持这一点。这种能力确保了整个 Workflow 可以无缝串联起来，同时保证了数
据存储的强一致性。 
 
2. Transform 
在 Transform 环节，我认为最重要的功能有三个： 
• 实时增量物化视图 
• Schema Evolving 
• Generated Columns 和 Built-in Functions。 
首先，实时增量物化视图可以高效地处理数据的实时更新和查询，大大提升了数据
处理的效率。大部分数据库系统只支持全量物化视图和非常有限的增量物化视图能力，
所以用户往往还需要 Flink 这种产品做数据的 Transform。Data Warebase 实现了完整了
增量物化视图的能力，以后数据的  Instant Transform 再也不需要  Flink 了。其次，
Schema Evolving 允许数据模式灵活演变，能够适应不断变化的数据结构。再次，
Generated Columns 功能也非常强大。用户可以直接在原表上添加一个新的计算列，而
无需使用物化视图，这使得 Transform 变得非常容易，成本更低。最后，Built-in Func-
tions 可以轻松解决大量数据加工的 ETL 工作。 
 
254 
热门演讲实录 | 落地和进化 
 
3. Explore 
在数据经过 Transform 之后，用户需要在上面进行各式各样的查询和分析。我刚才
提到，多模数据库非常重要，因为很多查询不仅仅是纯分析型 OLAP 的，也不是纯事务
型的，而是需要混合型的查询能力。此外，对于 AI 工程师来说，Sampling 功能也非常
重要，因为他们需要通过采样来观察数据的趋势。最后，正如刚才提到的，在有些时候
算法工程师需要研究 Feature 的变化对模型的影响，因此他们需要知道一个 Feature 在
不同时间点的精确数值，在普通的大数据系统中，这需要不断地存储所有 Feature 不同
时间的数值，造成大量的存储浪费。Data Warebase 作为一款数据库，支持 Transaction 
和 MVCC，因此有很好的 built-in 的 Time Travel 的能力，可以给算法同学提供低成本
的 Feature 按时序观测的能力。 
 
255 
InfoQ 架构师2025年第一季 
 
4. Retrieve 
在 Retrieve 环节，最关键的是要能做多模检索。如果没有多模检索的能力，很多
应用场景几乎是无法实现的。刚才介绍的几个具体场景，也看到了越来越多的场景需要
这种能力。因此，多模检索能力决定了系统在处理更复杂场景时的表现，尤其是当数据
量增大时。如果数据量很小，比如只有 100 行数据，那么问题不大，但随着数据量的
增加，这种能力就显得尤为重要。 
 
 
 
256 
热门演讲实录 | 落地和进化 
Use Cases of Data Warebase：典型落地场景 
接下来分享几个 Data Warebase 落地案例。简单来说，可分为六大类。但从抽象层
面来讲，其实只有两大类型。 
• 依靠多模能力精简架构（Simplicity）：例如 AI Agent 和 Feature Store，未来
大部分服务将依托 AI Agent 进行智能交互，而 AI Agent 需要一个强大的 Data 
API，Data Warebase 提供了强大的多模查询、极致弹性、以及分支管理的能力，
能够很好地支持 AI Agent 的场景。 
• 实时决策（Instant Decision）：例如超实时高吞吐的金融行情分析和风控，高
弹性高吞吐的运维可观测性场景，车联网车机信号实时监控与故障诊断需求，以
及实时搜索广告推荐系统。 
 
关于 AI Agent，之前已经解释过不再赘述。Instant Decision 下的一个大类是可观测
性。可观测性从广义上来说，万物似乎都具备可观测性，但这个范畴太宽泛了。而狭义
的可观测性，主要是指对日志、标签和行为的分析。以前，这个领域主要是时序数据库
的天下。然而，大家后来发现时序数据库存在一些局限性，比如它只能做数据的 Ap-
pend 插入，不能 Update，也无法进行文本检索和复杂的分析查询。 
于是，大家开始使用 ES 和 ClickHouse。不过，ES 最大的问题是冷热数据分层的挑
战（冷数据需要重新加载，否则无法直接访问），而且它主要只能用于标签过滤和文本
 
257 
InfoQ 架构师2025年第一季 
检索。ClickHouse 在大宽表上做多维分析的性能非常不错，但它的 Upsert 能力和 Join 
操作性能并不理想。更重要的是，在可观测性场景下，弹性能力至关重要。因为在系统
正常运行、没有报警或行情平稳时，可能只有小几个人在观测；而一旦系统出现问题或
者来了一波新的金融行情，会有更多的人涌入查看，系统很容易崩溃。因此，云上的弹
性能力非常重要。Data Warebase 因为使用了最领先的存算分离架构，可以做到业务无
感情况下的秒级弹性扩缩容。 
所以，其实可观测性场景即需要 Simplicity 又需要 Instant Decision 的能力。 
而在金融领域，像 Trading、Fraud Detection，以及车联网领域中的信号收集、检测
和报警，以及 Ads、Search 和 Recommendation 这几类场景中，它们都属于需要 In-
stant Decision 的场景。接下来介绍几个具体案例。 
案例一：AI Agent 
未来的 AI Agent，不需要对接多个 MCP，而是连接一个多模数据库。用一个数据
库，一个 MCP 接口，极大降低 LLM 大模型的智力和推理的门槛。 
 
首先是 AI Agent。未来，所有的服务都将提供 AI Agent 的服务。以我们的产品为
例，会出现至少两个大的 MCP 出口。 
第一个 MCP 是数据库本身。我们用标准的 PG MCP 就可以把数据库服务暴露给
 
258 
热门演讲实录 | 落地和进化 
大模型调用。客户既可以使用 SQL 来查询，也可以通过大模型来访问我们的产品，使
用 Data Warebase 会变得更加简单。 
第二个 MCP 是平台服务。除了数据库本身，Data Warebase 还提供平台服务（扩
缩容，监控，报警），这些平台服务也可以对外暴露 MCP 服务。这样，客户的 OPS 
系统可以通过 AI 来智能了解数据库的运行情况。运维同学可以直接提出具体的问题，
比如“今天一天中哪个时间点的 Workload 最高？”“今天的 Workload 比昨天高了多
少？”“有哪些指标有些异常？”. 
平台服务以前主要是通过 SDK 来实现的，但现在都转向了 MCP。未来应用层的业
务逻辑会越来越薄，业务应用慢慢的都会变成只由前端界面、AI 和数据这 3 层架构来
支持。 
另外，我刚才提到的 Data Warebase 的混合查询能力非常强。用户再也不用担心要
管理多个数据库，一个数据库就能搞定大部分的事情。此外 Data Warebase 还支持 
Scale to Zero，也就是说，当没有连接和 Activity 的时候，计算资源可以直接释放掉。同
时，它也能支持无限的水平扩容。最后，刚才提到的存算分离架构能够很好地支持数据 
Snapshot 的快速复制，可以很好地满足 AI Agent 在 Branching 上的能力需求。 
案例二：金融行业案例 
 
第二个案例是金融行业的一个场景，你可以把它理解为一个交易系统。这个系统会
 
259 
InfoQ 架构师2025年第一季 
接收到大量的行情数据，这些数据需要在客户端以最快的速度展示（Freshness 在亚秒
级），因为每当有一个交易完成后，后面会有大量的 AI 机器人做分析和交易决策。所
以，数据输入必须是 Instant 的，要求“写入即可见”，并且查询量非常大。另外，它
的查询也比一般的点查复杂的多。它不仅仅是简单地查看一行行数据，而是需要通过大
量的标签进行过滤做多维分析，以便能够只观测某些特别关注的标签并据此做出决策。
这也是为什么我之前提到可观测性的范畴非常大，从理论上讲，这也是可观测性的一个
应用场景。 
在这种能力要求下，传统数据库能够满足的是 Subsecond Level 的新鲜度和高吞吐
量，但它无法满足多维分析的需求。而 Search 和 Lakehouse 架构能够在一定程度上满
足分析需求，但它们无法同时满足高吞吐量和低延迟的要求。所以，正如我之前所说，
Data Warebase 的这种真正的混合能力，也就是多模查询的能力，在这里就显得非常重
要。 
 
 
 
 
 
 
260 
热门演讲实录 | 落地和进化 
案例三：车联网案例 
 
第三个案例是车联网。我们接入了一个头部的车联网用户，它的车机信号传输频率
非常高，每辆车每秒都会上传车机信号，100 万辆车就意味着每秒有 100 万条数据涌
入。以往，这些数据进来后，我们只是将其存储起来，以满足监管要求。但如今，随着
电动车越来越受欢迎，情况发生了变化。大家都知道，电动车的系统升级是通过 OTA 
来实现的，而不是像传统汽车那样需要开到车厂，插上线进行升级。这些电动车会不断
地推送软件更新，而这些软件更新可能会对车机产生影响。所以，现在数据进来之后，
我们还需要对某些关键列进行分析。即使在不升级的时候，也需要对核心车辆信号做实
时监控报警，确保车辆和车主的安全。 
以前的分析型数据库可以统计一些聚合值，但不擅长明细查询，因为明细查询的时
候可能需要对非主键字段做过滤，需要真正的全局二级索引，而这种索引一般也只有 
OLTP 数据才具有。所以，这种场景非常适合使用多模数据库。 
 
 
 
 
 
261 
InfoQ 架构师2025年第一季 
案例四：广告和推荐案例 
 
第四个案例是广告和推荐。广告的量比推荐大，因为大部分广告公司收集了众多 
APP 的流量，且每次做决策时的查询逻辑也比较复杂。当我们在使用各种手机应用时，
每次跳转到下一个界面，其实都是一个决策过程。这些决策过程中查询的数据量非常庞
大。推荐系统也是如此，现在几乎所有的推荐系统，尤其是电商平台的推荐系统，都需
要相对实时地进行决策。 
 
例如，当你在电商平台上搜索 1000 元的手机时，系统会在下一秒为你推荐 1000 
元左右的手机，而不是 1 万元的手机，因为系统已经根据你的搜索范围做出了精准的
判断。对于新用户，系统可能一开始对你不了解，但一旦你购买了某一类药品，系统就
能根据这一行为推断出你的大概年龄段和性别，从而进行个性化推荐。后续的推荐决策
 
262 
热门演讲实录 | 落地和进化 
会变得更加积极主动，进一步提升用户体验。这种实时性和个性化的能力，是现代推荐
系统区别于传统推荐系统的重要特征。这种推荐系统同样需要实时写入，且高频分析查
询。 
总结一下，今天主要分享了在 Data for AI 时代我观察到的现象和思考，以及 Data 
Warebase 的概念。最后，介绍了 Data Warebase 如何满足 AI 应用在 Ingestion、Trans-
form、Explore 和 Retrieve 等方面的需求。 
 
Data Warebase 与现有技术的差异与优势 
最后再简单提一下很多小伙伴过来询问 Data Warebase 与现有技术的差异与优势。 
1. Data Warebase 与 HTAP 的区别 
首先从客户的角度来看，不应该常常要关心去区分 TP 和 AP，因为 SQL 本身是能
写出来 TP 和 AP 的 Query 来的。只是在数据量大的时候，一个系统要么是 TP 性能
好一点，要么是 AP 的性能会好一点。所以 HTAP 要求的是一个系统能够在 TP 场景和 
AP 场景下性能都非常好。 
真正的 HTAP，不止是简单 TP+AP 的结合，更多的是存储，索引，和查询优化器一
体的结合。 
 
263 
InfoQ 架构师2025年第一季 
其次，HTAP 的核心在于是否能真正实现 TP 和 AP 的无缝融合。如果只是将 TP 
系统的数据同步到 AP 系统去满足报表查询，这并不算真正的 HTAP。真正的 HTAP 需
要具备以下特点： 
• 真正的 HTAP 数据库应该既能独立作为一个 OLTP 数据库，也能独立的作为一
个 OLAP 数据库，还能变成一个混合的 HTAP 数据库。 
• 低延迟：数据能够即时进入系统，无论在什么模式下，数据写入即可见，并且立
即能够无延迟的服务 AP 查询。 
• 高吞吐：能够支持高吞吐的查询。 
• 复杂查询：支持完整的复杂的 OLAP 分析查询。 
如果没有复杂查询的需求，那么基本可以通过传统的 TP 系统解决。只有像金融行
情分析这样的场景，需要数据实时写入和高吞吐的复杂查询，才是真正的 HTAP。Data 
Warebase 因为具有行列混存的能力以及丰富的索引，天然的支持 HTAP，用户做了
合理的存储和索引的配置后，所有查询 SQL 都能在物理极限上拿到最高的吞吐和最低
的延迟。用户再也不用为不同场景的数据库选型而担心。 
2. Data Warebase 与流批一体的区别 
流批一体的终极解法，不是 Flink，而是数据库的实时增量物化视图。 
流批一体是我们最早在阿里搜索主搜时提出的，当时用 Flink 做实时处理，再用批
计算，后来我们用 Flink 的批处理统一了流和批的计算框架和 SQL。但 Flink 运维难、
成本高，我们认为物化视图是解决流批一体的最佳方案。大部分数据系统只是支持全量
物化视图和非常有限的增量物化视图（例如双表的 join，大部分数据系统只能通过全量
物化视图来做）。Data Warebase 实现了实时增量物化视图，这使得真正的流批一体
最简单的方案成为现实。 
 
264 
热门演讲实录 | 落地和进化 
 
3. Data Warebase 与湖仓一体的区别 
关于湖仓一体，简单来说，就是让仓和湖之间的数据能够打通，流转起来，最终让
仓可以直接访问湖的数据，做一些查询加速。其次要求数据仓库能够对接标准的湖存储，
做外表的查询，计算和写入。 
刚才讲的是数据库的趋势。如果放大到大数据的趋势，只有一件事值得关注：未来
数据湖的标准只有一个，就是 Iceberg。因为全球两大数据巨头 Snowflake 和 Data-
bricks 都在围绕 Iceberg 展开。Snowflake 的存储一开始就是基于 Iceberg 设计和实现
的，Databricks 之前有自研的 Delta Lake，后来收购了 Iceberg 背后的公司 Tabular。所
以我们可以预见，未来这两个世界最大的数据巨头都会围绕着 Iceberg 来布局数据湖生
态。 
结语 
数据库和大数据演进到 Data Warebase，不只是架构革新，更是为 AI 工作流打下
坚实的数据底座。在新一轮的 AI 浪潮中，谁拥有更完整更强大的 Data API，谁就拥有
更高的智能上限。 
 
 
265 
InfoQ 架构师2025年第一季 
作者简介 
• 王绍翾，ProtonBase 创始人兼 CEO。曾在 Facebook 负责在线基础设施开发，并深
度参与了 Memcache，RocksDB 和自研分布式图数据库 TAO 的开发，该数据库支撑
了 Facebook 每秒几十亿次的海量数据查询。2015 年加入阿里巴巴，先后负责两项
核心工作：一是用 Flink 打造了搜索推荐相关的数据处理与 AI 机器学习平台，二是
负责达摩院机器智能工程团队，包括视觉 / 语音 /NLP 等 AI 场景的模型训练，推
理，以及向量检索技术。2021 年开始创业，创立“小质科技”，推出了自研产品 
ProtonBase，一款融合数据库与数据仓库能力于一体的新一代 Data Warebase（Data 
Warehouse + Database）。 
 
266 
热门演讲实录 | 落地和进化 
小红书鸿蒙 OS 下的性能优化探索与实践 
 
性能和体验在 iOS / Android 双端场景下已经是一个较为成熟的话题，但随着鸿蒙 
OS 的发展，端侧开发者需要更多的关注多端场景的差异性。在 InfoQ 举办的 QCon 全
球软件开发大会（上海站）上小红书鸿蒙工程师王劲鹏为我们带来了精彩专题演讲“小
红书鸿蒙 OS 下的性能优化探索与实践”，分享议题将以独特于双端的视角，分享小红
书在鸿蒙 OS 上的性能创新实践。 
内容亮点 
• 详细了解小红书在鸿蒙 OS 上的性能优化实践案例； 
• 对比 Android/iOS 端的类同性能优化场景，以前端视角展示鸿蒙 OS 上特有的
能力上有何不同。 
作分享嘉宾 王劲鹏  审校 Kitty 
策划 QCon 全球软件开发大会 
 
 
267 
InfoQ 架构师2025年第一季 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。 
我分享的主题是小红书在鸿蒙平台上的工程实践，主要聚焦于性能优化和探索。首
先，我先介绍一下自己的背景。我之前一直从事大前端领域的工作，主要专注于跨端和
容器化方案。我也曾手写过一个跨端框架，名为 Doric，它可以对标 React Native、Vue 
Native 和 Flutter 等。Doric 框架在落地时表现良好，还支持了一些自研的 3D 引擎方
案。除此之外，我还有播放器内核研发经验，以及大前端常规体系建设和 CI/CD 流水线
的工程经验。未来，我将持续关注大前端的演进，尤其是鸿蒙这样的多端和跨端平台。 
从 2023 年开始，鸿蒙的优势愈发明显，已经成为可与 iOS、安卓媲美的第三大移
动操作系统。从一些抖音视频中也可以看出，鸿蒙在流畅性方面甚至在某些层面上超过
了 iOS。 
今天的演讲内容分为四个部分。第一部分是介绍整个历程和背景；第二部分是介绍
鸿蒙 OS 的相关能力和小红书在该平台上的优化实践；第三部分是通过鸿蒙 OS 提供的
性能验证工具，展示小红书在鸿蒙平台上的性能优化验证方法、优化后的性能提升以及
具体的收益和结果；最后一部分是总结和展望。 
历程和背景 
小红书迭代历程 
从 2023 年年中开始，鸿蒙的“千帆计划”正式启动，并很快升级为“鸿飞计划”。
小红书作为 7 家头部合作商之一，率先支持了鸿蒙，并于 2023 年 11 月中旬上线了
一个基础版的 beta 版本 APP。这个版本主要包含笔记浏览和视频笔记浏览两大功能，
以及一些简单的个人设置。当时，小红书的动作非常迅速，可以说是头部应用厂商中对
华为支持最为积极的品牌之一。 
在整个鸿飞计划中，我们规划了三个核心里程碑：除了 2023 年 11 月的 beta 版
本外，还包括 2024 年 6 月的 HDC 版本和 2024 年 9 月的商用版本。HDC 版本主要
是针对华为正式宣发鸿蒙 3（HarmonyOS Next）开发者测试的情况。在 HDC 版本中，
我们上线了许多小红书特有的存量功能，包括视频拍摄、图文拍摄以及多设备协同等创
新特性。而到了 2024 年 9 月的商用版本交付时，小红书的核心功能已经基本与主端
 
268 
热门演讲实录 | 落地和进化 
对齐。考虑到鸿蒙的开发周期仅有一年，小红书的鸿蒙 APP 在这一年中要对齐开发了
十年甚至十几年的安卓和 iOS 版本，难度和压力都非常巨大。 
到 2024 年 9 月，除了对齐双端的所有功能外，我们还开发了许多其他功能，包
括华为支持的创新特性，例如智能拖拽——用户可以将图片拖拽到中转站或小艺等场景。
此外，商用版本还支持了用户呼声较高的 HDR 或 Moonlight Photo 拍摄能力。 
纯血鸿蒙与安卓的区别 
我从几个维度来对比一下纯血鸿蒙和安卓 OS 的主要区别。 
1. 内核架构纯血鸿蒙的本质是微内核，而安卓是基于 Linux 宏内核。微内核只提
供基础的内存和文件管理能力，驱动和其他系统能力都在 OS 之外。这样做的
好处是系统稳定性极高，即使应用崩溃，也不会导致整个系统崩溃（system 
crash）。而在 Linux 宏内核中，应用的不当行为可能会直接导致系统崩溃。 
2. 多设备适配鸿蒙目前支持多种设备类型，包括 Mate 60 Pro 这样的直板手机、
Mate X5 或非凡大师 XT 这样的双折叠和三折叠手机、平板电脑、车机，甚至华
为正在研发的鸿蒙 PC。鸿蒙真正实现了类似 iOS 的多端整合能力，通过一套代
码实现多端部署。其工程体系和架构支持单 HAP（Harmony Ability Package）多 
HSP（Harmony Service Package）模块，指令集适配了 ARM64 等多种架构，开发
者只需根据设备尺寸适配 UI 展示即可。例如，在 2024 年 9 月 的华为全场景
设备发布会上，余承东展示了小红书在从直板机到双折叠、三折叠设备上的适配
能力，完全实现了响应式编程，不同设备形态下有不同的浏览体验。 
3. 开发工具和编程模型鸿蒙的开发工具和编程模型与安卓差异较大。鸿蒙更类似于 
Flutter 的嵌套型容器布局，而不是安卓那种面向对象的开发方式。在语言层面，
鸿蒙完全封装了底层逻辑，采用类似前端 Flux 单向数据流模式，通过数据变更
驱动 UI 刷新。这种模式类似于前端 Redux 或 MobX 框架中的 state 管理 。 
从 2024 年 10 月 8 日公测开始，鸿蒙的应用生态正在逐渐繁荣。不过，目前像
微信这样的应用还处于抢先体验阶段。相比之下，安卓的生态已经相对成熟。鸿蒙的最
终目标是打造全场景智能设备生态，涵盖所有终端设备，以及基于 OpenHarmony 内核
开发的物联网终端。它还支持多种芯片体系，例如瑞芯微 RK3568 等。 
 
269 
InfoQ 架构师2025年第一季 
小红书鸿蒙应用架构层级 
小红书经过一年的迭代，其整体应用架构已经基本成熟。目前，整体代码量接近 
200 万行，达到了一个较高的复杂度。在一般成熟的 APP 架构中，通常会包含一些基
础底层能力，例如网络、磁盘存储、埋点体系、APM（应用性能管理）系统，以及一些
通用组件和能力。对于鸿蒙平台，小红书还具备一些特殊的公共通用能力。 
我们开发了一个“一多框架”，这是一个支持一套代码多端部署的具体框架体系。
通过这个框架，我们实现了多设备的断点控制功能。用户可以根据设备的尺寸和类型进
行适配，因为华为设备支持多端投屏。例如，用户可以在手机上浏览小红书，然后将内
容投屏到车机上。比如用户购买了一辆问界汽车，可以在车内通过车机继续浏览手机上
的小红书内容，这种场景在驾驶时尤其有用。 
除了底层框架，对于上层业务，小红书还有一套自研的组件库方案，这套组件库承
载了上层业务的多种功能，包括图文笔记、视频笔记浏览，以及一些 Hybrid 容器能力。
小红书本质上在跨端开发中仍然使用了 React Native（RN）和类 Web 技术。RN 引擎由
华为内部合作提供，采用了自研的 ohos 方案，用于解决 React Native 的 bundle 和 JS 
加载以及渲染问题。此外，还包括产品定制层，这里涵盖了所有相关的设备适配内容。 
 
性能优化与实践 
目前，安卓和 iOS 在性能优化方面已经相当成熟，包括如何分析性能热点问题、
 
270 
热门演讲实录 | 落地和进化 
有哪些工具以及最佳实践等。然而，对于鸿蒙来说，它是一个全新的系统。直到 2024 
年年中，鸿蒙的稳定性和流畅性都还存在一些问题。这里重点讲述小红书在 2024 年与
华为一起进行了哪些实践，以提升应用的性能和用户体验。 
我们定义了一个性能指标场景。这个指标体系是小红书与华为共同探讨的结果，因
为华为有一个性能工厂，它对每个应用的评级都有一个 S 标标准。小红书与华为一起
确定了针对小红书场景需要观测的具体指标。性能优化的核心是慢函数指标，它主要包
含两部分：过程时长和应用体验的流畅性。 
过程时长主要包含以下三点： 
1. 冷启动时长：这是用户最关心的指标之一，即从点击应用图标到应用完成动画并
展示第一帧的时间。对于多数应用，首页通常有缓存机制。例如，小红书会缓存
用户上次刷新的笔记，淘宝会缓存用户上次浏览的商品内容。 
2. 场景完成时长：指完成某个特定场景所需的时间。 
3. 应用响应时长：指用户操作界面后，界面真正发生变化的时间，即响应时延。 
流畅性方面，最基础的观测指标是平均 FPS（帧率），包括丢帧数、最大连续丢帧
数、丢帧卡顿次数以及卡顿率。卡顿率可以通过量化计算得出：当一个场景中出现丢帧
时，丢帧的时长与场景总时长的比值即为卡顿率，它是一个小于 1 的百分比数值。 
OS 能力 & 优化实践 
首先，针对 IO 场景，我们进行了相应的优化。鸿蒙 OS 的系统能力主要分为以下
三个方面： 
1. 并行化能力：鸿蒙 OS 提供了两种并行化能力：Worker 和 TaskPool。Worker 
类似于传统的线程模型，每个 Worker 都有自己的内存空间和执行单元，支持通
过消息（message）进行通信。TaskPool 则类似于协程或线程池，能够动态管理
线程数量，支持标记为 @concurrent 的函数直接在任务池中调度和运行。这两
种机制都支持线程间隔离，内存不共享。 
2. 多线程通信和数据传输：在多线程通信方面，鸿蒙 OS 支持序列化数据传输和
基于消息（message）的通信机制。此外，还引入了事件发射器（Emitter）用于
 
271 
InfoQ 架构师2025年第一季 
系统事件的发布和订阅。这种机制允许线程间通过消息传递来实现复杂的交互逻
辑。 
3. 同步转异步机制：鸿蒙 OS 支持基于 Promise 的异步编程模型，包括 async 和 
await 语法，以及 then 和 catch 方法。这种机制能够有效提升应用的响应性和
用户体验。 
并行化能力 
在并行化能力方面，鸿蒙 OS 提供了两套基础实现方式。开发者可以通过 RTS（运
行时系统）实现并行化，也可以通过底层库（如 C++ 标准库中的）实现。不过，如果
完全依赖底层库，可能会导致开发效率下降。为了满足业务需求，鸿蒙 OS 在年初引入
了 Worker 和 TaskPool 能力。Worker 类似于传统的线程模型，每个 Worker 都有独立
的内存空间和执行单元，支持通过消息进行通信。消息可以包含可序列化的数据，也可
以通过指针直接迁移数据。TaskPool 则类似于线程池，能够动态管理线程数量，支持标
记为 @concurrent 的函数直接在任务池中调度和运行。与安卓平台的线程池不同，鸿
蒙 OS 的 TaskPool 会根据硬件条件和任务负载动态调整线程数量。这种机制避免了安
卓平台中因线程池数量过多而导致的系统资源消耗问题。 
接下来我们对比鸿蒙 OS 的 Worker 并行化能力和安卓端的相关特性。从多个维度
来看，Worker 本质上不推荐手动创建，而是通过系统配置 build-provider.json 绑定 ETS 
文件来实现创建。这一点与安卓端并无明显差异，安卓端可以通过 THREAD 等方式启动
线程。 
在鸿蒙 OS 5.0 以下版本（如 4.2 版本）中，主要运行的仍然是安卓系统。这种情
况下，安卓线程数量存在上限，这对应用开发者来说是一个挑战。如果 SDK 集成过多，
线程数可能超标，进而导致应用被系统强制终止，或出现业务场景异常崩溃等稳定性问
题。 
数据传输方面，鸿蒙 OS 为了优化 Worker 的性能和负载，对 Worker 的数量和单
个  Worker 的传输上限进行了限制。鸿蒙  Worker 的单个传输上限类似于安卓中的 
Binder 机制，也存在类似的传输限制。不过，安卓线程通常没有严格限制，因为线程本
质上是一个内存拷贝过程，除非开发者通过指针等方式自定义线程间数据传输。 
 
272 
热门演讲实录 | 落地和进化 
在传输格式上，鸿蒙 OS 支持通过 Sendable 接口进行数据传输。Sendable 是一种
注解方式定义的数据结构，具有传染性，即如果一个类被标记为 Sendable，其关联属性
也必须是 Sendable 类型。鸿蒙 OS 支持基础数据类型（如 number、string）和集合类
型作为 Sendable 传输的内容。对于跨模块调用，鸿蒙 OS 不允许 Worker 跨 HAP 或
跨 HSP 调用。相比之下，安卓应用通常运行在一个或多个 Dex 文件中，允许跨 Dex 
或跨模块的线程间调用。 
TaskPool 类似于双端的协程概念，是一种轻量级线程，仅存储函数。不过，
TaskPool 与协程有所不同，它独立于任务维度，且任务执行时长有限制（超过 3 分钟
会被系统自动回收）。安卓平台可以通过 ASM 插桩技术对线程的创建和执行进行监控
和优化，但轻量级线程或协程的实现通常依赖于线程池或协程机制。 
TaskPool 中的任务默认支持数据转移（transfer），不支持拷贝。此外，TaskGroup 
不支持 SDK 初始化包的加载。某些同学习惯在异步线程中触发 SDK 的行为，在鸿蒙 
OS 上可能会因 TaskPool 生命周期结束而导致变量被释放。 
关于并行化数据传输的  Sendable 概念，Sendable 通过系统提供的 SharedHeap
（共享堆）实现传输。共享堆与本地堆（local Heap）的区别在于，共享堆支持 Senda-
ble 化数据的传输，而本地堆则需要序列化。共享堆的管理和控制耗费了华为专家大量
时间和精力，其中还涉及复杂的异步锁（async lock）机制。在 RTS 并发实例期间（包
括 Worker、TaskPool 等），数据可以通过 Sendable 传递，但 Worker 需要使用单独的 
API。TaskPool 则完全支持 Sendable 的直接传输。这种异步锁机制允许在 TaskPool 或 
Worker 中锁定其他任务中的某些函数，实现线程间的同步，类似于安卓中的 synchro-
nized 或其他锁机制。 
 
273 
InfoQ 架构师2025年第一季 
 
小红书典型并行化场景 
小红书在一些典型化场景中已经实现了并行化处理。例如，网络请求是一个典型的
耗时操作，因为请求过程中涉及验签和安全能力的处理，这些操作如果在主线程中同步
完成，可能会导致应用掉帧。当用户滑动时，掉帧现象会非常明显，这通常是由于大量
计算引起的。为了解决这一问题，我们采用了  Worker 化的方式，将这些操作移到 
Worker 线程中，从而避免主线程的卡顿。 
在进行埋点时，可能会涉及数据库的 IO 操作，这些操作也不建议在主线程中执行。
通过将这些操作放到 Worker 线程中，可以有效避免对主线程的影响。 
针对双列布局中的图片和资源预加载，我们采用华为自研的 RCP 网络解决方案
（类似于 HTTP），通过 Worker 线程在远端进行下载，并在完成后将结果返回到主线
程。此外，TaskPool 的应用场景也非常广泛，例如文件上传、多媒体操作以及启动任务
的编排等。TaskPool 的优势在于轻量化，避免了线程上下文切换带来的不必要耗时。 
关于冷启动和首刷场景的优化。这部分主要包括两个方面：模块的懒加载和动态组
件的复用池。懒加载是应用开发中常见的优化手段，类似于安卓端的 class order 机制。
当应用不需要某个类时，可以延迟加载该类，直到真正需要使用时才加载。这种方式可
以显著提高冷启动阶段的代码加载效率，从而大幅降低冷启动时长。 
动态组件和组件复用池则是为了解决 UI 组件重复创建的问题。在应用中，可能会
 
274 
热门演讲实录 | 落地和进化 
有多种相同类型的 UI 组件（例如小红书中的笔记组件）。为了避免重复创建带来的开
销，我们希望在运行时尽量复用已有的组件，而不是频繁地创建和销毁。 
类前端视角下的模块懒加载 
我们通过特定的分析工具对懒加载进行了深入分析。如图所示，我们能够识别出启
动过程中加载的各种模块，包括 RNOH（React Native on Harmony）、Web engine（网页
引擎）、Red Player（播放器）等组件。这些模块的加载过程涉及到多个.so 文件，即共
享对象文件。 
 
通过自上而下的分析方法，我们可以清晰地看到每个模块加载的具体耗时。进一步
分析这些.so 文件与 RTS（运行时系统）的关联，以及它们所引入的 Napi 的 TS 文件。
我们进行了懒加载潜在对象的分析，发现许多 RTS 实际上并不需要的类文件已经被加
载。这是因为开发者在编写代码时，可能并未充分考虑到导入一个类或方法对应用启动
延迟的影响。 
为了优化这一过程，我们的目标是减少字节码中需要加载的类文件数量，从而加快
应用的冷启动速度。华为提供的编译器能够将 RTS 编译成 Ark bytecode（方舟字节码），
这是一种高效的字节码格式。通过减少需要加载的类文件数量，我们可以显著提高应用
的启动速度。 
华为还提供了一种懒加载的导入方式，只有在真正需要使用某个类时，它才会被加
 
275 
InfoQ 架构师2025年第一季 
载。这种懒加载机制有助于减少应用启动时的资源消耗。这引发了一个问题：为什么华
为不默认采用全懒加载方式，即只有在使用时才加载类文件呢？我已经将这个问题反馈
给华为，并且系统侧可能会考虑在未来的版本中默认采用懒加载方式，同时仍然允许用
户手动选择非懒加载的方式进行类文件的加载。 
 
动态组件 
在小红书的首页场景中，笔记卡组件在多个场景中被复用。为了避免重复创建 UI 
导致的性能消耗，我们采用了动态组件的概念。动态组件的核心原理是利用占位符来延
迟组件的创建，这与 Android 开发中使用 Stub 模式的概念相似。在这种模式下，可以
使用一个代理对象（stub）来代表尚未初始化的组件，从而延迟组件的创建过程。当真
正需要渲染组件时，再将渲染内容填充进去，从而避免每次调用构建函数（如 build）
时的耗时。 
占位逻辑通过系统的 API 实现，涉及到 NodeContainer 和 NodeController 的绑定
关系。Container 和 Controller 一一映射，由 NodeCore 进行管理。Container 仅管理当
前展现的内存部分，使用完毕后需要将其放回池中进行回收和再利用。以冷启动首刷为
例，在启动阶段可以先获取磁盘上的笔记内容，然后在 BuilderNode 中预先创建多个 
Image 组件。这样，在等待网络或推荐接口响应时，Image 组件已经创建完毕，从而在
首页刷新时可以立即使用这些组件，这对于提高首刷非常有益。 
 
276 
热门演讲实录 | 落地和进化 
 
对于组件复用池，当动态组件不再使用时，需要将其返回到组件池中。对于自定义
组件，通过 NoteContainer 占位方式，由 NodeController 进行管理。在需要创建子组件
时，先在 NodePool 中查找，如果找不到，则创建新组件；如果找到，则尝试复用。流
程图展示了从 Container 装载 NodeItem 开始，通过 NodePool 查找，如果找到则进行
条件判断和复用。 
组件的新建和复用过程中，如果找到对应的 NodeItem，则调用 build 方法并更新
自定义组件的状态，完成复用。如果有对应的 NodeItem，可以直接通过 update 函数
更新内部状态并刷新 UI。但要注意，update 方法可能会因状态变量过于复杂而导致更
新延迟，出现图像残影。因此，需要拆分 state，使其足够小，以确保状态变更到通知 
UI 的时间缩短，消除残影。 
我们的策略是优先在 NodePool（节点池）中查找可用的 NodeItem（节点项）。如
果 NodePool 中存在可用的 NodeItem，我们就直接使用它，并通过 getNode 方法进行 
item 绑定，随后更新其状态以实现复用。如果 NodePool 中没有找到对应的 NodeItem，
那么我们将通过 makeNode 方法调用 build 函数来创建新的节点项。 
完成组件的复用后，我们需要将这些组件返回到缓存池中，以便在未来可以再次使
用。这个过程涉及到 NodeContainer（节点容器）和 NodeController（节点控制器）的
销毁，并将 NodeItem 重新放回 NodePool 中。为了更有效地管理缓存，业务层可以利
用 LRU（最近最少使用）算法，或者鸿蒙系统提供的 LRUCache 和 LiUHashMap 等数据
 
277 
InfoQ 架构师2025年第一季 
结构，来自定义缓存的大小，从而优化组件的复用和缓存策略。 
滑动类场景 
在小红书应用中，滑动类场景非常普遍，包括推荐页的子频道、个人页中的收藏点
赞以及用户自己发布的笔记，还有搜索结果页中的搜索结果和用户商品等，这些都是双
列滑动场景。这些双列滑动场景占据了小红书用户体验的 90% 到 95%，因此，滑动体
验的流畅性对于用户的整体体验至关重要。 
为了提升滑动场景的流畅性，小红书采用了 RCP 框架来优化网络资源的获取。RCP 
是华为提供的一个系统组件能力，主要解决网络资源获取效率问题。通过 RCP，开发者
可以在需要时发起网络请求，并自定义资源的写入地址，如文件或 ArrayBuffer。RCP 负
责高效地将资源写入指定位置，而在不需要时，可以取消 RCP 请求，从而优化资源管
理。 
 
RCP 的核心能力在于能够取消请求，并对弱网场景进行了优化，其建联过程优于 
HTTP 1.1 或  2.0。基于  RCP，小红书还应用了华为俄研所提供的  Prefetch 方案。
Prefetch 方案在瀑布流组件的可见区变更时，通过 worker 线程（如 prefetched worker）
启动资源获取，当不可见时关闭，从而优化快速滑动场景，减少不必要的带宽消耗。 
在快速滑动过程中，有些 item 可能短暂消失，对于双端场景，网络请求可能已经
发出且在途，无法取消，导致带宽浪费。Prefetch 和 RCP 结合的方式可以优化这种快
滑场景，防止真正想要看的内容出现白块。Prefetched worker 线程管理多个 RCP 请求，
 
278 
热门演讲实录 | 落地和进化 
每个请求都有完整的生命周期。当通过 RCP 请求获取到所需资源时，会通知主线程，
主线程根据地址加载资源到 Image 组件或占位符 RQI 组件中。 
性能热点问题定位：静态代码检测 
在小红书的开发过程中，我们遇到了一些性能热点问题，这些问题大多是通过 
Code Linter（代码检查工具）检测出来的。由于开发节奏快，开发者在编写代码时可能
难以关注到性能问题，因此需要 CI（持续集成）检查工具来辅助检查。常见的性能热点
包括： 
1. 在列表场景中频繁使用的 LadyForEach 组件，需要指定 key 以实现列表复用。
如果开发者忘记指定 key，Code Linter 会报错提示。 
2. 在  onClick 或  onVisible 等函数中编写空  callback（回调函数）。当这些空 
callback 积累到一定数量（如几百个或上千个）时，可能会严重拖慢应用性能。
Code Linter 可以扫描出这类问题。 
3. 未使用 TaskPool 处理网络资源。例如，Image Bitmap 直接传递 URL 进行同步
加载，当网络阻塞时会导致 UI 线程卡顿。 
4. 复杂的 ETS 组件在列表场景下未实现重用。未设置重用的 ETS 组件在列表滚动
时需要重新构建，非常耗时。组件嵌套层级过深也会导致性能问题。在安卓端，
布局检查器建议容器嵌套不超过四层。 
5. 使用 JSON.stringify 进行对象序列化。JSON.stringify 有一定耗时，尤其在处理 
100KB 左右的数据时，可能需要 10 毫秒左右。Code Linter 会提示这部分性能
问题，但是否需要转异步线程需要开发者自行判断。 
6. 调用 Image 的 syncLoad（同步加载）。在某些场景下，如转场动画，需要同步
加载 image 以保证连贯性。但如果 image 是非磁盘资源（如网络资源），会导
致卡帧。Code Linter 可以扫描出这类问题。 
7. 关于编译器的优化。ETS 组件应避免嵌套过深。如果嵌套过深，可以将每层函数
通过系统的 builder param 或 builder 函数转换。使用 @builder 注解标识的函
数会在编译期间与 ETS 代码整合，从而提高编译器优化效果。 
Code Linter 支持全量扫描和基于 Git DIFF 的增量扫描，但目前华为的 Code Linter 
还不能与 Git Prehook 关联，导致无法在流水线上自动检查。虽然 CI 检查阶段已有 
 
279 
InfoQ 架构师2025年第一季 
Code Linter，但本地代码提交阶段仍需手动运行脚本，无法实现自动检查。我们正在催
促华为解决这一问题。 
 
UI 重载场景分帧方案 
在处理 UI 重载场景时，我们采用了一种称为分帧方案的方法。分帧这个术语的含
义是，当应用在一帧内无法完成所有绘制工作，或者在多帧内都无法完成时，会导致屏
幕卡顿现象。尽管用户可以看到画面，但却无法进行滑动或操作。在这种情况下，分帧
方案就显得尤为合适。虽然分帧方案可能看起来不是最优雅的解决办法，但它确实能够
有效地解决性能问题，使应用性能达到预期标准。分帧方案虽然看似是一种应急措施，
但它能够帮助应用性能达标。 
分帧方案的流程大致如下：假设我们有数据 a、b、c 需要渲染，未采用分帧方案
前，数据 a、b、c 会同时到达并触发状态变更，进而驱动整个 UI 进行刷新。这会导致
在一帧内需要绘制大量 UI 组件，从而影响应用性能。为了解决这个问题，我们采用分
帧方案，将数据 a、b、c 拆分开，分别在不同的帧中进行渲染。例如，数据 a 在第一
帧中渲染完成后，通过调用宏观指令让其进入下一阶段，然后在下一帧中更新数据 b，
依此类推。 
 
280 
热门演讲实录 | 落地和进化 
 
在小红书的图文笔记场景中，分帧方案得到了应用。当用户在首页的双列场景中点
击一篇笔记进入笔记详情页时，这个过程涉及到许多组件的加载。我们可以将这些组件
拆分成不同的帧，例如帧 a、帧 b 和帧 c。对于用户而言，他们通常希望在第一时间
看到整个大屏的画面，因此我们会优先在帧 a 中展示大图。而在帧 b 和帧 c 中，我
们再处理顶部导航栏或底部交互区等内容。通过这种分帧策略，我们能够确保用户在第
一时间看到最关键的内容，同时避免了因为一次性加载过多组件而导致的性能问题。 
 
鸿蒙 OS 分析工具 
传统的主观工具对于鸿蒙 OS 的性能分析仍然适用。例如，抖音和小红书都通过竞
 
281 
InfoQ 架构师2025年第一季 
品分析来进行主观测评。这种能力主要是通过录屏来展示整个流程的耗时和时长，特别
适合评估冷启动完成时延和转场过程的性能。通过录屏，我们可以逐帧查看用户从点击
开始到结束的帧数和真实时长，以此来衡量整个过程的持续时间。 
鸿蒙性能分析工具：IDE Profiler 
除了主观工具，我们还可以使用 IDE 提供的性能分析工具，如 Profiler，来分析慢
函数。由于 ArkTS 编程语言框架主要通过 RTS 和 NAPI（原生应用接口）进行关联，因
此需要能够查看 ArkTS 和 NAPI 的整个堆栈层级。这与安卓有所不同，因为当 Java 通
过 Java Native API 与原生代码交互时，堆栈并不那么容易查看。 
在小红书的性能分析中，我们展示了一个整体线程分析的例子。在左侧，可以看到
小红书的主线程（如 com 点开头的线程）、Daemon 线程、Worker 线程以及 FFRT 线
程。FFRT 是一种运行函数流的线程，可以执行 TaskPool 上的函数。在下图右侧，我们
可以看到在 RTS 环境下的分析结果，其中顶部显示了 NAPI 调用，底部则是一些 C++ 
函数。整个调用栈和它们的执行时长是通过一种自上而下的视图来展示的。利用这种视
图，我们可以精确地识别出哪些慢函数是造成界面卡顿的原因。 
 
性能场景测试工具：DevEco Testing 
DevEco Testing 是一个性能测试工具，它的功能非常全面，性能测试只是其中的一
 
282 
热门演讲实录 | 落地和进化 
部分。除了性能测试，它还支持多种测试场景，包括 debug testing。在 debug testing 
场景中，用户可以自定义业务场景，监测 CPU 的耗时和负载、GPU 的耗时和负载、设
备发热情况以及功耗等问题。 
 
使用 DevEco Testing 进行性能测试的过程如下：首先定义测试场景，然后捕获主帧
数据。一旦开始捕获，就可以观测到 FPS（帧率）、GPU 负载以及整体功耗等数据。完
成性能数据捕获后，工具会生成一份报告，为用户提供了一个完整的场景分析。不过，
目前场景定义还缺乏脚本化能力，需要人工操作辅助。未来，我们期望能够实现场景定
义的脚本化配置，类似于自动化测试。这样，就可以通过自动化工具，实现更高效的测
试流程。 
总结 & 展望 
在对性能场景进行优化后，我们可以看到显著的收益。在实验室环境下的测试显示，
冷启动时间可以降低 50%，响应时延可以低于 100 毫秒，完成时延则保持与双端持平
或更优。在流畅性方面，在多场景和重载场景下均实现了 0 丢帧的成果。需要注意的
是，这里的测试是在非重载模式下进行的，即没有同时运行多个资源密集型应用，如
《王者荣耀》或《和平精英》等。在这种条件下，我们的核心场景，如冷启动、搜索和
个人页等，都能够与双端完全对齐。 
 
283 
InfoQ 架构师2025年第一季 
 
展望未来，有几个方向。首先，我们希望能够在全场景下实现组件复用，以最大程
度地实现 UI 复用。这样可以在多个业务之间的转场或 UI 创建过程中，将不必要的 UI 
创建和消耗降到最低。其次，我们正在考虑代码延迟加载的 lazy 机制。华为内部可能
将其作为通用的解决方案，但在实施过程中我们发现了许多问题，例如全 lazy 加载可
能会影响第三方 SDK，如支付宝等，因为它们可能进行了额外的二进制优化，导致加载
失败或无法响应。因此，我们期望通过代码延迟加载来实现持续治理，但目前它可能还
不适合全场景的 lazy import。最后，我们关注防劣化问题，即在每个版本发布时，我们
不希望性能指标出现劣化。我们希望能够在开发阶段就定义劣化指标和具体数据，以防
止应用劣化。这部分可能需要借助 DevEco Testing 和主观测评的方式来实现。包括我们
关注的指标，例如冷启动和流畅性等，未来可能会纳入防劣化场景。目前，我们的 CI 
环节或 RC 环节，包括流水线的性能管控和代码 CR 机制，都能够规避这类问题。 
嘉宾介绍 
• 王劲鹏，小红书鸿蒙工程师。目前主要负责小红书鸿蒙版的研发和工程建设，曾从
事过大前端架构设计、研发效能等方向的工作，在终端架构演进、性能优化以及跨
端容器和动态化等方面具备长期实践及深厚经验，持续关注大前端技术体系，鸿蒙
以及多端的演进。 
 
284 
热门演讲实录 | 落地和进化 
复杂场景下的 RAG 架构演进：跨模态知识联
邦与统一语义推理实践 
 
随着大语言模型（LLM）与检索增强生成（RAG）技术的深度整合，知识库检索与
问答系统已在智能客服、医疗辅助诊断、金融智能投研等场景实现规模化落地。然而，
在企业级复杂知识交互场景中，传统 RAG 技术在诸如知识片断的语义关联与跨模态融
合推理等方面遇到瓶颈。当检索到的文字内容散落于不同的段落、文档、以及不同的数
据源时，如何能高效准确地检索并关联所有相关片段汇总准确生成最终答案？ 
在 InfoQ 举办的 QCon 全球软件开发大会（北京站）上，枫清科技合伙人、智能
平台事业部总经理王传阳分享了“复杂场景下的 RAG 架构演进：跨模态知识联邦与统
一语义推理实践”，他深入剖析了基于跨模态知识联邦与统一语义推理的 RAG 架构，
演讲嘉宾 王传阳  编辑 Kitty 
策划 QCon 全球软件开发大会 
 
285 
InfoQ 架构师2025年第一季 
并结合生产实践分享实际应用成效，以及后续技术演进方向做了系统的分享。 
内容亮点 
• 面向复杂场景应用：聚焦企业级复杂知识交互，解决实际业务痛点 
• 跨模态知识联邦：突破传统 RAG 模态限制，实现多模态数据融合 
• 统一语义推理：提升知识关联和问答准确性，更智能 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。 
RAG 技术概览 
在大模型出现之初，我们发现其存在诸多问题，其中最典型的是“幻觉”现象。这
是因为大模型在训练初期并未接触到最新的知识，而在应用大模型时，也未使其了解我
们希望其掌握的最新知识。因此，当被问及较新的问题时，大模型很难获取到刚出现的
知识，从而产生各种幻觉。这引发了我们思考如何让大模型了解这些知识。为此，我们
通过在外部构建知识库，并利用 RAG（Retrieval-Augmented Generation）框架，使大模
型能够获取相关上下文，对其进行润色后，最终返回给用户作为问题的答案。 
 
RAG 主要范式 
在 RAG 的演进过程中，出现了多种形态。从最初的朴素 RAG，到高级 RAG，再到
 
286 
热门演讲实录 | 落地和进化 
模块化 RAG，其核心链路始终围绕三大步骤：索引、检索和内容生成。高级 RAG 在检
索环节前后增加了更多处理，而模块化 RAG 则将这三大步骤进一步细化为五个不同的
阶段，使每个部分都能更专注地进行理论和技术突破。例如，在索引部分，针对分块
（chunk）的优化成为许多论文和框架的研究重点。基于模块化 RAG，可以衍生出适用
于不同场景的多种 RAG 形态。此外，还有与组织编排相关的 graph RAG，利用知识图
谱（knowledge graph）进行知识提取、实体与关系连接等操作，这些内容也都在相关图
中得到了概括。 
 
RAG 主要使用场景 
RAG 在过去一段时间内的典型应用场景包括但不限于以下几种。在判断 RAG 是否
适用时，关键在于该场景下的知识库或大模型是否需要参考一个动态频繁更新的知识库。
如果需要，那么 RAG 几乎是必不可少的。虽然通过大模型微调或使用私有数据进行蒸
馏也可以实现类似功能，但从成本和速度来看，RAG 的见效速度更快。例如在智能客服
场景中，产品库会实时更新，通过 RAG 技术构建一个外部知识库，无论知识库如何变
化，大模型都能随时从中提取最新产品的相关信息，以回答用户的问题。 
复杂场景挑战 一 异构知识 
在 RAG 的场景中，除了前面提到的一些简单场景，还会有各种复杂的场景，尤其
是针对异构知识的处理。异构知识具有多种特点，今天主要聚焦于其中两个特点：结构
 
287 
InfoQ 架构师2025年第一季 
的离散性和模态的多样性。 
结构的离散性 
异构知识的结构离散性是一个复杂问题。虽然“结构离散”这个词可能有些牵强，
但异构知识确实存在这种特点，同步知识也可能面临类似问题。例如，相同主题的知识
可能分散在不同的文档、媒介中，甚至可能出现在图片、文档、视频或关系型数据库中，
它们可能都在描述同一件事情。如果要让大模型统一了解这些知识，就需要考虑如何实
现。 
即使在同一个文档内部，知识离散性也是一个问题。例如，一个 200 页的 PDF 文
档中，可能在第一页提到“我叫张三，今年多少岁，外号小三子”，而最后一页总结时
提到“小三子这一生是辉煌的一生”。在这种情况下，如何让大模型理解“小三子”其
实就是“张三”，并且在文档的不同部分建立联系，是一个现实问题。传统的 RAG 技
术在处理这种问题时面临挑战，因为很难保证切片逻辑在语义上是连贯的，也无法将同
一主题的内容切到一起。此外，切片大小也是一个问题。如果切片过大，会影响最终召
回知识的准确度；切片过小，则会导致内容更加离散。同时，只要进行切片和向量入库，
就很难保证跨文档知识的完整性。最终，用户很难获得关于某个知识主题的完整信息，
大模型也无法以此为上下文回答用户的各种问题，通常只能给出片面的答案。 
模态的多样性 
模态多样性是导致结构离散的底层原因之一。知识的模态包括文本、音视频、图片、
关系型数据库、表格等多种形式。传统 RAG 技术在过去主要处理文本数据，现在虽然
有一些技术开始处理多模态数据，但从整体来看，很难有一个理想的 RAG 框架能够很
好地处理各种多模态数据。这带来了两个主要问题： 
1. 多模态检索：例如，提供一段文字，能够检索出相关的图片、视频、表格等不同
模态的内容。 
2. 跨模态检索：相对简单一些，例如提供一段文字，能够检索出相关的图片，或者
反过来，输入一张图片，能够检索出相关联的文字。 
 
 
288 
热门演讲实录 | 落地和进化 
如何应对异构知识的挑战 
在面对这样的挑战时，我在制作 PPT 时尝试咨询了 DeepSeek 的观点，他们给出
的回答和思考过程在大方向上是比较靠谱的。对于结构离散的问题，他们提到可以动态
地将相关段落进行合并，这是一个很好的指导思想，但具体如何实现合并并没有详细说
明。此外，他们还提到可以使用图结构或知识图谱来连接相关段落，这似乎是一个比较
接近解决方案的方向。对于模态多样性的问题，他们指出需要处理不同模态的数据，这
虽然是一个正确的方向，但感觉并没有深入展开。他们提到图像应该如何处理，文本应
该如何处理，并且强调在处理完成后需要综合多模态信息，结合两者进行综合生成。从
大方向上看，这些思路是没有问题的。不过，在细节上，如果基于这样的内容继续追问，
DeepSeek 可能会提供更多的详细信息。 
 
基于融合知识库与统一语义层的 RAG 架构 
核心组件 —— 融合知识库 
融合知识库它主要是将左边散落的各种多元异构数据，包括指标、结构化数据、文
档、图片等信息，通过知识融合技术整合到一个统一的融合知识库中。知识融合的过程
涉及多种常用技术手段。例如，对于文档数据，会使用文档解析器和切片工具进行处理，
并提取元数据。对于关系型数据库中的数据源，会提取其中的指标和元数据。此外，还
会通过可视化工具帮助用户理解知识库中融合了哪些数据以及它们之间的关联关系。融
 
289 
InfoQ 架构师2025年第一季 
合知识库中包含不同粒度、不同模态的数据，作为一个逻辑存储单元，位于架构的底层。
整个过程实际上是将多模态数据转化为融合知识的过程。 
从技术形态和产品形态来看，融合知识库是一个逻辑概念。底层的物理存储保持不
变，例如文档仍然存储在向量数据库中，关系型数据库的数据源（如 MySQL 或 Post-
greSQL）仍然保留在原处，只是它们的元数据被抽取出来，以逻辑视图的形式存储在融
合知识库中。 
从产品形态上来说，我们可以通过产品中的知识库模块向用户展示融合知识库的概
念。例如，用户可以创建不同类型的单体知识库，如文档知识库、网页知识库、数据源
知识库、指标知识库等，也可以创建一个融合知识库。创建融合知识库后，用户有两种
选择：一是在空的融合知识库中新建文档库、指标库、图片库等，统一组织数据；二是
将已有的文档库、指标库、数据源等导入融合知识库，以逻辑方式组织起来，形成新的
融合知识库。此外，用户还可以在融合知识库中指定关注的领域模型，即关注的实体和
关系。例如，在教育行业的融合知识库中，可以配置关注教师、试题、学生等与教学相
关的实体及其关系，这为后续生成统一语义层做好铺垫。 
 
核心组件 —— 统一知识图谱：图谱生成 
统一知识图谱在使用时首先需要考虑的是如何生成，然后才是如何使用。生成统一
知识图谱的过程可以通过一个图来理解。在图的最左边，我们有各种不同模态的数据，
 
290 
热门演讲实录 | 落地和进化 
如文档、图片、视频和关系型数据库等。对于这些不同模态的数据，我们提取相应的元
素放入知识图谱时，这些元素并不完全相同。 
对于文档数据，我们使用了简化版的 Graph RAG（例如 Graph RAG Light）来提取文
档中的实体和关系。通常不会提取全部内容，因为这会带来较高的成本和性能问题。我
们会指定关注的实体和关系，这与前面构建融合知识库时的业务问题和领域模型密切相
关。如果在构建知识库时明确了要解决的业务问题和关注的领域模型，那么在提取文档
内容时就会更有针对性和高效性。 
 
对于图片和视频，我们可以通过多模态大模型或客户已有的系统（这些系统在大模
型时代之前就做过很多打标签等工作）来提取元信息，并将这些信息放入统一知识图谱
中。对于关系型数据库，我们会提取各种元数据，例如数据表的用途、字段的业务含义
等，并将这些信息也放入统一知识图谱中。在放入知识图谱的过程中，必然会涉及实体
的合并以及关系的发现，最终形成一个完整统一的知识图谱。 
核心组件 —— 统一知识图谱：图谱检索 
有了这样一个知识图谱后，我们如何在框架中使用它呢？从用户问题出发，用户提
出问题或表达任务需求时，我们会先在向量库中进行实体和关系的匹配，这其实是一种
语义搜索。目的是理解用户问题中涉及的实体和关系类型。然后，我们会根据这些实体
和关系，在统一知识图谱中进行 N 度扩展查询（例如二跳或三跳查询），以提取相关
的子图。二跳或三跳查询的具体跳数可以根据实际需求配置，通常二跳查询已经足够，
 
291 
InfoQ 架构师2025年第一季 
因为三跳查询可能会过于庞大。通过这种查询，我们可以关联出问题中相关的其他实体
和关系，形成一个子图。 
 
这个子图必然涉及多模态信息，包括文本、关系型数据库以及音视频文件的元数据。
基于这些元数据，我们从底层物理存储中提取具体的数据，并将其送入大模型进行润色
和输出，从而为用户提供一个相对完整的答案。因此，这里的核心是基于统一知识图谱
生成包含多模态信息的子图。 
与 GraphRAG 的对比 
在文本处理方面，我们会使用 GraphRAG 相关技术。大家可能会问，整个技术是否
主要围绕 GraphRAG 展开，这种想法其实是有一定道理的。我们在文本处理上是站在巨
人肩膀上的，但也做了一些改进。例如，通过指定实体和关系，减轻了 GraphRAG 盲目
抽取的工作。这与其说是与 GraphRAG 的对比，不如说是对它的优化和改进。 
在多模态知识方面，GraphRAG 目前主要支持针对文本（如单篇或多篇文档）抽取
实体关系并构建图谱。从算力角度讲，通过预先指定领域模型、实体或关系，可以让抽
取过程更加聚焦。在冷启动难度上，我们提到的架构中，各种图表、指标等元数据的准
备工作确实需要花费一些时间来整理并放入统一图谱中。在技术复杂度方面，我们对 
GraphRAG 进行了纵向简化，没有使用其完整的链路。同时，在横向工程化方面进行了
扩展，例如加入了元数据提取、多模态图片和视频识别等功能，构建了一个包含多模态
 
292 
热门演讲实录 | 落地和进化 
信息的知识图谱。 
基于融合知识库与统一语义层的应用架构 
重新梳理一下我们提到的架构，两个关键词依然是融合知识库和统一语义层，它们
都在这个架构图中得到了体现。下面是融合知识库，其中包含了各种不同粒度、不同模
态的数据；上面则是统一语义层，包括统一语义库和统一知识图谱，为上层的语义服务
提供支持。通常，我们会通过企业知识中台或其他平台来管理底层的统一语义层和融合
知识库，进行优化和更新等工作。企业知识中台还能承上启下，为上层应用提供应用构
建的支持。因为我们需要将底层的知识和数据转化为应用价值并落地，最终还是要通过
具体的应用来实现。所以，企业知识中台在这里提供构建应用的能力，比如智能体应用、
工作流应用等各种形式的应用。这些应用结合统一语义层和融合知识库构建而成，最终
交付给客户并落地为具体的智能体。 
 
生产场景实践分享 
案例分享一：某医院电子病历查询与智能问答业务 
在生产场景中，我们有两个具体的实践案例，第一个是某医院的电子病历查询与智
能问答业务。近年来，流行的呼吸道传染病频繁爆发，如甲流、乙流等，给医疗系统带
来了巨大压力。患者看病时往往面临“看病 30 秒，排队 3 小时”的困境，而医生也
 
293 
InfoQ 架构师2025年第一季 
希望能有一种更智能的方式，帮助他们快速回顾过去病人的症状、治疗方案和恢复情况，
以便为当前患者提供更精准的治疗建议。 
具体场景是这样的：张三因流鼻涕和发热等症状来看病，医生将这些症状输入智能
小助手，查询推荐的治疗方案。为此，我们采用了基于融合知识库与统一语义层的方案。
融合知识库中包含了关系型数据库中的结构化数据（如患者信息、医嘱、住院记录）和
电子病历中的文本数据。统一语义层则对医疗相关的概念进行了定义，例如“发热”是
指体温在 37℃ 到 40℃ 之间，并描述了相关症状。 
在统一知识图谱中，我们构建了一个以患者为中心的图结构，包括医嘱、住院记录、
手术记录等实体，并从电子病历文本中抽取病症、治疗措施和效果等信息，将其关联到
图中。例如，一个患者节点下会关联具体的病症（如支气管炎）、治疗措施（如药物名
称）和治疗效果（如康复情况）。 
在实际应用中，医生提出问题，如“张三有这些症状，需要一个治疗方案”，系统
会根据时间范围（如过去一个月）进行语义搜索，找到相关实体和关系，并在知识图谱
中提取子图。例如，系统会通过向量库快速检索与“发热”和“咳嗽”相关的患者记录，
并结合电子病历中的详细信息，生成一个综合的治疗建议。最终，医生可以看到历史上
类似患者的治疗方案，并据此为张三提供参考。 
案例分享二：某银行风险监管指标分析助手 
第二个案例的业务背景是银行的风险指标监管。银行已经建立了风险指标库，并希
望通过传统 BI 系统以点击按钮等方式查询指标。目前，许多产品和解决方案已经支持
通过自然语言查询指标，并在此基础上进行预测和分析，例如归因分析和辅助决策。然
而，客户的需求不止于此。他们指出，指标不仅与数据有关，还与许多文档规定相关。
例如，其他部门经常发送与指标和监管相关的文档，提醒指标应符合相关规定，或者通
知指标算法的变更。这些文件可能会干扰指标系统的输出，这是真实场景中常见的问题。 
针对这些问题，客户提出了一个需求：在指标应用场景中，结合文档分析，综合进
行指标查询和分析。具体效果是这样的：当查询“过去一个月的不良贷款率是多少”时，
一般的指标系统可能只会返回 5.01% 的结果，更高级的系统会进一步分析相关数据的
波动情况。但如果文档库中有一份文档明确规定不良贷款率不能高于 5%，并且这份文
 
294 
热门演讲实录 | 落地和进化 
档被纳入知识库，那么当查询结果显示超过 5% 时，系统需要额外提示，显示具体的规
定来源和内容，提醒业务人员注意这一问题。 
这种效果的实现基于我们之前提到的框架。具体来说，我们将融合知识库中的指标
库和文档进行统一的元数据和数据实体抽取。在大模型进行指标分析时，通过提示词引
导模型查看文档库中是否有相关的警告或规定。如果发现相关规定，模型会发出相应的
警告或提示。这样，业务人员可以快速了解指标问题的根源，从而做出更准确的决策。 
未来演进方向展望 
在分享的场景中，虽然许多案例是成功的，但在实践过程中也确实存在一些挑战。
从正面来看，这些挑战可以被视为未来展望的方向；反过来，也可以说目前这些方面还
存在不足。以下从四个方面进行分享。 
 
统一语义层的动态更新 
这是一个非常实际的问题。由于使用了图（graph）和相关算法，图的更新，尤其是
实时动态更新，并不容易。目前，虽然有一些工具或框架可以支持实时图更新和计算，
但这些技术手段在一定程度上只能部分解决统一语义层动态更新的问题。 
 
 
295 
InfoQ 架构师2025年第一季 
图像、视频数据的高效处理 
目前，我们在处理图像和视频数据时，还停留在元数据阶段，没有将图像本身的数
据与文本进行联合嵌入，也没有实现统一的召回。虽然我们一定会评估和考虑相关的技
术手段，包括其性能和效果，但可以肯定的是，这是一个未来的发展方向。 
行业语义模型与行业图谱的赋能 
目前，企业通常需要依靠自身力量构建与自身业务相关的语义模型和图谱。如果同
行业之间能够共享已有的行业语义模型，显然会对整个架构的发展起到很大的推动作用。
然而，目前这方面的建设还相对薄弱。 
融合知识库的标准化和场景化 
在多个实践场景中，我们发现某些模态组合非常顺畅，能够体现很多场景价值，例
如指标库与文档库的组合，或者文档库与关系型数据库的组合。然而，并非所有模态组
合都能找到相关的应用场景，因此还需要进一步摸索。如果能够对这些组合进行更深入
的分析并固化下来，例如建立一个“指标 + 文档”的知识库，那么在效率、性能等方
面都可以进行有针对性的优化。这显然比开放式的自由组合更容易实现。 
这些挑战虽然存在，但也为未来的发展提供了方向。通过不断探索和改进，我们可
以逐步克服这些问题，推动技术的进一步发展。 
嘉宾介绍 
• 王传阳，枫清科技（Fabarta）合伙人、智能平台事业部总经理，曾任 IBM 认知计算
研究院企业数字化转型研发负责人，具备十余年软件开发与软件工程经验，以及丰
富的图数据库与图算法应用实践与客户交付经验。 
 
296 
热门演讲实录 | 落地和进化 
揭秘千卡 GPU 集群如何高效训练多模态大模
型：vivo AI 团队实战经验分享 
 
多模态大模型在智能客服、自动驾驶、AIGC 等领域的应用需求不断增长，但其训
练工程面临计算、存储、数据处理、分布式通信等多重挑战。特别是在千卡级 GPU 训
练集群上，如何优化数据加载、提升训练稳定性、突破计算与存储瓶颈，成为 AI Infra 
需要重点攻克的难题。 
在 InfoQ 举办的 AICon 全球人工智能开发与应用大会上 vivo AI 研究院 AI 架构
师王兆雄做了专题演讲“千卡级分布式集群上的视觉多模态大模型落地实践”，
基于 LLaVA 视觉多模态理解模型和 DiT 文生图模型的训练工程实践，详细解析
大规模 GPU 训练集群下的数据存储优化、分布式计算策略、训练容错机制，并
演讲嘉宾 王兆雄  编辑 李忠良 
策划 AICon 全球人工智能开发与应用大会 
 
297 
InfoQ 架构师2025年第一季 
探讨如何提升大规模多模态模型的训练效率和稳定性。演讲将重点介绍混合并行
训练、数据高效加载、自动容错恢复等技术方案，为业界提供可落地的工程实践
经验。 
内容亮点 
• 深入理解多模态大模型的训练挑战，尤其是理解模型 vs 生成模型的工程区别 
• 掌握大规模 GPU 训练集群的优化策略，包括数据处理、并行计算、通信优化 
• 学习如何提升训练稳定性，减少长时间训练中的失败率 
• 借鉴 LLaVA 和 DiT 训练的实际优化经验，为自身多模态模型训练提供参考 
以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。 
随着人工智能技术的飞速发展，图文多模态大模型的需求正呈现出快速增长的态势。
在 vivo，我们也在积极推动这类模型的产品化落地。例如，我们推出的“小 V 圈搜”
功能，能够帮助家长一键圈出题目，快速获取解析，从而让辅导作业变得更加轻松。 
此外，我们的“蓝心”AI 中文绘画大模型，它更贴近东方审美，更符合国源语境，
为用户带来独特的 AIGC 会话体验。这些功能的背后，主要依赖于我们强大的图文理解
与生成模型。 
然而，在千卡级 GPU 上完成这类大模型的训练任务，其挑战远不止算力，还涵盖
了数据预处理、模型结构设计、分布式调度与通信等多方面的工程问题。今天，我将结
合 vivo 在这些产品背后的真实训练工程经验，从问题出发，分享我们是如何在千卡集
上顺利完成模型高效落地的。 
多模态大模型的训练工程挑战 
多模态大模型的训练工程面临着诸多核心挑战。我们先从模型类型的演进谈起。传
统的单模态模型，如自然语言处理（NLP）模型仅处理文本，计算机视觉（CV）模型仅
处理图像，自动语音识别（ASR）模型仅处理音频，它们互不交集。 
而多模态大模型则能够同时处理文本、图像、音频甚至是视频，将它们统一在一个
模型架构内，通过下一个 token 预测的方式实现跨模态的理解与生成。这标志着从各自
 
298 
热门演讲实录 | 落地和进化 
为政走向融合，是迈向更高效、更通用人工智能系统的重要一步。 
再来看多模态模型结构的两种典型应用场景。第一类是理解类模型，例如 LLaVA，
它类似于一个图像问答助手，处理结构规整的图文输入，训练相对简单，计算开销也较
小。第二类是生成类模型，如 DiT，它需要根据文本生成高质量的图像。这类模型不仅
计算量大，而且图像分辨率各不相同，对显存提出了更高的要求。 
简单来说，理解类模型更侧重于离散 token 的训练，训练稳定性至关重要；生成类
模型则更侧重于连续的 latent，其训练复杂度和显存压力都更高。 
在具体训练过程中，我们总结了四个核心挑战。首先，是算力压力大。由于模型体
积较大，图像分辨率存在差异，kernel 启动频繁导致调度延迟加大，从而降低算力利用
率。 
其次，存储 I/O 与 CPU 预处理造成的加载延迟。数据量较大导致加载速度慢，IO 
不均匀，样本处理流程也较为复杂。第三，数据吞吐受限。多模态样本结构相对复杂，
处理 pipeline 较长，容易造成数据通道拥塞。 
最后，通信并行调度较为困难。模型增大后需要进行拆分，如 TP、PP、长序列的 
CP 等并行策略需要反复调优，网络拓扑与并行分区不匹配时，通信链路负载不均导致
跨区域带宽争用。在这些挑战中，我们也曾实际遇到过不少问题。 
我们面临的整个训练挑战贯穿了整个训练链路。从最初的数据加载阶段开始，样本
结构复杂，解码与预处理耗时较大。 
在模型输入阶段，需要进行图文对齐，编码过程也会消耗不少算力。接下来是并行
训练阶段，TP、PP、CP 等调度可能不合理，导致 GPU 利用率难以提升。分布式通信
则决定了我们能否从百卡到千卡甚至万卡实现线性加速比对。 
最后是稳定性问题，一旦中断能否快速恢复，这也是一个很大的挑战。这些看似细
微的问题，在千卡级集群中会被放大，成为我们必须解决的关键弱链路。 
Al Infra 四大优化方向 
我将围绕前面提到的四大优化方向，分享我们在落地过程中的一些优化实践经验。 
 
299 
InfoQ 架构师2025年第一季 
数据处理优化：让数据流转快起，GPU 不再空等 
多模态训练很多时候不是卡在计算，而是“数据先断流”。为此，我们从两个阶段
进行了优化。在数据准备与存储阶段，我们将图文数据预处理成多个 shard 小块，每个
进程仅加载属于自己的那部分数据，这样 IO 操作就会更加轻量。同时，我们将解码、
resize 等操作提前完成，在训练时，只需进行拼接、映射或内存直读等操作，显著提高
了加载效率。 
在训练阶段的加载优化方面，我们采用异步加载和缓存预取机制，确保每张卡都能
即时获取数据，保持数据队列的充足。 
对于高频样本，我们进行本地缓存，避免重复跨节点读取。这些优化手段让 GPU 
不再干等，从而提升了训练速度。 
 
模型计算优化：发挥每张卡的最大算力价值 
图像经过 ViT 编码，文本经过 Tokenizer 编码后与图像特征融合，再通过 Trans-
former 层输出结果或进行理解、生成等操作。 
在这个过程中，存在不少算力浪费点，例如 Block 结构不统一导致 kernel 启动频
率高，attention 计算利用率低，调度碎片化；跨模态融合模块可能打断计算流水线；输
入数据分布不均导致 batch 无法拉大，浪费显存等问题。针对这些问题，我们进行了优
化。 
 
300 
热门演讲实录 | 落地和进化 
例如，进行算子融合，将多个小操作合并成一个复合操作，减少操作的启动次数；
利用高效的 attention 计算，如 Flash attention 等，提升 FLOPs 利用率；采用了混合并
行加 Interleaved 1f1b 操作，让每张卡在运行当前阶段（Stage）的计算任务的同时，并
行准备下一步的前向（Forward）或反向（Backward）计算，从而打通 pipeline，提升整
体吞吐效率；还可以进行激活重算、混合精度等操作，释放显存，进一步拉大 batch 
size，使每张卡都能充分发挥性能。 
 
分布式通信优化：打通卡问瓶颈，让模型不等通信 
节点内通信通常问题不大，但跨节点的 AllReduce 操作容易遇到带宽瓶颈，导致 
GPU 空等。多跳网络路径在 checkpoint 写入 / 读取时显著增加延迟，拖慢整体训练。
我们归纳出四类常见问题：跨节点带宽受限、通信与计算不重叠、多流调度不均、拓扑
与并行策略不匹配。 
针对这些问题，我们采用了以下优化： 
• 拓扑感知调度：根据物理交换机层级和机架分布，最小化通信路径长度。 
• 通信 - 计算重叠：在 CUDA Stream 上并行调度 AllReduce 与前向／反向计算。 
• NCCL 多通道：启用多网络接口或多 NVLink 通道并行传输，分散热力学热点。 
• CPU 核绑定：将通信线程绑定至特定 NUMA 节点的 CPU 核心，减少远程访问
延迟并提升稳定性。 
 
301 
InfoQ 架构师2025年第一季 
只有通信顺畅，整个训练才能顺利进行。 
 
训练稳定性建设：让训练跑得久，跑得稳 
多模态训练周期长、数据量大、集群规模大，出现中断的概率显著提高，且重启代
价高昂。我们围绕以下三方面进行系统性优化： 
• 降低中断概率 
○ 分片数据广播后进行一致性校验，防止节点间数据不一致导致训练崩溃。 
○ 训练前运行深度健康检查，剔除潜在故障节点。 
○ 任务级重试机制，单卡或单节点失败时自动重跑，提升容错能力。 
• 缩短恢复时间 
○ 启用 checkpoint 缓存机制，将写入／读取先缓存至本地高速介质，避开网络 
I/O 瓶颈。 
○ 从缓存中快速加载最近 checkpoint，确保训练恢复秒级启动。 
• 减少重复训练损耗 
○ 采用增量 checkpoint，在关键训练步骤或时间间隔内保存状态。 
○ 结合并行写入与本地缓存，平衡 I/O 开销与恢复速度。 
以上优化结合了 NVMe 加速写入、异步缓存读取和分布式任务重试等业界最佳实
践，实现了多模态大规模训练的长跑与稳跑目标。 
 
302 
热门演讲实录 | 落地和进化 
 
训练工程案例：LLavA & DiT 
接下来，我将通过一些具体的案例，深入探讨图文理解模型 LLaVA 和文本生成图
像模型 DiT 的优化实践。这两种模型类型不同，痛点各异，优化路径也各有侧重。通过
具体案例的拆解，大家能够更直观地看到它们在工程实践上面临的挑战。 
LLavA 训练工程实践 
我们对 LLaVA 和 DiT 的模型结构进行了整体对比。LLaVA 的模型结构是从 ViT 抽
取图像特征，经过投影对齐到文本空间，最后通过大语言模型进行问答输出。而 DiT 的
生成模型则是图像由 VAE encoder 得到 latent，文本由 CLIP/text Transformer encoder 得
到 embedding，再逐步还原出最终的图像。 
尽管一个是理解模型，另一个是生成模型，但在训练落地时，它们都面临着四大共
性挑战：多模态数据对齐难，I/O 压力大；模型结构复杂，融合难度高；通信开销大，
容易产生阻塞；训练容易中断，容错要求高。因此，我们从数据处理、模型计算、通信
效率提升以及稳定性优化等方面入手，为它们提供保障。 
在大规模模型训练中，数据加载往往是第一个瓶颈。如果加载速度慢，GPU 可能会
空闲，导致算力浪费；而加载进程过多，又容易占用过多内存，引发系统内存溢出等问
题。 
 
303 
InfoQ 架构师2025年第一季 
我们通过监控 GPU 端数据接收的耗时来发现问题，一般来说，耗时在毫秒级是正
常的，如果耗时上升，就可能意味着读取数据出现了问题。数据供应不足会导致整个训
练链路变慢。 
为此，我们从四个方面对链路系统进行了优化：一是优化任务下发，使其更轻量，
主进程统一生成列表，并发地给多个子进程加载，减少调度负担；二是在子进程中进行
解码或 resize 操作，避免主进程阻塞；三是启用锁页内存（Pin Memory），让数据流转
更高效；四是在拉取数据时，利用本地缓存，避免频繁跨节点读取高频数据，直接从本
地缓存拉取。 
此外，子进程的数量和预取因子的设置也是我们在实践中踩过坑的地方。这些参数
并不是设置得越大越好，而是要结合 GPU 的吞吐能力和 IO 性能动态优化配置。 
经过优化，数据加载耗时压缩到了原来的 10% 左右，GPU 几乎满载，训练速度提
升了 50%。这种多进程、锁页内存和本地缓存的组合优化，成功打通了数据链路。 
 
在模态融合下的算力利用率优化方面，早期我们采用均匀流水线，但发现存在问题。 
例如，Transformer 层有 32 层，如果 PP 设置为 8，每个阶段会分配 4 层。在语
言模型中可能没有太大问题，但在多模态模型中，前面还有 ViT、project 等操作，这些
也会放到第一个阶段，而后面的 output 则放到最后一个阶段。这就导致前后阶段的算
力和显存占用特别满，而中间阶段可能比较空，使得整个 GPU 卡的负载不均。 
 
304 
热门演讲实录 | 落地和进化 
为此，我们引入非均匀流水线重构，根据每层的参数量和显存占用重新划分，使资
源分配更均匀。在读取数据时，我们发现输入的图文对参差不齐，如果在一个 batch 中
有长有短，最终都会 padding 到最长的图文对上，导致无效计算。 
为了解决这个问题，我们进行了离线数据拼接，将相近的图文对分类，使每个 
batch 的图文对长度相对均匀，从而提高算力利用率。我们还配套了流水线并行调度，
降低流水线并行的空泡。 
因为开启流水线后，仍会存在一些空泡，导致 forward 或 backward 操作中出现延
迟。我们通过调整 Micro Batch Size（微批次大小）和 PP 数，尽量让 Micro Batch Size 
大于 PP 数，以减少空泡。 
此外，我们还进行了算子融合，将多步操作压缩成一步，进一步提高效率。通过这
一套优化，我们将 MFU 从 34% 提升到了 45%，在千卡级上的表现还算不错。 
 
在通信层面的优化中，分布式通信是决定我们能否在千卡级甚至更大规模上实现线
性扩展的关键环节。 
我们在物理级别上进行了一些优化。首先，我们的网络结构可以支持万卡，采用三
层胖树结构，并进行了导轨级优化。这样做的核心是让跨节点通信更高效，不同 Pod 
的同号 GPU 都连接到接入交换机，AllReduce 操作只需一跳即可完成，无需经过汇聚或
核心交换机。 
 
305 
InfoQ 架构师2025年第一季 
实测结果显示，这种优化使 Pool 内的吞吐量提升了 150%。此外，我们还利用了 
RDMA 通信加速，由于我们使用的是 IB（InfiniBand）网络，通信传输无需经过 CPU，
直接从网口到 GPU，这也有很大的提升。在并行策略的调度优化方面，我们尽量避免 
TP 跨节点，因为 TP 的通信量很大，所以 TP 的切分尽量在一台节点内完成。 
同时，PP 也尽量避免跨节点通信，以避免通信拥塞。在软件层面，我们进行了通
信计算重叠优化，进一步提升了效率。通过这些优化，通信空等时长下降了 40%，训练
速度提升了 20%。 
 
在 LLaVA 训练稳定性方面，我们也进行了一些实践。在训练过程中，如果数据量
较大，尤其是预训练数据清洗不够干净，难免会遇到一些异常数据。 
此外，由于数据存储可能在远端，拉取数据时也可能出现拉不到的情况。为此，我
们设计了异常数据跳过机制。当检测到异常数据或处理数据出现异常时，子进程会将异
常上报给主进程进行判断。 
如果异常阈值没有超过设定值，我们就跳过该数据，继续获取下一条数据。这样可
以避免重复训练，使训练过程更加稳定。由于我们支持异常数据跳过，可能会出现一个
问题，即在进行流水线拆分时，某张卡跳过了数据，而其他卡没有跳过，导致切片之间
的数据不一致。 
针对这个问题，我们在 TP 广播的基础上增加了 PP 广播。在启动时，主节点获取
 
306 
热门演讲实录 | 落地和进化 
数据并广播到其他卡，从而避免了训练过程中的崩溃。此外，我们还进行了一些任务级
的异常检测，以便在出现异常后及时发现集群是否 down 掉，并能自动拉起集群，从最
近的 checkpoint 恢复训练，提升训练稳定性。 
通过实际落地这一套优化，我们的千卡训练能够稳定运行 139 小时以上，相当于 
5 天多，无需人工值守，支持异常自动恢复和任务续跑。 
 
DiT 训练工程实践 
在 DiT 图像生成任务中，为了提升生成图像的多样性，在预训练阶段需要使用大量
不同分辨率的图像。这样虽然增强了模型的泛化能力，但也带来了训练工程上的一大挑
战：图像尺寸高度不一致，导致每个 batch 中图像维度不同，不能直接拼接。 
常规的做法是统一 padding，也就是把所有图像补齐到最大尺寸再训练。但问题在
于，大量 padding token 实际上不含有效信息，却要耗费完整的显存和计算资源，导致
训练效率明显下降。 
为了解决这个问题，我们采取了两步式的数据端优化方案： 
第一步是图像分桶： 
我们将训练集中的图像按宽高比或分辨率进行划分，分成若干个 Bucket，每个 
Bucket 内图像尺寸相近。这样在组成 batch 时，padding 大大减少，有效计算比提升显
 
307 
InfoQ 架构师2025年第一季 
著。 
第二步是动态 Batch Size： 
由于不同 Bucket 的图像大小不同，对显存的占用也不同。我们根据每个 Bucket 
的平均图像尺寸动态分配 Batch Size——小图用大 Batch，大图用小 Batch，在显存范围
内尽可能提高吞吐。 
通过这套“图像分桶 + 动态 Batch Size”的组合策略，我们将 DiT 的整体训练时
长缩短了近一半，训练效率提升接近一倍，是图像生成任务中提升性价比的核心工程优
化。 
 
在模型计算优化方面，我们发现要深入提升训练性能，就需要进行算子级别的优化。
我们进行了算子融合，例如将三步操作合成一步，减少了 kernel 的启动次数和显存算
力的利用，使 batch 可以更大。 
此外，我们还进行了编译优化，让计算更高效。激活重算在图像领域，尤其是文生
图领域应用较多，因为分辨率和帧数较多时，显存压力很大。激活重算即在前向时不存
激活值，反向计算时重新计算，从而节省显存，使 Batch 可以更大。通过这些优化，显
存减半，支持更高的 Batch，训练速度提升了 60%。 
 
308 
热门演讲实录 | 落地和进化 
 
在通信优化方面，我们主要使用 FSDP 框架。FSDP 的 AllGather 通信原本需要等
反向做完再统一进行梯度更新。我们进行了两个优化：一是将 AllGather 拆成多个小段
按层触发；二是启用独立的 CUDA 流，让反向传播与梯度并行进行。 
优化后，AllReduce 或 AllGather 与反向传播交叉进行。我们还进一步发现，尽管
进行了分桶和动态 batch size 优化，但仍有卡间像素计算量的差异。 
于是，我们进行了彻底对齐 GPU 负载的优化，通过 Package Shuffle 操作，将一个
大 batch 的图像打包成一个 package，使单个 batch 从 package 中获取相同的像素量，
且这个像素量是 GPU 的最大算力。 
最终，完全对齐了算力，充分利用了 GPU 的算力。优化后，通信等待时长降低了 
98%，训练更流畅，卡间的像素对齐进一步提速了 10%。 
 
309 
InfoQ 架构师2025年第一季 
 
在训练稳定性方面，我们曾遇到一个案例，原本 7 天可完成的任务，因中断问题
最终耗时 15 天，算力利用率很低。 
为此，我们进行了优化：一是异步 checkpoint 保存，不占用主进程，节省时间；
二是利用分布式缓存，降低 IO，提高速度；三是触发式保存，当检测到异常时触发保
存，减少重复训练时间。 
落地后，快照保存时间提升了 88%，故障恢复时间下降了 90%，实现了分钟级恢
复。 
 
在 LLaVA 和 DiT 模型的训练工程实践中，我们通过一系列的优化措施，实现了显
 
310 
热门演讲实录 | 落地和进化 
著的性能提升。以下是我们实践的总结。 
数据处理优化 
优化措施： 
• 对于 LLaVA 模型，我们采用了多进程数据加载架构，支持海量数据训练，减少
了 IO 等待时间。 
• 对于 DiT 模型，我们实施了图像分桶和动态 Batch Size 策略，最大化了单卡负
载。 
落地成效： 
• LLaVA 模型的 GPU 满载率提升，训练速度加快了 50%。 
• DiT 模型的训练时长减少，效率翻倍。 
模型计算优化 
优化措施： 
• 对 LLaVA 模型，我们进行了流水线并行与离线拼接优化，释放了存储瓶颈，提
高了计算密度。 
• 对 DiT 模型，我们融合了算子、激活重算，提升了大模型训练效率。 
落地成效： 
• LLaVA 模型的算力利用率（MFU）从 34% 提升至 45%。 
• DiT 模型的显存使用减少 50%，训练吞吐提升约 60%。 
分布式通信优化 
优化措施： 
• 对 LLaVA 模型，我们引入了 RDMA 加速、TP/PP 优化、AllReduce 融合压缩通
信等技术。 
• 对 DiT 模型，我们采用了 FSDP 独立通信流优化，GPU 负载对齐，提升了训练
 
311 
InfoQ 架构师2025年第一季 
同步效率。 
落地成效： 
• LLaVA 模型的通信延时降低，训练加速 20%。 
• DiT 模型通信与计算 overlap 实现率超 98%，训练流程更流畅。 
训练稳定性建设 
优化措施： 
• 对 LLaVA 模型，我们实施了流水线数据扩播，异常检测与故障自动恢复。 
• 对 DiT 模型，我们采取了异步保存、分布式缓存和触发保护，缩短了恢复时间。 
落地成效： 
• LLaVA 模型无需人工值守，支持 139+ 小时千卡稳定训练。 
• DiT 模型故障恢复耗时降低 90%，续训恢复至数分钟。 
Al Infra 未来展望 
在探讨人工智能基础设施（AI Infra）的未来时，我认为我们需要从三个维度进行展
望：数据、算法和算力。 
首先，在数据维度上，我们正从海量数据向高质量数据转变。数据并非越多越好，
关键在于质量。为此，我们将引入自动化的数据清洗机制，以提高样本的准确性，同时
保护用户隐私，例如应用差分隐私技术。在多模态数据方面，我们将加强模态增强和合
成，通过智能方式生成更多有用的样本，以支持训练效率的提升。 
其次，在算法维度上，我们正在从规模扩展向智能优化转变。在模型结构上，我们
将探索更高效的结构，如混合专家模型（MoE）和微分流水线并行等。在训练范式上，
我们强调自监督学习和小样本学习，以减少对人工标注的依赖，提高模型的泛化能力。 
最后，在算力维度上，我们正从扩展性向高效性转变。通过统一调度和定制加速芯
片，我们可以实现资源的最大化利用。此外，我们还将注重可持续发展。 
 
312 
热门演讲实录 | 落地和进化 
我们的落地路径是围绕这三个方向重构一个真正的平台化 AI 基础设施。从数据准
备到算法选择，再到分布式训练，以及评估优化，整个链路将实现高度自动化，形成一
个闭环。这样，多模态大模型的训练将进入一个更大规模、更强泛化能力以及更低成本
的新阶段。 
我的核心认知是，训练工程必须稳定，模型才能走得更远。只有底层链路畅通，保
证算力资源长时间稳定运行，模型才能顺利完成迭代。其次，训练链路必须彻底打通，
才能实现真正的多模态闭环。 
从数据到通信，从模型到调度，只有每个环节都高效协同，整个系统才能高效运转。
最后，算法的突破不能仅仅依靠工程来补短板，而是需要工程来推动。即使模型再先进，
如果缺乏稳定、高效的工程支撑，也很难真正落地。 
嘉宾介绍 
• 王兆雄，曾就职于京东商城和猎豹移动，拥有丰富的大数据分析和游戏服务端研发
经验，主导设计并实现了支撑数千万日活用户的轻量级游戏服务端架构。目前在 
vivo AI 研究院任职，负责过 vivo 手机智慧桌面信息流和全局搜索服务端的推荐与搜
索架构，支撑亿级用户。现负责视觉多模态大模型的训练工程，具备千卡级分布式
集群上大模型训练的丰富经验，致力于构建高性能、可扩展的 AI 解决方案。 
 
 
313 
InfoQ 架构师2025年第一季 
Java 三十周年重磅发声：James Gosling 狠
批 AI 是“一场骗局”，是科技高管“压榨程
序员的新利器” 
 
“科技行业里骗子和炒作者的数量之多，令人难以置信。”面对当下火热的 AI 
浪潮，Java 之父直言不讳地对背后推波助澜的炒作者们发出尖锐批评。 
5 月 23 日是 Java 编程语言诞生三十周年纪念日。Java 凭借着“一次编写、随处
运行”的理念彻底改变了软件开发领域。 
这三十年里，Java 经受住了技术浪潮、竞争语言和范式更替的考验。从 Applet 和 
Servlet，到微服务和云原生架构，Java 一路演进，同时又始终保持熟悉的面貌。它为开
编译 核子可乐 Tina 
  
314 
推荐文章 | Article 
源软件进入企业级市场铺平了道路。 
尽管已近三十年历史，并且面对 2025 年众多新兴语言的挑战，Java 依然是全球最
受欢迎的编程语言之一。回望 Java 的起点，也离不开它的缔造者——James Gosling。
从一位聪慧的加拿大少年，到成为全球计算领域的先驱人物，他的人生轨迹为整个技术
行业的发展提供了宝贵启示。而他对语言和技术的发展，以及对当前 AI 浪潮的深刻见
解，都持续影响着我们对编程未来的思考。 
Java 如何彻底改变开发格局 
30 年前，Java 编程语言首次公开亮相，向世界展示了“一次编写，到处运行”
（Write Once, Run Anywhere）的愿景，也为开发者们带来了一个比 C 和 C++ 更友好亲
和的选择。 
Java 最初名为“Oak”，由 James Gosling 在 1990 年代初期于 Sun Microsystems 
设计开发。虽然最初面向的是数字设备，但很快它将重心转向了彼时仍属新兴的万维网
（World Wide Web）平台。 
这门语言在语法上与 C 和 C++ 有些相似，通常会被编译成字节码，理论上可以在
任何 Java 虚拟机（JVM）上运行。这个机制旨在实现“Write Once, Run Anywhere”，尽
管由于不同 JVM 实现之间存在细微差异，这一愿景并不总能完全实现。曾有人调侃说，
与其说是“一次编写，到处运行”，不如叫做“一次编写，到处测试”（Write Once, 
Test Everywhere）——因为又一个 JVM 的意外表现让应用跑出了预料之外的结果。 
尽管如此，Java 很快便流行开来，并迅速成为企业级开发的中流砥柱。微软随后也
推出了自己的 Java 变种——Visual J++。虽然该语言在语法上符合 Java 规范，但未通过 
Sun 的兼容性测试，最终在 1999 年引发了一场诉讼。2000 年，Visual Studio 中移除了 
J++，随后该项目被正式弃用。 
Java 的热度在发布后持续攀升，直到近几年才略有下降，原因在于越来越多开发者
开始选择其他语言。根据 2024 年的 Stack Overflow 调查，Java 仍位列前十热门语言之
中，领先于 C#、C++ 和 C。而在 TIOBE 指数中，Java 虽不再是榜首，但依然排名第四。
相比之下，Python 因其在 AI 和数据科学领域的广泛应用，已遥遥领先。值得一提的是，
 
315 
InfoQ 架构师2025年第一季 
TIOBE 曾在 2015 年将 Java 评为“年度语言”。 
事实上，尽管编程趋势不断更替，Java 在企业级系统中的广泛应用，始终让对 
Java 技能的需求居高不下。这段长达 30 年的历史，不只是关于一门编程语言，更是关
于信任——一种源自 James Gosling 及其团队数十年来对可靠性、规范性和技术共享的
不懈坚持所建立起来的信任。 
James Gosling：Java 语言背后的天才 
Gosling 不仅仅是“Java 之父”，同时也是一位谦逊的天才，拥有将复杂概念简单
阐释的非凡能力。在最近一次访谈中，Gosling 分享了自己引人入胜的科技之旅，并回
顾了和他和他的团队如何在 30 年间从缔造 Java 到逐步完善的整个历程。 
Gosling 的编程之路，展现出他作为一名创新者的诸多特质。他从小家中一贫如洗，
却总能将从生活的必需品中找到创作灵感。“我的玩具是从其他人的垃圾桶里翻出来的，
还有从旧电视里拆下来的元器件。”而他亲手组装的第一台电脑，实际上是从电话公司
废弃物中回收的继电器架拼凑而成——种种成就充分展现了他的早期技术天赋。 
Gosling 父亲的朋友曾带他参观卡尔加里大学的计算机中心，这也成为他人生中的
一个关键时刻。他回忆道，“我一下子就被迷住了。屏幕、磁带、闪烁的灯光——简直
让我眼花缭乱。”这样的好奇心也贯穿了他的整个职业生涯，成为驱动他人生的一大关
键特质。 
少年 Gosling 才华横溢，通过非常规的方式自学掌握了编程：他翻垃圾桶找到带密
码的穿孔卡片。当时多数青少年都更想去当售货员，而 Gosling 在高中阶段就在大学物
理系找到了第一份工作，内容是开发一款处理卫星数据的软件。谈到这段影响到自己人
生成长的经历时，他表示“我玩得很开心，而且他们居然还愿意为此付钱，真是太棒
了！” 
他的早期编程经历涵盖 IBM 大型机上 PL/1 和 Fortran 语言、PDP-8 汇编语言和 
CDC 6400 代码。一贯低调的他漫不经心地提到，自己“曾在暑假从事过编写 COBOL 编
译器的工作”——事实上，这对大多数经验丰富的程序员来说都仍是一项艰巨的任务。 
Gosling 也毫不惮于直截了当地表达自己对于学术界的看法。他将卡耐基梅隆大学
  
316 
推荐文章 | Article 
享有盛誉的计算机科学博士项目描述为“实际上就是一个把研究生当作廉价劳动力的研
究机构”。一贯务实的他曾在学习期间抽出时间在湾区一家初创公司工作，之后又回到
匹兹堡完成了学业。 
从卡耐基梅隆大学毕业后，他的第一份工作是在 IBM 研究院。多年后他对 IBM 的
论文依然尖锐，宣称蓝色巨人的行为类似于“坚持要搬起石头砸自己的脚”。这也是对
他冷幽默下深刻见解的最好诠释。早期工作经历影响了他在 Sun Microsystems 的工作方
式，也真正让他的职业生涯蓬勃发展、一路起飞。 
 
Java 封神之后，Java 之父的人生轨迹 
诞生于三十年前的 Java 无疑是 Gosling 最具标志性的成就。在被问及对创造出如
此具有影响力的成果有何感受时，他仍然以谦逊的视角予以回应：“我时不时会在街上
被人拦住，问「您是 James Gosling 吗？谢谢你让我有了工作。我写 Java 代码有 20 
年了，这是份很棒的职业。」这样的情景总是让我心满意足。” 
 
317 
InfoQ 架构师2025年第一季 
回顾 Java 的演变历程，Gosling 提到了 lambda 表达式（JDK 8 中的新增设计）等
功能特性，而且希望这些成果能够早点出现。但他也解释了自己怎样以谨慎的态度设计
编程语言：“我不希望添加任何不合适的东西。”泛型和 lambda 表达式等功能特性的
挑战在于，如何为其确定最佳实现方式——“前 90% 总是容易理解，但最后 10% 的
收尾工作则非常困难。” 
谈到甲骨文在收购 Sun 之后如何管理 Java 项目时，Gosling 同样给出了审慎的评
价：“他们的表现比预期中要好，但也必须得承认，我本就没抱太高的期待。”他认为
社区才是推动 Java 持续发展和创新的绝对支柱。 
Gosling 还提到，如今的 Java 已经与云环境高度契合，并强调“过去三十年间的一
系列变化，使得 Java 在云环境中变得非常非常稳定。”他强调了 Java 在多核处理器
应用、内存管理、特别是垃圾收集方面的改进，并认为最新版本的垃圾收集机制“拥有
现象级的实际表现”。 
超越 Java：后 Sun 时代的创业之路 
在 2010 年甲骨文收购 Sun 之后，Gosling 短暂休息了一段时间，之后加入了谷歌。
在搜索巨头效力“整整六个月”之后，他又加入 Liquid Robotics，负责自主海洋机器人
的控制系统开发工作。这份工作既有技术挑战，也让他获得了施展才华的独特空间：
“想做好这方面开发，就必须熟悉浮潜。而且我们工作的重要组成部分，就是在夏威夷
待上一个礼拜甚至一整个月。” 
Liquid Robotics 的工作涉及环境监测，包括研究北极和南极地区的温度变化。然而 
Gosling 也承认，“研究这方面问题的项目根本就拿不到任何资金”，因此风险投资公
司一直不支持大家在这方面“浪费金钱”。随着资方要求公司向国防应用领域转型，
Gosling 对这样的应用思路感到不安，并最终选择离开。 
Gosling 接下来加入了亚马逊云科技，在那里参与了 Greengrass 项目和其他开发工
具的创作，直到去年正式退休。在整个职业生涯中，Gosling 不仅始终聚焦于技术，也
没有忘记牢牢把握自己的道德判断。 
 
  
318 
推荐文章 | Article 
关于开源与行业趋势：冲破炒作迷雾 
关于开源事业的发展，Gosling 观察到“很多人已经找到了让开源成果为自己所用
的方法”，而不同的应用模式也在不同环境下应运而生。在 Sun，开源“既关乎协作，
也与开发者关系和市场营销密不可分”，由此建立起的自下而上的应用方式，与自上而
下的传统企业应用思路截然不同。 
在被问及“低代码和无代码”趋势时，Gosling 结合自己丰富的从业背景提出了质
疑：“几十年来，人们一直在聊低代码和无代码，而且基本思路早在 COBOL 时代就已
经喊过。”在他看来，这种方法往往只在特定领域表现出色，但在其他普适性的复杂问
题上却举步维艰。 
在谈到 AI 和机器学习（ML）时，Gosling 也对专业术语提出了批评：“对于 AI 
和机器学习，我最难接受的就是它们的名称。”在他看来，与其使用这些具有误导性、
将其与人类推理能力关联起来的迷惑表述，“高级统计方法”反而是更准确的字眼。
Gosling 认为这些技术也仅仅是“极其复杂的锤子和螺丝刀”：仍然是人类手中的工
具，而不是能够威胁就业的自主系统。 
Gosling 主要使用 NetBeans IDE 进行开发，并对该项目的开源 Apache 许可属性与
专注的技术社区赞不绝口。他对那些固守过时工具的开发者们感到失望：“最让我难以
理解的，就是那帮固执坚持上世纪 80 年代甚至 70 年代老古董的家伙——他们还觉得 
Vi 好用……如果回到上世纪 70 年代，Vi 倒还能算是高科技。” 
Gosling 承认自己偶尔也会使用 Vi，毕竟它“无处不在”，但同时也提倡在成规模
的编码工作中使用现代开发环境。 
有趣的是，最终奠定 Java 虚拟机（JVM）基础的概念其实源自 Gosling 研究生阶
段的经历。他曾探索过“架构中立的分发格式”的想法，并尝试在不同机器架构之间进
行跨指令转换。 
这项早期探索为 JVM 的开发提供了依据，也让 JVM 后来成为一项基础性技术—
—不仅造福于 Java，更让许多其他语言能够在不同的硬件平台上顺畅运行。“一次编写、
随处运行”这个当初被认为缺乏足够的数学基础、因此无法支撑起一篇博士论文的愿景，
 
319 
InfoQ 架构师2025年第一季 
最终改变了全球的软件开发实践。 
退休后在干什么 
去年从亚马逊云科技退休之前，Gosling 致力于 Greengrass 的开发——这是一套用
于构建物联网应用的 AWS 框架。此项目完美体现了 Gosling 的技术理念：以优雅的简
洁性设计，解决复杂且普遍的问题。 
Gosling 解释道，“从「一款能用的玩具」到可以真正大规模部署的成果，这其中
有很多可以作为模板和参考的东西。”而他拥有一种独特的能力，可以将复杂的想法瞬
间变得触手可及。Greengrass 就囊括并实现了大量繁琐的元素——无线更新、远程命令
与控制、遥测、网络可靠性、安全性、凭证管理等等……让开发人员能够专注于打磨特
定应用中的核心特性。 
Greengrass 的设备端部分为开源成果，体现了 Gosling 长久以来对于社区贡献的感
激之情。这种方法也的确带来了效益，用户们自发创建了面向亚马逊并未优先考虑的其
他平台（例如 RISC-V）的接口。他对这一切深感欣慰。 
在 Greengrass 之后，Gosling 又投身于另一个与软件开发工具相关的项目，却不料
就此卷入一场“AI 末世录”。 
科技大佬在胡说八道，编程根本不会被 AI 淘汰 
在前段时间接受采访时，Gosling 对于席卷整个科技行业的 AI 革命提出了自己的怀
疑。他直言不讳地表示，“这基本上就是一场骗局”，并将 AI 描述为“自带误导属性
的营销术语”。虽然 Gosling 承认这些系统背后有着令人印象深刻的数理逻辑，但他也
担心 AI 这个标签会掩盖其作为高级统计技术的本质。 
他还特别批评了那帮炒作 AI 风潮的风险投资者，强调“科技行业里骗子和炒作者
的数量之多，令人难以置信。”而且，风险投资者们“只关心成功获利”，而不是开发
出真正有用的技术。他预测称，“绝大多数 AI 投资都会被烧个精光。” 
 
  
320 
推荐文章 | Article 
氛围编程靠谱吗？演示令人印象深刻，但实用性有限 
在谈到生成式 AI 编程助手时，Gosling 承认它们第一眼看上去令人惊艳，但也强调
了其显著的局限性。他警告称，“刚开始接触氛围编程，会觉得它特别的酷炫。可一旦
项目变得稍微复杂一点，氛围编程就会很快耗尽开发者的脑力。” 
在 Gosling 看来，根本问题在于这些工具的基本原理是抓取现有代码示例，因此只
能重现它们之前见过的代码。这跟专业软件开发存在根本性的冲突，因为此类开发中
“真正有趣的东西永远是开拓性的”，没办法用已经被打包成现成库的方式来实现。 
Gosling 认为，AI 技术最有价值的编码应用并不在于取代程序员，而是“生成没
人愿意去写的文档”——它在本质上更像是一种智能搜索引擎，可以理解代码的工作原
理，并帮助解释如何使用特定的 API 或者功能。 
Java 的演变：语言特性与运行时改进 
在被问及 Java 的最新进展时，Gosling 分享了一系列有价值的增强特性：“Java 
在类型推断方面的许多改进都很棒，数组声明的演进也非常顺利。我认为这方面还有进
一步提升的空间。” 
但他同时强调，Java 近期最令人印象深刻的进步还是体现在其运行时环境和库方面。
他解释道，“如今 JVM 的代码质量非常高，现代 JVM 版本中的垃圾收集机制令人惊
叹，线性性能也极其出色。” 
他特别赞扬了内存管理与性能可预测性方面的改进：“长期以来，Java 的存储管理
一直比 malloc 和 C 更加高效，如今更是达到了前所未有的水平。”曾经的垃圾收集机
制需要暂停“10 到 20 秒”，而如今在精心设计下已经可以缩短到几毫秒。“哪怕没
那么精心，整个过程也完全可以在亚秒级别完成。”JVM 现在还能以相当惊艳的效率处
理“一切大到离谱的内存空间”。 
在被问及应该使用哪种编程语言来重建美国航空管理局（FAA）的空中交通管制系
统时，Gosling 首先否定了这个问题的预设前提。“这就像建造一栋房子，我们最开始
要考虑的肯定不是该买什么品牌的锤子。” 
 
321 
InfoQ 架构师2025年第一季 
相反，他主张先吃透问题领域——包括通信系统、国际法规、飞机跟踪/防撞以及
飞行路径规划等需求，之后再为不同组件选择合适的技术。他建议称，“一定得根据打
算实现的目标特性来做选择”，但同时也承认 Java 坚实的可靠性在关键大型系统中确
实表现较好。 
AI 世界中的编程未来 
尽管 AI 技术取得了长足进步，但 Gosling 坚信编程仍是一项必备技能。“如果现
在我有个小孩，那我绝对会先教他们编程”，他解释称“哪怕 AI 最终接管了一切，人
们也必须有能力理解它们的系统是如何运作的。” 
他驳斥了扎克伯格等一众科技高管关于 AI 技术将减少软件工程师需求的说法，称
其“完全是自私自利的胡说八道”。在他看来，这些言论只是一种定位性的策略，同时
也是在给他们疯狂榨取员工更多产出做出不负责任的铺垫和威胁。 
Java 能够长盛不衰的秘诀 
当被问及为什么 Java 能够屹立三十年，而许多其他编程语言却逐渐淡出舞台时，
Gosling 总结了几个关键原因：聚焦实际问题、尊重用户、坚持向下兼容、提升开发者
生产力，以及始终将可靠性放在首位。 
他解释道：“Java 从不追逐潮流，而是始终专注于把事情做好，尤其是帮助工程师
解决现实中的需求。”正是这种对实用性而非流行趋势的坚持，让 Java 在高度依赖稳
定性的企业环境中历久弥新，始终保持活力。 
谈及 Java 在被甲骨文收购后的发展，Gosling 给出了一个“B+”的评价。他坦言：
“我当时真的很担心他们会乱来。毕竟，甲骨文的过往战绩几乎可以说是‘巧取豪夺’
的代名词。但说实话，他们最后竟然还算克制，真的出乎我意料。” 
尽管他认为 Java 团队理应获得更多资源支持，但也承认甲骨文在整体上并未过多
干预研发节奏，这已大大超出了他最初的悲观预期。 
30 多年来，Java 已从一门新兴语言发展成为企业赖以生存的语言。诚然，它可能
不具备当今 AI 应用所需的那些耀眼功能，但它仍然是当今许多现代软件开发的基础。
  
322 
推荐文章 | Article 
蓬勃发展的生态系统和庞大的爱好者社区意味着，在步入第四个十年之际，Java 依然具
有举足轻重的地位。 
参考链接 
• https://www.theregister.com/2025/05/23/30_years_ago_java_arrived/ 
 
323 
InfoQ 架构师2025年第一季 
Redis 之父：哪怕被喷我也得说，AI 远远落
后于人类程序员！ 
 
Redis 之父 Salvatore Sanfilippo 近日分享了自己的一次研发经历并直接表达了自己
的观点：人类程序员仍比大模型更出色。“因为我们能够真正打破常规、设想出一些奇
特且并不精确、但就是更有成效的解法，而这对大模型来说则极其困难。” 
在社区中，大家更习惯称呼 Sanfilippo 为 Antirez。Antirez 在 2009 年启动了 Re-
dis 项目，并在 2020 年卸任维护者职位，转而担任 Redis Labs 的技术顾问，继续为 
Redis 的未来发展提供指导。Antirez 的分享迅速引发广大开发者的激烈讨论。 
 
作者 褚杏娟 
  
324 
推荐文章 | Article 
Antirez：AI 水平不错，但远落后人类智能 
“今天我要分享一个人类为何仍比大语言模型更有优势的小故事。道德澄清，我并
不反对 AI 或者类似的技术成果，持续关注我的朋友都知道。我经常用大模型，现在也
一样。之所以会有这段故事，是因为我想测试自己的想法、进行代码审查、看看 AI 会
不会有比我更好的灵感、探索点专业范围内的更多可能性之类。”Antirez 在开篇写道，
并直接抛出了结论： 
总之，我得出的结论是：虽然目前的 AI 水平不错、颇具实用性，但仍然远远落
后于人类智能。我知道这是个很有争议的结论，容易在网上挨喷，但……我的感
受就是如此。 
接下来，Antirez 讲述了自己的经历。 
最近 Antirez 正在为 Redis 开发 Vector Sets，打算修复一个复杂的 bug：在离开 
Redis 期间，Antirez 的同事们引入了防止数据校验通过但 RDB 和 RESTORE 负载损坏
的功能。此功能会默认关闭，只是为需要的人多提供一层更强的安全保障。 
但有一个比较大的问题：为了让 HNSW 能够快速保存到 Redis RDB 并加载回来，
Antirez 序列化了 graph 表示，而非元素—向量对，否则就得把数据重新插入 HNSW，
这会把速度拖慢 100 倍！总之，Antirez 将各节点与其他节点间的所有链接存储成整数，
然后把它们解析成指针。 
这是个很实用的技巧，效果也不错。然而，在将这种处理方法跟表示的随机损坏、
还有 Antirez 对于 HNSW 的改进结合起来，强制各节点间建立互换链接（Antirez 自己
编写了 HSNW 实现，其中包含许多有用的功能，但不少功能的实现都离不开互换链接）
时，则可能发生以下情况： 
1. 加载损坏的数据，该数据表明 A 链接到 B，但 B 不再链接到 A（节点 ID 损
坏）。 
2. 删除掉节点 B：由于互换性发生违反，Antirez 和同事们不会清除从 A 到 B 的
链接。 
3. 之后在扫描该 graph 时，一旦到达 B 时就会遇到 A：释放后重用…… 
 
325 
InfoQ 架构师2025年第一季 
因此在加载数据之后，Antirez 需要检查每个链接是否互换。在一般情况下，结果
应该是 O(N^2)，代表着对于每个节点，开发人员需要扫描所有层级、在每个层级上扫
描该节点的全部邻居，再通过扫描该层级的链接来检查其是否同样链接至该节点。“这
显然不好。” 
人类对大模型 
Antirez 首先采用了最常规的办法，看看模糊测试器能不能找到 bug。结果确实有
效，但加载一个包含 2000 万向量的大型向量集的时间从 45 秒变成了 90 秒左右。这
当然不能接受，于是 Antirez 打开了 Gemini 2.5 PRO 的聊天窗口，问大模型：“我该怎
么办？有没有速度更快的办法？” 
Gemini 给出的最佳方案是：对相邻链接的指针进行排序，这样就能使用二分查找。 
Antirez 认为这也有点道理……他知道可以这样，但不确定在拥有 16/32 个指针的
数组中，这种方法是不是真的更快。所以 Antirez 问：“还有其他办法吗？” 
很遗憾，Gemini 给不出更好的方案。所以，Antirez 告诉它：那咱们这样想，当我
们在 X 层级上看到 A 链接到 B 时，会将其以 A:B:X 的形式将其存储在一个哈希表中
（我们会始终对 A 和 B 进行排序，使得 A>B，因此无论方向如何链接均相同），而在
再次看到该链接时就将其清除。这样我们只需要扫描整个表，类似于我们在将 ID 解析
为链接中指针时所做的那样。如果最后该哈希表不为空，我们就能确定肯定存在着某个
非互换链接。 
Gemini 表示这确实是个好主意，虽然这需要使用 snprintf()来创建键，而且哈希运
算也需要时间等待……但已经比 Antirez 的方法（甚至包括对指针进行排序）要好。
Antirez 则提醒 Gemini：其实这里不需要 snprintf()，可以直接 memcpy()固定大小的键
中的指针。Gemini 再次被说服。 
之后，Antirez 告诉 Gemini：要不要对 A:B:X 使用一个固定大小的累加器？这甚至
连哈希表都不需要。每当我们看到一条链接（A:B:X，也就是 8+8+4 个字节）时，我们
就把它跟当前的 12 字节累加器进行异或运算，而如果存储两次，则结果抵消，因此最
后如果寄存器非零，我们就可以判断是否出了问题！ 
  
326 
推荐文章 | Article 
Antirez 还预料到了 Gemini 可能提出的异议，并提前做好了回应——尽管 Redis 
会默认关闭此功能，但一部分用户确实会需要启用这项额外检查来获得更强的保护，以
防攻击者故意创建恶意负载。 
Gemini 对这个想法印象深刻，但仍固执地提醒 Antirez：指针也拥有类似的结构、
只是改变了几个 bit。所以如果有三条伪链接 L1、L2 和 L3，L1 和 L2 之间的异或运算
结果有可能跟 L3 的 bit 相同，这样我们就会遇到漏报的问题（寄存器仍然为零）。
Antirez 还想到，分配器往往非常容易预测，而且很可能被外部人士猜到。 
Antirez 询问 Gemini 该如何改进这个问题，它还是没什么好主意。后来 Antirez 想，
其实可以用一条质量够好且速度够快的哈希函数来做哈希处理，比如 murmur-128 之类
（在这项任务中，不需要它具有加密属性），于是向 Gemini 提出了以下方案： 
1. 获取链接 A:B:X，但使用通过/dev/urandom 获取的种子作为所有密钥的前缀，
这样我们实际上就得到了 S:A:B:X。 
2. 只需将 murmur-128（S:A:B:X）的输出进行异或运算，放入 128 位寄存器即可。 
3. 最后，我们检查寄存器是否为 0（所有链接均互换）。 
Antirez 要求 Gemini 对此进行分析，而它最终给出了极高的评价，认为这样大大降
低了随机碰上异或恰好为 0 的孤立链接的可能性，而且外部攻击者也无法抓住这点乘
虚而入——毕竟“S”未知，指针也需要控制，所有这些偶然因素很难都碰在一起。另
外，这项功能只是一种锦上添花式的额外保护措施，默认关闭、需要用户主动开启，因
此从实践角度看应该不会造成太大的性能损失。 
Antirez 刚完成整个分析，就坐下来写了文章分享。 
“我不确定自己会不会用上这套系统（很有可能会），但事实证明人类的创造力相
较大模型仍有优势，因为我们能够真正打破常规、设想出一些奇特且并不精确、但就是
更有成效的解法，而这对大模型来说则极其困难。”Antirez 说道。 
最后，他补充道，“尽管如此，在验证自己思路的可行性过程中，Gemini 仍然发
挥了重大作用。所以……我或许应该把它当成一位‘足够聪明的副手’看待，在讨论中
逐步摸索出更好的答案。” 
 
327 
InfoQ 架构师2025年第一季 
开发者：盲目自信的“AI 橡皮鸭” 
“这与我的体验相符。实际上，我觉得大模型助手对我来说很大一部分价值在于，
它像一个有一定智能的‘橡皮鸭’一样可以与我交流。现在这个‘鸭子’偶尔还会提出
异议，甚至有时还能帮我完善思路。”开发者 mattnewton 提到。 
编者注：小黄鸭调试（rubberducking）是一种通过用口头或书面自然语言清晰描
述问题来调试代码的方法。其名称来源于《程序员修炼之道》中的一个故事，故事中程
序员会随身携带一只小黄鸭，强迫自己逐行向鸭子解释代码，以此来调试代码。 
“我也有过类似的想法”有其他开发者赞同道，“在结对编程时，有一个 AI 橡皮
鸭可以让你倾诉和交流想法会很棒（这样你就不会在同事面前显得很笨，也不会浪费他
们的时间）。”这个开发者做了一个支持自带 API key 的  VSCode 插件，它使用了 
OpenAI 的实时 API，可以和一个橡皮鸭进行互动式语音对话。 
可以看出，一些开发者已经可以把大模型当编程助手看待，但这个助手仍然让人
“闹心”。 
“这是一只极其自信的鸭子，其自信程度与它的能力完全不成比例。我已经看到太
多的人因为与它交谈而误入歧途。”开发者 marcosdumay 指出。 
有人跟贴赞同道，“这正是我很快关掉 JetBrains AI 助手的原因：多行补全功能严
重干扰了我的思路，尤其是当它提供看起来正确、实际错误的建议时。为了判断这些建
议是否正确而停下来分析，会彻底打断我的思路。” 
还有开发者表示，大模型对其来说不是“橡皮鸭”，而是“错误答案”。“我让大
模型做一些简单但繁琐的事，它却错得离谱。然后我被气得不行，都有劲儿自己动手干
了。” 
有开发者指出，有效利用大模型的关键主要取决于经验。“我写软件的时间够长了，
所以当我给大模型一些代码和一个问题时，我能立刻判断大模型是否理解了，或者是不
是被一些不相关的问题迷惑住了。但初级开发人员会很难受，因为大模型生成的代码表
面上通常质量很高，即使功能完全错了也看不出来。” 
  
328 
推荐文章 | Article 
“我是自学编程的，平时写代码也不多，但我觉得大模型对我帮助非常大。它们能
给出具体的答案，而这些问题如果靠我自己查文档或看 Stack Overflow，可能要花很长
时间才能搞明白。甚至有时候它们能生成代码片段，我只需要判断这些代码是否可行就
行了。”开发者 cogogo 说道。但随后 cogogo 也指出，“我很难想象，如果一个人从
一开始学编程就接触了大模型，那该怎么教他。用大模型太容易走捷径了，这样一来，
学编程本来需要的那些关键性思维能力和解决问题的技巧就很容易丢掉。而恰恰这些能
力，既是写代码所需的，也是能真正把大模型用好的前提。” 
结束语 
但 Antirez 说的情况在两年，甚至更久一些的时间后，还会成立吗？ 
NVIDIA 首席执行官黄仁勋在去年 2 月份时候声称，随着 AI 的迅速普及，编程可
能已经“凉了”。对于任何想要进入科技行业的人来说，学习编程不应该成为优先事项。 
今年 3 月份时，Anthropic CEO Dario Amodei 也加入了讨论，并指出软件工程是非
常容易被 AI 自动化的领域：“编程是 AI 进展最快的领域之一。我们发现，再过三到
六个月，我们可能就会进入一个由 AI 编写 90% 代码的世界。而再过 12 个月，AI 可
能几乎能写出所有代码。” 
而微软首席技术官 Kevin Scott 的预测则是，到 2030 年，95% 的编程代码将由 AI 
生成。不过，他迅速澄清，这并不意味着软件工程的工作将完全由 AI 接管，人类依然
会写代码，但这一变化会让我们从编程语言的输入大师，转变为 AI 指令的引导者。 
无论 AI 能否写 100% 的代码，但像 IDE、版本控制、自动化测试一样，AI 正在逐
渐成为每位开发者的关键工具，比如定期使用像 GitHub Copilot 这样的 AI 助手；借助 
AI 查找 bug 和优化性能；利用 AI 快速进行原型设计与测试。不同之处在于：工程师
不只是使用 AI，还要理解 AI。 
至少当下，AI 抢不了程序员的饭碗，但确实改变了开发者的工作方式。AI 能生成
代码、自动化任务，甚至调试软件，但它仍缺乏创造力、批判性思维与人类直觉。 
因此，现在与其问“AI 是否取代软件工程师”，或许更好的问题是：“软件工程师
如何随着 AI 而进化？” 
 
329 
InfoQ 架构师2025年第一季 
参考链接 
• https://antirez.com/news/153 
• https://www.windowscentral.com/software-apps/work-productivity/anthropic-ceo-dario-
amodei-says-ai-will-write-90-percent-of-code-in-6-months 
• https://techpoint.africa/guide/will-ai-replace-software-engineers/ 
  
330 
推荐文章 | Article 
“前端已死”是危言耸听吗？ 
 
随着互联网技术的快速发展，大前端领域正经历着前所未有的变革。从传统的 
Web 开发到移动应用、小程序、IoT、乃至新兴的 AR/VR，大前端技术不仅需要适应越
来越多样化的需求场景，还面临着如何更高效地利用现有资源、提升用户体验等挑战。
那么，前端开发的本质变了吗？“AI 代替前端”是危言耸听吗？ 
近日 InfoQ《极客有约》X QCon 直播栏目特别邀请了淘天集团 1688 终端架构负
责人曹立成（蒜蓉）担任主持人，和快手基础架构中心负责人周全、菜鸟网络资深技术
专家唐爽一起，在 QCon 全球软件开发大会 2025 北京站即将召开之际，共同探讨前
端在 AI 时代下的生存法则。 
 
作者 QCon 全球软件开发大会 
策划 燕珊  编辑 宇琪 
 
331 
InfoQ 架构师2025年第一季 
部分精彩观点如下： 
• 会 AI 不一定让你不被淘汰，但是不会 AI 早晚会被淘汰。AI 不会淘汰你，把 
AI 用好的人可能会淘汰你，淘汰你的从来都是人。 
• 全栈化改革的目标是减少中间的协同成本，让一个开发者能完成更多任务，同时
将更专业的人才放在更需要专业技能的岗位上 
• AI 对前端领域的冲击，实际上对有经验的资深开发者是一个利好。 
• AI 目前还不是神，它仍然像一个 3 到 4 岁智力的孩子，我们需要为其提供更
加细致的指导，以确保其能有效工作。 
• 虽然 AI 可以完成大量工作，甚至高达 80% 的工作量，但那剩下的 20% 才是
最为关键的部分，而这正是我们人类的优势所在。 
以下内容基于直播速记整理，经 InfoQ 删减。 
完整直播回放可查看：https://www.infoq.cn/video/IoNRKHcNaV3p0TPX2JFz 
前端开发的本质发生了什么变化 
曹立成：我们都知道，前端行业这两年变化很大，AI 让代码生成更容易，全栈化
让前端的边界变得模糊，甚至很多人都在问：前端开发会被 AI 取代吗？全栈是进化还
是无奈？写代码不重要了，关键是怎么向 AI 提问，让它产出更好？今天这场直播就是
要直面这些扎心的问题！ 
我们先从一个最基础的问题聊起——前端开发的核心问题变了吗？过去，我们前端
工程师天天研究怎么写代码、怎么优化性能、怎么造轮子。但现在，AI 工具越来越强。
有人说：“前端已经不是如何写代码的问题，而是怎么结合 AI 快速产出的问题”，你
们怎么看？前端开发的本质真的变了吗，有哪些痛点？ 
曹立成：我认为现在前端与客户端的边界逐渐模糊，近年来“大前端”概念的兴起
和公司组织架构的调整都体现了这一点。尽管形式上有所变化，开发的本质并未改变。
过去，前端领域常被称为“娱乐圈”，充斥着各种框架和工具，甚至有一些开发者因此
成为“小明星”。然而，近年来这种现象减少，行业更加关注技术的实际价值。许多大
厂为了 KPI 推出的项目，往往只是制造轮子，未能解决根本问题。如今，行业更注重技
  
332 
推荐文章 | Article 
术的实际应用。尽管轮子数量减少，但对工程师的技术要求却提高了。经过多年的优胜
劣汰，行业逐渐回归理性。许多框架和技术方案并未真正解决本质问题，开发的核心仍
然在于代码本身。 
AI 在前端开发中提供了一些辅助，例如低代码搭建和上层 UI 开发。然而，在中间
件和底层开发领域，AI 的帮助有限。在底层代码的采纳上，开发者仍持谨慎态度。前端
开发不仅涉及 JavaScript，工具链复杂，边界也在不断拓宽。此外，前端还涵盖 VR、AR 
等混合现实技术，涉及图形学、音视频处理等多学科知识。AI 在这些领域的辅助作用有
限，尽管它可以取代一些初级工作，例如编写简单的 UI 组件。 
从我个人的经验来看，AI 在代码生成方面的采纳率较低。去年，我的代码中 AI 生
成的部分不到 10%。尽管 AI 能力增强，但它更适合作为提效工具，而非完全替代开发
者。因此，无论是 Web 前端还是客户端开发，开发者都应更加关注语言本质和性能问
题，AI 无法解答代码在不同环境下的执行效率和内存消耗等具体问题。这些影响因素复
杂多样，仍是开发者需要关注的重点。 
周全：我认为这个问题有些危言耸听，AI 带来了量变，但尚未形成质变。举个例
子，团队中常有新人或外包人员协助开发代码，他们能否完全替代正式开发者？显然不
能。代码仍需 review、质量检测、自测和优化，这些环节离不开正式开发者的参与。AI 
工具的作用与此类似，它可以帮助生成代码，但开发者仍需主导整个过程。 
此外，我想问大家多久更新一次开发工具或效率工具？例如，终端工具、IDE、插
件等。过去多年，开发工具一直在迭代，从 Turbo C 到现代 IDE，再到各种插件，效率
工具从未停止进步。然而，从未有人质疑这些工具会取代开发者。使用新 IDE 提升 10% 
的效率，并不意味着团队可以裁员 10%。效率工具无法完全取代开发者，但它们是开发
者必须掌握的技能，掌握这些工具可以显著提升开发效率和竞争力。如果开发者连基本
的工具更新都跟不上，那么被淘汰是迟早的事，与 AI 无关。 
AI 工具确实带来了效率的提升，让开发者能够更快地输出代码和理解需求，掌握 
AI 工具的开发者将更具竞争力。因此，问题的核心不在于 AI 是否会取代前端开发，而
在于开发者是否能够适应技术进步。会 AI 不一定让你不被淘汰，但是不会 AI 早晚会
被淘汰。 
 
333 
InfoQ 架构师2025年第一季 
唐爽：前端开发的方式确实在不断变化，但这种变化与 AI 关系不大，而是技术发
展的自然结果。我刚开始工作时，还在解决 IE6、IE7 的问题，而现在前端技术的本质
并未改变。前端始终是用技术解决业务端的问题，而不是单纯依赖 JavaScript。业务端
的问题涉及需求、体验、性能、创新、埋点、数据可视化等多个方面，这些都是前端岗
位的核心。 
AI 的出现确实减少了重复性代码的编写，例如函数生成等任务，AI 比人类更快。
然而，AI 生成的代码仍需人工评估，业务逻辑的调整和优化也离不开开发者的参与，因
为只有开发者最清楚业务需求。AI 提升了编码效率，但也带来了新的挑战。例如，AI 
可能在项目中生成重复代码，如果没有统一的架构和管控，项目腐化的速度可能会加快。
因此，AI 并未降低对开发者的要求，反而要求开发者具备更高的经验和能力，以确保项
目的可持续维护性。 
观众：AI 时代给前端开发带来了哪些改变？有 AI 加持后，整个前端会往什么方
向发展？ 
周全：大前端是最接近用户的岗位之一，仅次于产品经理和运营。因此，前端开发
者不仅需要专业能力，还需要较强的横向能力和产品思维，以确保动效、用户体验和性
能能够满足用户需求。过去，移动端开发主要局限于自身平台，难以参与 AI 等新兴领
域，因为这些领域的技术门槛较高，通常需要专业背景，如神经网络和大数据。 
然而，随着 AI 技术的发展，尤其是大语言模型的出现，技术门槛显著降低。前端
开发者虽然不直接参与模型训练或算法设计，但可以在应用层接入 AI 能力，从而扩展
开发范围。这种变化使得前端开发者的优势更加明显，因为他们具备较强的产品能力和
横向能力。当 API 的使用变得简单时，前端开发者能够更快地将技术转化为用户价值
和业务价值。此外，大前端带有一定的全栈属性。 
过去，前端开发者若想涉足后端开发，可能需要学习 Python、SQL 等技术，学习
成本较高。而现在，AI 工具可以快速解答技术问题，降低了学习门槛。这使得前端开发
者能够更高效地掌握后端、SRE 等领域的技术，从而提升综合能力。 
唐爽：我赞同周老师的观点，AI 时代为前端带来了很好的机遇。过去由于技术和
岗位的限制，前端工作主要集中在页面设计和切面实现。现在，我认为前端岗位是最接
  
334 
推荐文章 | Article 
近用户的，因为我们非常熟悉产品的形态。 
前端虽然不是数据的生产者，但却是数据的第一消费者。虽然我们不是交互设计专
家，但了解每个交互细节。借助大模型工具如 Cursor、Copilot，开发效率大大提升，过
去需要多人合作的工作现在可以由独立开发者完成。有了大模型，许多以前依赖算法的
工作变得更容易，而我们熟悉产品界面的实现，可以将这两者结合，完成过去无法做到
的任务。 
前端岗位将从代码实现者转变为智能体验的架构师或设计师，这些变化使得我们可
以将一些普适性和重复性的工作交给 AI 来完成，从而为我们创造更多时间去进行创新
和探索例如，我们团队一直在做低代码系统，菜鸟有一个叫“文生应用”的功能，它允
许用户通过自然语言描述所需的页面，AI 就可以自动生成该页面。这不仅仅是完成 UI 
的设计，AI 还会自动生成元数据和后端数据结构，整个页面可以直接上线并运行。这种
系统是前端工程师实现的，展示了过去无法想象的可能性。 
曹立成：我们发现 AI 确实能提高生产力，前端开发者的价值也正在改变，这也引
出了我们的下一个问题——前端为什么越来越全栈化？以前，我们可以做纯前端，切页
面、写组件、调接口，后端的事不管。但现在，有些公司招聘前端都要求懂一点后端，
比如 Node.js、数据库、微服务，甚至直接让前端变成全栈开发。大家怎么看，你们公
司对前端的要求有什么变化？ 
唐爽：菜鸟在过去几年一直在进行全栈化改革，主要将服务端开发人员培养为前端
开发者，同时也让部分前端开发者参与服务端的工作。现在，菜鸟没有严格区分前端和
后端岗位，而是更多地采用全栈开发模式，所有开发者都被称为“开发工程师”。目前，
约 80% 的页面都是由全栈开发人员完成。这些开发人员既负责前端也负责后端，开发
模式主要依赖低代码工具。通过低代码体系，原本从事 Java 开发的员工能够快速上手
前端工作，快速完成页面开发。这一模式在菜鸟的业务背景下非常有效，尤其是我们 
80% 的业务是面向 B 端的 PC 端，主要是各种管理系统。 
前端开发的挑战不在于技术复杂度，而在于与服务端的联调。过去，前端与后端的
协作往往会增加很多成本，特别是在页面内容和数据处理上。菜鸟希望通过全栈开发，
减少这些协同成本，让一个人能负责前后端的开发。菜鸟的全栈化改革并不是让开发者
做简单的 CRUD 页面，而是让他们能深入到业务逻辑的层面。前端开发者不仅要做页面
 
335 
InfoQ 架构师2025年第一季 
开发，也可以参与到业务需求和技术实现中，逐步转向更复杂的工作，提升职业发展的
深度。 
低代码体系使得前端开发者可以快速适应并参与更多业务工作。同时，一些更复杂
的页面，像大屏展示、双十一特殊页面、消费者端和互动游戏等，依然需要前端专家的
参与。这些领域需要更高的前端专业能力，也为前端开发者提供了更多挑战和机会。举
例来说，我们将传统的 100 多个下拉框整合为一个智能搜索框，提升了 60% 的工作效
率，这种创新是传统需求接收和手工开发无法实现的。 
全栈化改革的目标是减少中间的协同成本，让一个开发者能完成更多任务，同时将
更专业的人才放在更需要专业技能的岗位上，这推动了菜鸟整个技术体系的不断演进。
然而，这也带来一些挑战，比如是否会因为全栈化而忽视前端专业性。虽然全栈化可以
扩展开发者的技能广度，但前端的专业能力依然重要。全栈化并不意味着放弃专业技能，
而是将更多任务整合，让开发者在多个领域发挥作用。 
周全：全栈并不是一个新概念，每隔几年就会被讨论一次。我在 2014 年毕业时加
入豆瓣，当时我们称之为“产品开发工程师”，工程师不局限于某一领域，而是为整个
产品负责，哪里需要就去做什么。后来，随着前端技术的成熟，全栈的概念逐渐兴起。
如今，全栈或大前端的概念如何理解，取决于个人以及所在组织对 ROI（投资回报率）
的看法。 
对于组织而言，小厂通常希望员工具备全栈能力，能够应对各种问题。然而，大厂
则更倾向于专业化分工，不需要员工跨领域填补空缺，而是希望每个人在自己的领域成
为专家，发挥最大价值。这种专业化分工在过去一段时间内占据主导地位，大前端的概
念也因此被淡化。 
然而，随着效能的提升，全栈和大前端的概念再次被提及。首先，企业开始注重成
本控制，不再无限制地招聘人员。例如，字节跳动等公司已经开始“去肥增瘦”，优化
人员结构。其次，学习成本降低。借助 AI 工具和现有技术，开发人员可以快速切换到
其他领域。只要具备基本的研发思维和架构能力，换个语言或平台的成本已经大大降低。
AI 工具还能帮助开发人员快速识别和解决技术问题。第三，互联网经过多年的发展，效
能基建和分工已经相对成熟。一部分人继续走专家路线，专注于高精尖领域；另一部分
人则通过低代码等工具，专注于业务开发。例如，前端的低代码平台和后端的 FaaS
  
336 
推荐文章 | Article 
（函数即服务）让业务开发变得更加简单。开发人员只需关注业务逻辑，而不需要深入
了解底层技术细节。这种分工模式使得业务开发人员的认知复杂度降低，他们只需关注
最基本的逻辑即可完成开发任务。这种全栈岗位在成本和效能上达到了较好的平衡。 
对于个人而言，是否选择大前端路线取决于职业发展规划。有些同学适合专注于某
一领域，成为专家型开发人员，这部分人才在行业中依然稀缺。有些同学则横向能力较
强，借助现有工具，可以在全栈领域创造更大价值。 
曹立成：不仅仅是前端开发，所有工种的价值都在发生变化，现在正处于一个大融
合的阶段，前端的要求也在变化。淘天集团正在将前端和客户端合并，这与大前端的概
念相似，都是在拓宽边界，只是它拓宽的方向是前端和客户端，而不是与服务端的融合。
例如，当我们在 APP 里展示 H5 页面时，页面的加载速度和用户体验不仅仅是前端或
客户端的问题，而是需要跨工种合作来解决。解决这个问题的方式要么是团队内有融合
型的工种，要么是资深开发者同时精通客户端和前端，能独立解决。 
无论是业务团队还是技术团队，实际问题都是跨工种的。现在，团队更多是围绕问
题和解决方案进行调整，而不是单纯按工种来划分。AI 对前端领域的冲击，实际上对有
经验的资深开发者是一个利好。因为他们在某一领域已有深度，拓展其他领域的广度成
本较低，AI 可以帮助他们快速进入新领域解决问题。 
曹立成：底层技术创新如何影响上层变革？ 
周全：这是一个正向命题，底层技术的变革必然引发上层的调整。互联网领域常提
到的金字塔原理和第一性原理都表明，底层的变化会直接影响上层的开发模式、效率和
交付方式。更关键的问题是，如何让上层在底层技术变革时能够放心、低成本地适应，
并符合公司和团队的价值导向。过去，前端领域曾出现大量重复造轮子的现象，不同团
队使用不同的技术栈，最终经过试错才确定统一的技术选型。然而，如今团队没有足够
的时间、资金和人力去试错，必须快速找到适合的技术方向，这对决策者提出了更高的
要求。 
当前的技术环境也发生了变化。过去，后端技术更新较慢，追求稳定；移动端以年
为单位更新；而前端则以周为单位快速迭代。但现在，所有技术的迭代速度都放缓了。
无论是移动端还是开源社区，都缺乏令人眼前一亮的创新技术。因此，如何在现有环境
 
337 
InfoQ 架构师2025年第一季 
下通过底层技术变革推动业务价值，成为当前的核心问题。 
底层技术的变革需要满足三个条件：一是变革必然发生；二是控制变革成本；三是
找到创新的技术方向。如何通过正确的底层技术变革带来业务价值，是当前需要探讨的
重点。这与上一个话题类似，底层技术的选择同样需要评估 ROI，只有当收益足够大时，
决策者才会支持变革。 
以跨端技术为例，快手曾尝试过多种技术（如 Flutter、Uni APP 等），但踩过不少
坑。如今，面对鸿蒙系统的兴起，我们在选择跨端技术时非常谨慎。例如，KMP（Kotlin 
Multiplatform）在鸿蒙上的实践效果不错，但在公司内部仍面临质疑：这项技术能否真
正为公司带来价值？毕竟，过去的技术尝试并未完全成功。 
选择底层技术时，必须算清账：它是否能在短期内带来收益，并在长期内创造持久
价值？以跨端技术为例，鸿蒙系统要求开发三个 APP，而通过跨端技术，我们可以将开
发成本降低 66%。此外，KMP 允许复用现有安卓代码，进一步节省了人力成本。综合
考虑性能、稳定性和生态支持，KMP 成为我们的优选。技术的选择还需要评估其长期
成本，不能只追求短期收益，而忽视技术的可持续性。例如，Flutter 近期因团队稳定性
问题受到质疑，而 KMP 则因其稳定的生态支持和较低的维护成本脱颖而出。 
推动底层技术变革，首先需要评估技术的收益和成本，确保其能为团队带来实质价
值。其次，工程师需要具备专业能力，能够通过技术经验和行业趋势做出准确判断。最
后，技术的选择还需与现有生态互补，避免重复造轮子。 
唐爽：刚加入菜鸟时，许多业务系统是基于 PC 端的供应链和仓库管理系统，而竞
争对手已转向移动化。由于多年的积累，不同时期的系统体验不一致，亟需解决。我们
的解决方案是通过技术手段解决业务问题。我常强调三句话：通过建设某个平台或技术
手段，实现某种技术能力，最终帮助业务达成目标或解决问题，这三者相辅相成。 
当时菜鸟的业务主要基于 PC 端，已有上万个页面。如何快速适配移动端？重新开
发一套移动端系统工作量巨大。我们通过低代码体系和统一的设计模式、前端组件体系，
实现了一码多端：只需在低代码平台配置 PC 端页面，即可自动适配移动端。这不仅限
于响应式设计，而是通过运行时和编译时的转换，完全模拟移动端原生效果，实现“买
一送一”——开发一次 PC 端，自动生成移动端。这一底层技术让菜鸟的业务自动具备 
  
338 
推荐文章 | Article 
PC 和移动端适配能力，大幅减少开发成本。许多业务不再依赖 PC 端操作，员工可以
随时随地通过手机处理任务。即使某些操作仍需 PC 端，移动端也能在紧急情况下使用，
且开发成本几乎为零。此外，我们的低代码平台积累了上万个页面的描述信息。结合 AI 
技术，这些数据可以生成千人千面的内容。例如，根据仓库运营人员的岗位、工作习惯
和时间段，自动推荐相关数据。这是低代码与 AI 结合带来的未来探索方向。 
底层技术的变革不仅影响了业务，也推动了组织变革。由于大量页面通过低代码搭
建，设计师和产品经理的岗位逐渐融合。设计师从仅负责交互设计，扩展到参与产品链
路设计，拓宽了职业发展路径。前端开发人员通过低代码了解业务逻辑，从页面逻辑转
向业务逻辑，职业发展更加立体。 
曹立成：提到底层技术时，不一定指内核或汇编语言，而是一个相对概念。近年来，
像 DeepSeek、鸿蒙系统以及 Meta、苹果的 AR/AI 技术，都是底层技术的发展。这些
进步影响了上层产品的使用方式，带来了普惠性的变化，例如，微信正在逐步推出 AI 
搜索功能。这些变化表明，近年来底层技术的创新，尤其是 AI 领域的进步，正在迅速
推动上层产品和服务的转型，甚至影响了各行各业的工作方式与价值边界的拓展。 
前端开发者如何“转型” 
曹立成：现在的前端方向，甚至整个软件开发的主旋律，就是看哪家公司能把 AI 
用得更好，例如工程化方面，审查代码、分析项目结构、分析需求等等。想请各位结合
经验分享下，AI + 前端，开发流程是否被重塑了，日常工作中，AI 用得好与不好的关
键点在哪？以及 AI 的局限性又在哪？ 
唐爽：我们使用 VS Code 和 Composer 时，虽然大家使用的是相同的工具和模型，
但效果往往因人而异。比如，我们在使用 Composer 时，时间长了可能会遇到一些问题，
比如产生幻觉或者代码错误不断增加。那么如何解决这种问题呢？我的做法是：我会同
时管理多个 Composer，每个负责不同的任务。例如，一个 Composer 专注于组件开发，
另一个负责应用工程开发。专门做组件开发的人员只需要专注于组件本身，应用开发的
人员则非常熟悉业务逻辑和页面逻辑，清楚如何与服务端接口对接。而数据埋点开发人
员则专注于用户行为的埋点，熟悉整个埋点体系。通过这样的分工，可以确保每个人专
注于自己的职责，不会互相干扰。 
 
339 
InfoQ 架构师2025年第一季 
如果所有任务都交给一个人来做，他的工作量会非常大。而且，在团队中很难找到
一个全能的员工。所以，不要强求 AI 解决所有问题，它的上下文也有其局限性。当然，
如果连自己的业务逻辑都没有理清楚，就在一个大型工程中开发新功能，这时就不能强
求 AI 去解决问题了。在这种情况下，AI 只能为你提供一些启发。所以，首先要理清楚
自己的思路，制定一个大致的架构蓝图，然后再去利用 AI。 
周全：尽管业界有很多关于 AI 的讲座，吹捧其神奇效果，实际应用时我们却遇到
不少问题。例如，AI 的代码补全和错误排查功能效果较差，有时会给出错误的代码改动
建议，甚至在完美的代码中提出多个无关的修改问题，造成效果不理想。 
我们还尝试过将 AI 用于“全能”的场景设计，比如让 AI 帮忙创建团队、管理 
Git 仓库等，但这些尝试效果不佳，最终我们得出了一个结论：AI 的应用应该更加聚焦
和细化，按照场景来分配任务。比如，在进行代码审查时，AI 不应只是全面地列出问题，
而应该专注于某一类问题，比如查找 Java 代码中的 NPE（空指针异常）。 
AI 目前还不是神，它仍然像一个 3 到 4 岁智力的孩子，我们需要为其提供更加
细致的指导，以确保其能有效工作。在实际应用中，通过使用 AI 工具，我们可以自动
处理常见的编译错误和相关问题，减少人工干预。这大大提高了效率，节省了人力资源。
然而，我认为当前阶段 AI 的智力水平仍然有限，适合用于一些特定场景和细节问题，
能够有效节省人力，但不会带来革命性的突破。 
代码本身既是逻辑又具有艺术性，AI 目前难以一次性解决其中的复杂思维问题。因
此，我们不应对 AI 有过高的期待，而应将其应用于具体且可落地的场景中，以获取更
实际的收益。我们在未来的一到两年内应避免将 AI 过于理想化。 
曹立成：AI 确实在进步，但我也深刻感受到它的局限性。比如，我有时会写一些 
Gradle 脚本，Gradle 语言相对冷门，我尝试让 AI 帮我完成脚本开发，结果发现它生成
的代码看起来很漂亮，但实际上根本无法执行。经过反思，我认为这可能是因为我对某
些方法是否存在都不确定，而 AI 在这种情况下容易产生“幻觉”，生成一些看似合理
但实际上不存在的方法。 
后来我发现，当某个领域的资料或代码库比较丰富时，AI 的表现会好一些，因为它
本质上依赖于学习已有的数据。但如果资料稀缺，AI 就容易凭空创造，生成一些看似合
  
340 
推荐文章 | Article 
理但实际无效的代码。这种经历让我意识到，AI 目前还无法完全替代程序员。它虽然在
某些场景下能提供帮助，但在复杂或冷门的领域，仍然需要程序员的专业判断和调试能
力。 
曹立成：在 AI 辅助写代码、跨端开发等等趋势下，前端开发者需要如何转型？是
否还需要关心技术？ 
唐爽：首先，我们仍然需要继续关注技术。正如我刚才提到的，AI 的出现并没有
降低对人的要求，反而提高了要求。因此，对于刚入职的同学，我认为不需要过于担心，
关键还是要打好基本功。正如我们之前讨论的，很多项目的架构是需要在复杂的业务中
逐渐积累经验的。一个好的架构并不是设计出来的，而是在实践中逐步完善的。有些架
构可能是前人经验的积累，你可能没有亲自参与其中，但你仍然可以从中学习。 
第二，随着技术的发展，我们需要更好地理解如何提出有效的问题，这是一个至关
重要的能力。我们往往难以准确表达问题，尤其是在向 AI 提问时，如何提供适当的信
息和上下文是至关重要的。 
第三，AI 不会替代人类工作。虽然 AI 可以完成大量工作，甚至高达 80% 的工作
量，但那剩下的 20% 才是最为关键的部分，而这正是我们人类的优势所在。AI 的智能
水平虽然很高，但它无法取代人类的判断和情感价值。无论是从纯技术角度，还是如何
通过技术放大商业指标，AI 目前都无法完全替代。无论是否有 AI，行业的发展仍然在
继续。即使没有 AI，技术人员也不可能在同一个岗位上工作一辈子。而且有一个事实值
得承认：从前端领域晋升为 CTO 的案例相对较少，相比之下，后端或更懂业务的人晋
升为 CTO 的可能性较大。 
在 AI 时代，转型的机会依然存在。例如，我的团队中有些人从传统的代码实现者
转变为智能体验架构师；过去仅仅负责页面逻辑的人员，逐渐深入到业务领域，成为了
小型业务团队的领导者，并有可能在未来晋升为某一领域的 CTO。 
我认为，持续学习是非常重要的。在我刚开始从业时，技术主要集中在 PC 端，后
来又经历了移动互联网时代。每隔几年，技术发展都会带来一些困惑，前几年元宇宙的
兴起也让我们讨论是否应该涉足这一领域。随着 AI 的出现，这些技术变革依旧持续，
迫使我们不断学习和适应。AI 的出现为前端开发人员提供了更多的机会。利用好 AI，
 
341 
InfoQ 架构师2025年第一季 
不仅能提升产品体验，还能完成以往难以实现的任务，尤其是在小规模团队中，甚至是
初创公司，这种转变变得更加可行。 
周全：互联网时代永远不变的就是“永远在变”。我记得我上大学的时候，最常用
的手机是诺基亚。当时，全班几乎每个人都用诺基亚，但到了 2014 年，几乎所有人都
换成了安卓或 iOS 手机。这个变化是显而易见的。同样的，技术也在变化。当时我们
做前端开发时，最老的开发者逐渐被淘汰。而现在，我们回头看，PC 端的使用者越来
越少，大家都转向了移动端。比如当时使用淘宝时，我认为移动端非常难用，而更倾向
于使用网页端，因为网页端可以同时比较多个页面，但现在大家都习惯了移动端，这就
是变革。环境和用户的习惯在变化，技术也需要随之调整。从跨端开发到各种操作系统
的出现，例如鸿蒙系统、MIUI、ColorOS 等。作为开发者，我们无法忽视这些变化。技
术不断演进，我们需要随时适应新的开发场景。这是市场和技术环境的必然变化。 
现在，我已经超过 30 岁，写代码的能力比一些年轻的同学差了很多。然而，我依
然能够跟上变化，依靠 AI 工具来生成代码。因此，我认为技术环境的变化虽然会带来
挑战，但我们依然可以通过学习和适应，不断提升自己的能力。 
其次，“Talk is cheap, show me your code”，这句话意味着，我们不应仅仅依赖空谈
和理论。对于现在的年轻人来说，AI 的出现确实让很多人开始焦虑。是否意味着我该转
行，或者是不是应该多学一些 AI，甚至使用低代码或无代码工具来生成代码？我认为，
首先大家可以亲自尝试这些 AI 工具，这些工具的开源代码和文档都已经非常完善，大
家完全可以通过实践来理解和应用它们，看看它们是否能在实际场景中提升工作效率。 
很多人产生焦虑，可能是因为他们已经失去了亲自实践、动手操作的能力。AI 当前
非常火，但未来会是什么？如果我们看一些科幻电影，可能会看到更先进的技术出现。
新的技术不断涌现，可能会取代当前的技术。我们需要做的就是适应这些新技术，并利
用它们让自己变得更强。 
曹立成：关于 AI 是否能替代我们的问题，本质上是大家内心的焦虑所引发的。无
论是 AI 的冲击，还是裁员、行业发展、经济不景气等因素，都可能让我们感受到压力。
在这样的背景下，大家的焦虑是可以理解的。面对 AI 时，我常常通过提问来增强自信，
当发现 AI 无法解决我的问题时，我就觉得自己依然不可替代。 
  
342 
推荐文章 | Article 
以最近很火的《哪吒》电影为例，这部电影花费了五年的时间，才完成了每个镜头
的打磨。制作团队并没有很强的资金支持，但他们坚持自己的理念，最终得到了回报。
这个过程告诉我们，做事需要耐心和坚持。技术领域也是如此，真正做出优质框架或技
术方案的团队，能在实践中解决问题，最终会获得认可。 
从技术角度讲，认真做出来的技术方案能给用户带来真实的价值。当用户感受到你
技术方案的有效性时，你就成功地解决了他们的问题。这个过程需要我们踏实地走下去，
而 AI 的出现更多是为那些不太踏实的上层人员带来冲击，真正的技术人员会一步一个
脚印地走得更远。 
关于是否需要转型的问题，我认为这与个人的职业发展密切相关。作为前端开发者，
是否转型并没有标准答案，每个人的路径不同。然而，是否继续关注技术这一点，我认
为答案是肯定的。只要你在这条船上，无论你在船头还是船尾，只要船起航了，它就能
带着你前进。技术是这条船的动力源泉。很多人希望不被时代抛弃，虽然不一定要成为
时代的领袖，但至少不想被落下。因此，关心技术的变化是必要的，技术变化背后原因
的分析过程才是值得我们深入思考的，AI 目前还无法替代这种过程。 
 
343 
InfoQ 架构师2025年第一季 
InfoQ 2025 年趋势报告：软件架构和设计 
 
每年，InfoQ 的编辑都会与行业专家会聚在一起，讨论我们在软件架构与设计领域
观察到的最新发展趋势。我们借助 Geoffrey A. Moore 的《跨越鸿沟》模型对这些趋势
进行分类。基于我们集体的洞察与判断，那些被认定为创新者或早期采用者的趋势是 
InfoQ 编辑团队下一年的主要关注点。这些也是 InfoQ 读者应该关注的话题，因为它们
可能会为用户解决方案的灵感来源。除了本报告和趋势图外，附带的播客还包含了一场
小组讨论，深入探讨了一些趋势。 
由于软件架构决策总是充满了权衡，从来就不存在一种能够解决所有挑战的完美方
案。正因为如此，编辑们对于某个趋势何时应该沿着采用曲线移动，一直保持着一种积
极、理性且富有建设性的讨论。就像在设计系统时的那些“视情况而定”的选项一样，
作者 Thomas Betts、Sarah Wells 等 
译者 明知山  策划 Tina 
  
344 
推荐文章 | Article 
有时这更多的是依赖于主观判断，而非可量化的数据。 
确定技术或趋势所处位置的一个关键因素是其稳定性。通常，创新想法发展迅猛，
但因缺乏成熟度和稳定性，往往需要额外的投入才能取得成功。当一个趋势已经从早期
采用者阶段跨越到早期大众阶段时，这表明更多的公司应该能够采用它，或者至少可以
考虑它是否适用于他们的场景。 
 
从架构师角度看 AI 趋势 
在过去的一年里，与 AI 相关的话题在许多方面都在不断演变，未来仍将是整个行
业的创新重点。如今，大语言模型（LLM）已经足够普及，它们不仅跨越了鸿沟，还从
早期采用者阶段直接跳到了晚期大众阶段。LLM 已成为每一家公司声称自己在使用 AI 
时最常提及的部分。然而，这种普遍性也导致了关于 LLM 应用场景及是否为合适工具
的清晰度的缺失。颇具讽刺意味的是，这也揭示了另一种识别新技术何时跨越鸿沟的方
式——当它开始被用于不恰当的场景时。 
InfoQ AI、ML 和数据工程趋势报告对其中一些话题进行了更深入的探讨。在本报告
中，我们聚焦于架构和设计趋势，着重关注一些软件架构师需要了解的主要趋势。可以
将这些趋势视为上下文或组件图中的方框。虽然架构师不必亲自实现每一个组件和子系
统，但他们需要了解 AI 元素如何与系统的其他部分相关联。输入和输出是什么？如何
衡量性能、可扩展性、成本和其他跨功能需求？ 
 
345 
InfoQ 架构师2025年第一季 
读者可以关注 InfoQ 的 AI、ML & 数据工程话题，以便获取更多信息。 
Agentic AI - 创新者 
除了  LLM，软件架构师应该关注的  AI 创新领域还有  Agentic AI 和小语言模型
（SLM）。Agentic AI 之前叫作“AI 智能体”，其理念是设计能够自主完成任务的 AI 
模型。在某些情况下，多个智能体可以协作以获得更好的结果。在传统软件中，我们可
能会看到用于管理工作流的编排模式。Agentic AI 从让智能体执行特定任务开始，后续
可能发展成为一种监督式的方法，由 AI 来决定在业务流程中遵循哪些步骤。由于公司
对非确定性软件做出重要决策的信任度存在较大差距，因此这种方法仍处于创新者阶段。 
架构师在设计 Agentic 工作流时可以借鉴微服务的一些模式，让每个智能体都有明
确的边界和交互模式。这种设计能带来更好的结果，因为智能体之间的响应和执行任务
的质量可以被观察和调整。可维护性也得到了提升，随着新模型的出现，个体智能体模
块可以被升级或替换。为了确保智能体响应得当，需随着智能体或其行动的演变持续进
行测试。 
观看 Shruti Bhat 在 QCon San Francisco 的主题演讲：开拓未来：推进 AI 智能体的
基础设施以获取更多信息。 
小语言模型（SLM）– 创新者 
架构师正在将小语言模型（SLMs）视为利用 LLM 特性的一种选择，同时也在改进
它们的一些缺点。SLM 通常比 LLM 更专业化，因此能够在某些任务中超越 LLM 的表
现。这种专业化也意味着它们的训练过程更简便、成本更低，使得更多的公司能够创建
专业化的模型。它们较小的规模还带来了较低的运营成本、更小的碳足迹以及更多的部
署选项。与通过云托管的 API 访问 LLM 不同，SLM 可以部署在自托管的硬件上，或者
部署在边缘计算机上，从而消除了网络流量延迟并提升了数据安全性。 
关注 InfoQ 的大语言模型话题（大多数 SLM 内容与 LLM 相关）以获取更多信息。 
检索增强生成（RAG）– 早期采用者 
要从 LLM 获取更高质量的结果，最常用的技术是 RAG。尽管 RAG 已迅速成为一
  
346 
推荐文章 | Article 
种主流方法，但要实现有效应用仍需付出努力。软件架构师正在调整他们的系统，以便
提供更易于被 RAG 场景使用的数据。在未来，系统的设计可能会直接围绕着将数据应
用于 RAG 场景来进行。这与数据驱动架构的趋势相契合。 
关注 InfoQ 的检索增强生成话题以获取更多信息。 
AI 辅助开发 – 早期大众 
此前，InfoQ 一直把低代码/无代码作为架构趋势来跟踪，因为良好的 API 设计为
平民开发者提供了可扩展性。现在我们决定用 AI 辅助开发来取代这一趋势，因为它们
的用例有着显著的重叠。许多专业软件工程师正在使用 AI 工具来辅助编写代码，而平
民程序员对 AI 工具的使用则是架构师需要关注的一个领域。在设计促进低代码开发的
系统时，通常会格外小心以确保 API 的安全性。AI 编程助手进一步降低了入门门槛，
在某些情况下，它们会让原本未为低代码解决方案或非专业开发人员设计的 API 变得
更容易使用。尽管 AI 辅助开发已经广泛普及，但它的快速出现可能意味着架构师在设
计能够应对被编写的代码访问的系统时跟不上节奏。 
架构师和工程师还担心 AI 工具生成的代码质量，即使有专业的开发者参与也是如
此。因为输出取决于提示词的质量，架构师正在寻找提供良好提示词的方法，确保代码
和架构符合相关规范。目前，助力 AI 辅助开发的工具正在出现，但尚未达到其他编码
风格标准（如代码检查或 EditorConfig）的水平。 
绿色软件 – 创新者 
碳高效和碳感知软件仍然是值得关注的创新趋势。许多公司专注于降低云托管成本，
这可以作为减少能源消耗的有效手段。尽管能源消耗的减少带来了诸多益处，但目前对
于软件系统碳足迹削减的关注度却远远不够。这就需要架构师们进行更深层次的思考，
充分考量软件的运行地点与运行时间，从而更好地利用可再生能源。 
时间和地点是让软件变得更绿色的关键因素，因为数据中心的电力供应方式很大程
度上取决于这些因素。“追日”策略通常用于最大化太阳能的利用。然而，让服务器接
近满负荷运行通常比让它们处于空闲状态更节能，因此在需求较低且已有容量的情况下，
在晚上执行某些任务或许会更好。网络流量也是能源消耗的重要来源，因此应尽可能在
 
347 
InfoQ 架构师2025年第一季 
本地处理数据。这些例子展示了绿色软件的复杂性，也解释了为什么这一领域将会继续
出现创新。 
关注 InfoQ 的绿色软件和可持续计算话题，收听《构建绿色软件》作者的播客以获
取更多信息。 
隐私工程 – 创新者 
2024 年，隐私工程被纳入趋势图，以凸显那些将隐私视为主要功能而非仅因法规
或事件驱动而被动实施隐私策略的公司。在某种程度上，AI 的兴起让在前期考虑隐私问
题的理由变得愈发充分。在实现大语言模型之前，架构师会着重考量将通过网络传输哪
些数据、这些数据是否用于训练模型以及对数据的使用是否符合先前批准的使用条款。 
关注 InfoQ 的隐私话题以获取更多信息。 
社会技术架构 – 早期采用者 
架构师角色的演变以及架构实践方式始终是一个备受关注的话题。复杂的软件系统
需要围绕负责构建、支持和演化软件的人员进行设计。“左移”是这一趋势的一种体现，
软件生命周期各个阶段的问题都在流程的更早期得到解决。这不应被误认为是回归瀑布
式项目管理和前期大规模设计。相反，这是架构师更早地与利益相关者讨论关注点，并
设计出能够演进且避免为满足所有人需求而进行最后一刻返工的系统。 
创新的架构师致力于在决策过程中减少瓶颈。他们不应是唯一的决策者，他们也需
要帮助其他团队成员做出架构决策。通过提供建议和引导决策过程，团队能更快行动，
做出更贴合实际的决策，并对设计有更深入的理解和信心。 
团队构建和发布软件的方式会影响他们能够构建的东西。因此，平台架构是架构师
需要考虑的。工程平台已经从定制解决方案转变为类似“平台即产品”的商品。在做构
建还是购买的决策时，要从社会技术视角出发，让架构师和其他利益相关者考虑这些方
案如何有利于软件的编写、部署和维护人员。 
 
  
348 
推荐文章 | Article 
原文链接 
• https://www.infoq.com/articles/architecture-trends-2025/ 
 
349 
InfoQ 架构师2025年第一季 
AI 将如何颠覆传统软件开发团队 
 
软件行业正在经历自云计算以来最重大的变革。AI 正在从根本上改变我们构建、运
营和与软件交互的方式。作为一名长期观察并撰写了关于行业重大变革的作者，我见证
了从面向服务架构（SOA）到微服务 的转变，再到容器化和无服务器架构的演变。现在， 
AI 正在推动一场更深刻的变化。这不仅仅是关于自动化编码任务或在应用程序中添加聊
天机器人，我们正在见证新的开发范式、运营实践和用户交互模型的出现，这些将重塑
团队的结构和软件的消费方式。 
本文将探讨五个已经对软件团队产生影响并将在未来几年内变得越来越有影响力的
趋势。对于每个趋势，我们将探讨其变化所在、现实世界的例子，并讨论不同角色——
从开发者到架构师，再到产品经理——如何适应并在这个新的环境中茁壮成长。让我们
作者 Bilgin Ibryam 
译者 明知山  策划 丁晓昀 
  
350 
推荐文章 | Article 
从最根本的变化开始：我们编写代码的方式。 
生成式软件开发 
软件开发已经经历了从劳动密集型的打孔编程到多层抽象编程的演变。 
这段旅程从需要深厚技术专长的汇编语言开始，随后发展到系统级语言如 C 和 
C++，再到具有托管运行时的 Java 和 JavaScript，再进一步发展到高级脚本语言，如 
Python——每一步都使开发变得更加容易，同时以牺牲对底层的控制为代价。AI 原生开
发（已知还有多种名称）代表了这一演变的最新阶段。 
生成式 AI （GenAI）和大语言模型（LLM）正在减少对手动编码的需求。开发者不
再需要逐行编写代码，而是指导 AI 系统执行代码编辑、生成应用程序框架，甚至创建
完整的软件组件。 
在某些领域和受控的环境中，例如 Web 应用程序， AI 甚至能够通过自然语言指
令（文本或语音）和图像来创建和运行全栈应用程序。这不仅延续了软件开发向更容易、
更抽象的方向发展的历史趋势，而且正在改变传统的开发流程。 
 
351 
InfoQ 架构师2025年第一季 
 
图 1：AI 编码辅助工具一览（来源：GenerativeProgrammer.com） 
当前的 AI 辅助开发工具正朝着两个方向演变： 
• AI 增强型 IDE 和代码助手：如 GitHub Copilot、Cursor 和 Windsurf，通过提供
智能代码补全和生成来增强传统开发流程。这些助手会分析项目上下文、依赖关
系和模式，提供相关代码片段建议并补全函数——所有这些都在开发者熟悉的环
境中完成。还有一些工具可以帮助进行代码评审和现代化遗留应用程序。所有这
些工具都提供了一条低风险、增量式的路径，让开发者在保持现有编码实践和流
程的同时将 AI 整合到他们的工作流中。 
• 自主编码智能体：如 Devin、Bolt、v0、Replit 和 Lovable，它们超越了仅提供建
议的范畴。它们在受控环境和受限领域（例如 UI 和 JavaScript）中运行，解释
高层次需求，提出架构，生成整个应用程序，甚至可以部署和运行。这些平台将
软件构建扩展到了开发者之外，让非传统开发者和半技术用户能够进行“氛围编
码”——通过自然语言进行原型设计，设计草图，并进行迭代，直至它们变得可
  
352 
推荐文章 | Article 
用。然而，生成式软件开发仍处于早期阶段，难以可靠地复制，并且尚未很好地
与现有的迭代和增量软件工程实践相结合。尽管像验收测试和行为规范这样的概
念在提高一致性方面显示出潜力，但该领域仍在发展，许多问题仍有待解答。 
AI 正在改变代码的编写方式，开发者必须适应这一变化。那些能够从“专家代码打
字员”转变为 AI 合作者的开发者，通过提供清晰的上下文、将需求细化为提示词并引
导 AI 提供期望的结果，将能够节省很多时间并专注于更高价值的任务。尽管 AI 可以
生成代码，但它仍然缺乏对可扩展性、安全性、风险分析的判断能力，特别是在特定的
业务背景下。生成式软件开发仍处于起步阶段，通常不太可靠，也难以整合到现有的流
程中。最有价值的工程师将是那些能够理解架构、系统设计、完整软件技术栈、端到端
软件开发生命周期（SDLC）、业务优先级和非功能性需求（NFR）的人。他们还会进行
各种权衡，并确保 AI 生成的代码与这些考量因素保持一致。 
为了为未来做好准备，开发者需要加深对 AI 的理解，掌握提示词工程（了解 AI 
擅长的领域和盲点在哪里），并学习新的工具和实践。工程师必须通过专注于系统设计、
架构、领域专业知识和批判性思维技能来适应变化。AI 工具可以自动化某些编码任务，
但理解复杂系统、确保安全性和将业务需求转化为技术解决方案的能力仍然是人类独有
的，对于维持职业的持久性来说至关重要。软件工程的未来在于那些能够将人类解决问
题的能力与 AI 功能相结合的人，他们能够提供更快更好的解决方案，而不仅仅是生成
更多的代码。 
AI 驱动的运维 
现代分布式系统的规模和复杂性已经超出了人类对传统监控、故障排除、安全和运
营的能力。随着 AI 辅助代码生成加快开发速度，未来应用程序的规模和复杂性只会随
之增加。传统可观察性方法——手动检查日志、基于阈值的警报和静态仪表盘——正变
得越来越不起作用。监控和维护 AI 生成应用程序的唯一可行途径将是使用 AI 驱动的
工具，这些工具能够实现与可观察性数据的自然语言交互、预测性问题检测与模拟、自
动化根本原因分析，以及在需要最少监督的情况下进行总结和补救。 
主要的可观察性供应商，如 New Relic、Splunk 和 DataDog，已经将 AI 整合到他
们的应用性能监控（APM）工具中。这些增强功能使得从海量遥测数据中提取可操作的
 
353 
InfoQ 架构师2025年第一季 
见解成为可能，减轻了认知负担并加快了事件解决速度。传统机器学习和 GenAI 在现
代可观察性和 安全性 领域的常见应用包括： 
• 预测性分析：这种方法通过分析过去的攻击数据来发现复杂模式并识别潜在威胁。
AI 可以使用真实和合成数据集模拟攻击场景。 
• 行为分析：与预测性分析（检查历史趋势）不同，行为分析侧重于实时用户活动。
AI 可以检测可能表明凭证被泄露或存在内部威胁的模式，而传统的安全工具通
常会忽略这些模式。 
• 异常检测：AI 持续监控网络流量、系统日志和 API 交互数据，以便发现与既定
规范的意外偏差。AI 通过生成合成异常、对检测模型进行压力测试和加强针对
零日攻击和新兴威胁模式的防御来增强这一过程。 
• 根本原因分析：传统根本原因分析通常涉及筛选海量日志、关联指标、阅读非结
构化文档和手动识别模式——这是一个缓慢且容易出错的过程。AI 驱动的平台
（如 Resolve.ai）通过聚合整个操作堆栈的数据——从基础设施指标和应用追踪
到部署历史和文档——来自动化这一过程。 
 
自动化根本原因分析（示例：Resolve.ai） 
对于运维团队而言，AI 将可观察性从认知密集型的信号匹配转变为自动化、可操作
的洞见。AI 可以处理来自维基和聊天对话的非结构化数据，将遥测数据与代码变更联系
起来，生成动态的事件仪表盘，并提出具体的解决方案，包括逐步的说明。例如，如果
某个服务出现延迟峰值，AI 可以立即将这些峰值与最近的部署、基础设施变更以及过去
类似的事件相关联。此外，AI 能够确定根本原因，并在一个自动生成的仪表盘上展示调
查结果，同时在公司的 Slack 频道中请求恢复确认。这种程度的自动化减少了平均解决
  
354 
推荐文章 | Article 
时间（MTTR），将运维从被动式的救火转变为主动式的问题预防。最重要的是，它捕
获了记忆，将每个事件变成未来可参考的教训。 
要在这个新的环境中生存下来并蓬勃发展，运维团队必须积累使用 AI 驱动的运维
工具的专业知识，从编写长查询、解析日志和手动编写自动化脚本转变为设计全面的可
观察性策略，引导 AI 系统做出我们所期望的行为。尽管 AI 可以处理大量的运维数据
并提出解决方案，但运维人员需要理解系统架构、业务背景和影响分析策略，以便评估
这些建议并做出明智的决策。 
上下文感知的交互式文档 
对于软件的采用来说，好的软件文档一直是至关重要的，无论是开源项目还是商业 
SaaS 产品。软件 文档 包括：面向初学者的教程、针对特定任务的指南、提供详细信
息的参考指南以及用于帮助人们进行深入理解的解释性内容。尽管这种结构仍然还有价
值，但随着软件的演变速度越来越快，保持文档的准确性和相关性变得越来越具有挑战
性。 
基础 AI 模型的一个主要限制在于知识的陈旧和过时。但随着检索增强生成（RAG）
的兴起，LLM 可以通过直接从代码库、API 规范和文档存储库中提取数据来提供实时且
最新的响应。凭借这种能力，AI 正在改变文档的编写方式和开发者与文档的互动方式。
CrewAI 的“与文档聊天”功能让开发者不再需要手动搜索大量文档或 StackOverFlow 
网页，而是使用 AI 驱动的聊天界面来获取相关答案。在新的软件项目中，开发者越来
越多地利用 LLM 的实时代码生成和执行能力，通过编码来了解项目。文档领域的最新
发展包括： 
• 文档创建：许多 工具 通过分析源代码、API 和开发者讨论建议内容来简化文档
编写工作。AI 可以生成结构化文档、代码片段和常见问题解答，从而减轻技术
作者的手动工作量。 
• 通过嵌入式聊天阅读文档：一些工具，如 Kapa.ai 和 Inkeep 直接集成到文档门
户、产品界面甚至是营销网站中，让开发者可以用对话的方式查询文档。还有一
些工具，如 DevDocs，通过模型上下文协议（MCP）将交互式文档访问集成到命
令行界面（CLI）和集成开发环境（IDE）中。这些 AI 驱动的文档工具通过提供
 
355 
InfoQ 架构师2025年第一季 
即时、相关的响应来减少支持工作量，改善了开发者体验。 
• 自动化的知识捕获与支持整合：一些工具，如 Pylon，通过引入 Copilot 来分析
开发者问题、支持工单和事件报告，动态地丰富了文档。它们不再依赖于预先定
义的手册，而是根据与实际用户的互动来创建基于现实世界的常见问题解答、最
佳实践和故障排除指南。 
这些 AI 驱动的工具不仅能搜索文档，当被集成到用户流程中时，它们还能够理解
产品上下文，读取错误堆栈跟踪信息，从多个来源编译相关信息，并以与用户专业知识
水平相匹配的对话格式提供答案。 
对于技术写手和文档团队来说，工具正在发生翻天覆地的变化。如果你还在手动编
写和更新文档，而没有利用 AI ，你可能会很快被自动化工具所取代。只是编写传统的
文档或复制粘贴 AI 生成的内容已经不够了，成功的关键在于要将 AI 作为生成和消费
文档的力量倍增器。专注于更高价值的活动，例如：捕获动态内容（如用户问题）、积
累最佳实践、记录事件教训、分析文档使用模式、识别缺失的知识，并在正确的时间和
场合提供这些信息。文档的未来不再是静态文本，而是对话式的、上下文感知的，并深
度集成到用户的工作流程和工具中。那些能够适应这种变化的人将成为不可或缺的人，
而那些无法适应的人将难以跟上时代的步伐。 
上下文感知的“AI 助手即 SaaS”接口 
无服务器架构和许多以开发者为中心的 SaaS 的原始承诺是引人注目的：让开发者
专注于业务逻辑，而平台处理基础设施配置、扩展、安全性和可观察性。虽然这一理论
在理论上很好，但无服务器复杂性的现实带来了新的挑战。开发者不得不应对大量的服
务、API 和配置。文档负担呈指数级增长。跟上最佳实践成为了一份全职工作。随着无
服务器服务变得更加强大和细粒度，所需的配置量也越来越多，使得开发者难以保持生
产力。 
AI 将通过在 SaaS 产品中集成上下文感知助手来提升用户体验。开发者不再需要在
文档中搜索、安装定制的命令行界面，或者通过 curl 来理解 API 调用。相反， AI 驱
动的界面将提供实时、上下文感知的指导。更重要的是，这些界面能够根据自然语言指
令执行操作，自动化常规任务。随着 MCP 等新兴标准的出现， AI 解读用户上下文并
  
356 
推荐文章 | Article 
在外部资源上执行操作的能力正在迅速增强。不久的将来，用户不仅能获得分步指导，
还能在聊天界面中直接完成任务，将 AI 从被动助手转变为主动的问题解决者。 
 
SaaS AI 助手模型：嵌入式、扩展式和外部式 
现在有多种方式可以集成 AI 助手： 
• 深度集成到 SaaS 中的上下文感知 AI 助手 
• 作为服务扩展（通常是入口点或服务的部分能力）的 AI 助手 
• 完全独立的第三方 / 外部 AI 助手服务 
 
357 
InfoQ 架构师2025年第一季 
以 Supabase AI Assistant 为例，这是 Supabase UI 中深度集成的 AI 助手。它不只
是一个文档聊天机器人或搜索工具，还是一个上下文感知助手，它能够理解产品领域知
识（Supabase）、用户当前状态（拥有哪些服务和访问权限），并直接与平台的 API 交
互。例如，当开发者在查询数据库遇到困难时，这些助手不仅可以向开发者解释相关概
念，还可以生成正确的查询，分析潜在的性能影响，甚至在开发者请求时直接执行查询。
这些助手结合了实时辅助和采取行动的能力，在促进用户更好地使用平台方面发挥了很
大的作用。 
另一个例子是 Vercel 的 v0.dev，它独立于 Vercel 运营，旨在吸引想要创建网站并
最终在 Vercel 或其他平台托管的新用户。通过独立托管，这项服务不会将 Vercel 的所
有功能和复杂性暴露给可能只想要创建简单网站的非技术用户，但会逐渐引导他们成为 
Vercel 的用户。尽管是独立运营，这些 AI 入口点最终将更紧密地集成到主要 SaaS 平
台中，实现用户在 AI 功能与传统 SaaS 功能之间的无缝切换。 
在最后一个类别中，我们可以看到像 Lovable.dev、Bolt.new、Replit 这样的 AI 原
生 SaaS 服务。这些服务正在探索新的应用场景，作为传统 SaaS 的第三方前端，吸引
非技术用户和半技术用户。例如，Lovable 能够与 Supabase 这样的目标部署平台实现
无缝集成。同样，Bolt 也与 Netlify 和 Github 等平台有着类似的集成。 
这一转变将对所有 SaaS 产品产生影响。自然语言正逐渐成为用户交互的必备界面，
尤其是在复杂技术产品的入门阶段。它将成为产品主导增长（PLG）的新动力，让新用
户和不太技术的用户快速上手，直观地探索功能，并更快地实现价值。但在前进的道路
上，不仅仅是添加一个聊天机器人那么简单，还需要重新思考如何以 AI 增强的方式为
用户提供最有价值的东西。如果你是一家数据存储供应商，这可能意味着通过提示词而
非始终依赖 SQL 客户端来创建架构、查询数据和生成测试数据。如果你提供的是一个
可观测性平台，这可能意味着可以通过一个提示词来检查日志和分析使用模式，等等。
现有的 SaaS 供应商如果没有积极计划整合 AI 助手，可能会被具有更高效用户体验的 
AI 原生初创公司所颠覆。 
如果你是一家 SaaS 公司的产品体验负责人，你必须保持领先地位： 
• 亲自使用 AI ——尝试使用 AI Copilot 和助手，深入了解它们的功能。 
• 在公司内部启动一个 AI 项目——培训团队成员，寻找机会。 
  
358 
推荐文章 | Article 
• 识别产品使用中出现的问题，并利用自然语言界面（聊天）解决这些问题。 
• 寻找真正的价值——不只是添加一个聊天界面那么简单，还需要确定 AI 将如何
提升你的价值主张。 
• AI 是一个新的能力推动器。探索 AI 如何为你的产品开辟全新的应用场景或用
户群体。 
智能体系统的兴起 
组织开始越来越多地采用 自主 AI 智能体，这些智能体可以协调、规划和执行复杂
业务任务，几乎不需要人工干预。AutoGPT、AutoGen、Dapr Agents 和 LangGraph 等项
目是构建智能体的流行框架的早期代表，而更完整的软件技术栈正在迅速增长。这些智
能体系统不再是由孤立的 AI 模型执行单一任务，而是正在演变为 AI 服务网络。这些
网络需要具备分布式系统功能，包括工作流编排、异步消息传递、状态管理、可靠性、
安全性和可观察性，这些功能远远超出了简单的 API 集成。 
这一转变将以类似互联网、微服务、云和无服务器架构影响组织的方式影响每一种
技术角色： 
• 开发者必须掌握 智能体设计模式、与 LLM 的对话式 API 和智能体编排技术，
以便能够高效地连接和协调智能体。 
• 架构师需要设计出生产就绪且具备成本效益的 AI 解决方案，将智能体系统与现
有的云和 SaaS 平台集成起来。 
• 运维团队必须部署新的 LLM 监控、可观察性和追踪 工具，用于 LLM 驱动的应
用程序，因为这些应用程序的行为与传统软件不同。此外，这些新的工作负载和
工具需要与现有的工具和运维实践集成。例如，我介绍了 Dapr 项目如何将它的 
Conversation API 与现有的可观察性和安全工具集成。 
• 平台工程师需要创建更好的框架，以便更容易地开发、部署和管理 AI 智能体。 
• 产品经理必须了解 评估技术，以便精准衡量 AI 接口的行为表现与有效性，尤
其是在以提示词和响应为核心交互方式的场景中。 
好消息是，目前有不断增长的开源工具和无尽的免费学习资源可供那些愿意深入研
究的人使用。在这个快速发展的领域，组织有两个选择：提升团队在智能体系统开发方
 
359 
InfoQ 架构师2025年第一季 
面的技能，或者招聘已经具备必要专业知识的人才。AI 驱动的智能体系统并非一种短暂
趋势，而是软件自动化的下一个重大演变。 
AI 行动计划 
AI 的快速发展要求我们采取一种有意识和有计划的方法来构建强大的 LLM 基础知
识基础，了解它们的工作原理、能力以及局限性。掌握提示词工程的基础知识，并熟悉
那些可能长期存在的成熟工具。这些知识将使你能够与同事就 AI 进行富有成效的讨论，
并为跟踪其发展动态以及寻找相关机会打下坚实的基础。 
你的下一步行动应与你在软件开发领域中所扮演的角色保持一致： 
• 对于开发者来说，使用 Cursor 和 GitHub Copilot 等编码助手是基本要求。使用 
CodeRabbit 等工具进行自动化代码评审也是一个容易实现的目标。将这些工具
集成到日常工作中，找到它们适用的低风险场景。如果你的雇主不允许使用这些
工具，可以在开源项目或个人副项目中进行实践，并与同事分享它们的优缺点。 
• 运维团队应该探索如何让 AI 自动化更多任务，减少人工干预。然后为运维 AI 
工作负载做好准备，无论是仅涉及对外部 LLM 的少量调用，还是运行完整的智
能体系统。 
• 架构师应该专注于了解端到端的 LLM 驱动架构以及如何将智能体系统融入企业
环境。这不仅涉及单个 AI 组件，还包括如何设计可靠、安全 的系统，同时利
用 AI 能力，保持企业级质量。重点应放在识别组织内的战略机会上，无论是利
用 AI 的能力来现代化遗留应用程序，还是从头开始设计新的原生 AI 系统。 
• 技术写手需要将 AI 工具作为新的文字编辑器，尝试各种工具、模型和提示词，
专注于自动化写作流程。未来的内容将是对话式的。 
• 产品经理需要关注 AI 发展趋势及其对产品战略的潜在影响，研究 AI 原生产品，
了解自然语言界面和 AI 辅助功能如何提升用户体验。 
设计、运维和我们过去所熟知的编程方式将继续发生演变，但掌握这些基础技能能
够让你为这些演变做好准备，以应对接下来发生的一切。现在就开始吧，因为这一趋势
将在未来十年内持续存在。 
  
360 
推荐文章 | Article 
英文原文 
• https://www.infoq.com/articles/ai-trends-disrupting-software-teams/ 
 
361 
InfoQ 架构师2025年第一季 
API 网关十五年演进：从微服务核心到 AI 时
代的神经网络 
 
2014 年，伴随全球科技公司涌现的“微服务化”浪潮，API 网关作为系统拆分的
“守门人”（Facade Pattern）应运而生。它从最初简单的请求路由，逐步发展到处理身
份验证、授权和速率限制等关键任务，如今更成为实时数据与 AI 模型交互的核心枢纽。
这一演变历程，映照出传统技术架构从服务拆分走向大模型时代的深层逻辑。 
我们所认知的 AI 和大模型，实际上背后都是由无数的 API 所驱动的。正如当 
Kong 公司中国区负责人戴冠兰在采访中所说：“No AI without API”。大模型每一次智
能应答、图像生成乃至决策推理，本质上都是无数 API 的精密协作，大模型技术的快
速发展将带来 API 的指数级增长。 
作者 Tina 
  
362 
推荐文章 | Article 
在 InfoQ 的专访中，戴冠兰回顾了 Kong 网关从 2015 年开源项目到如今演变为 
API 和 AI 混合网关的历程。现在，通过 Kong AI 网关(AI Gateway)，我们能够屏蔽不同
大语言模型（LLM）提供商之间的差异，为用户/客户端提供统一的 API 接口。此外，
由于所有的请求和响应都经过 Kong AI 网关，这使得在其中实现可观测性、认证授权以
及重试等治理能力变得非常便捷。未来，Kong AI 网关将持续集成与 AI 相关的核心能
力，全面支持包括 MCP 协议、Google A2A 标准在内的新兴技术体系。 
嘉宾简介 
戴冠兰，现任 Kong 中国区总裁，并兼任全球网关核心研发总监，全面负责 Kong 
Gateway、服务网格等核心产品的研发战略与全球技术布局。在加入 Kong 之前，戴冠
兰曾在 Cloudflare 担任边缘计算、Web 应用防火墙（WAF）和内容分发网络（CDN）等
关键技术的负责人，领导团队处理每日超过万亿级别的请求，积累了丰富的大规模分布
式系统的架构与运营经验。 
他拥有美国东北大学计算机科学硕士学位，具备坚实的技术背景和国际化视野。清
华大学五道口金融学院、纪源资本创投研修班 OMEGA 项目的优秀学员。 
2021 年主导成立 Kong 亚太区研发中心，从 0 到 1 搭建团队，覆盖多条产品线，
如 API 网关、服务网格等。中国区研发团队贡献了超过半数的核心研发成果，显著提
升了  Kong 应对高并发、复杂流量场景的技术能力。推动  Kong 从单一  API 网关
（Kong Gateway）扩展至全生命周期微服务管理平台，形成“全家桶”式产品生态（如 
Kong Mesh、Konnect 云服务等），支持企业在多云环境下的高效 API 管理。他在开源
商业化、全球化产品战略、AI 网关演进等方面具有深刻见解，致力于将 Kong 打造成
全球领先的 API 管理平台。 
从微服务到大模型时代网关的演进 
InfoQ：网关技术产生于微服务时代，那么它本身是如何随着时间的推移而发展的？ 
戴冠兰：API 网关技术经历了从支持微服务架构到适应 AI 应用的演进。以下是 
Kong 在不同阶段的关键发展： 
1. 微服务时代的起点（2015 年）：随着企业从单体架构转向微服务，Kong 于 
 
363 
InfoQ 架构师2025年第一季 
2015 年推出了开源 API 网关，提供统一的流量入口、身份验证、限流和监控等
功能，帮助企业顺利过渡到微服务架构。 
2. 服务网格与 Serverless 的融合（2019 年）：微服务数量的增加带来了服务间
通信的复杂性。为此，Kong 推出了 Kuma 服务网格，并在企业级产品中集成为 
Kong Mesh，增强了服务间的通信管理和安全性。同时，Kong 还支持 Serverless 
架构，适应了无服务器计算的需求。 
3. 云原生控制平台的构建（2020 年）：为了统一管理南北向和东西向流量，Kong 
推出了 Kong Konnect 云原生控制平台，提供统一的可视化管理、策略控制和数
据洞察，提升了微服务治理的效率。 
4. AI 网关的引入（2023 年）：面对生成式 AI 的兴起，Kong 于 2023 年推出了 
AI 网关，支持多种大语言模型（LLMs）的集成，提供无代码插件、语义缓存、
提示词防火墙等功能，帮助企业安全、高效地部署 AI 应用。 
5. 持续的创新与发展（2024 年）：Kong 在 2024 年发布了 AI 网关 v3.8 版本，
进一步增强了智能语义能力，引入了语义缓存、语义路由和语义提示词防护等功
能，提升了 AI 应用的性能、安全性和用户体验。 
 
InfoQ：当网关用于基于 AI 的交互时，它与我们传统的微服务程序交互有什么不
同？ 
戴冠兰：当 API 网关用于 AI 驱动的交互场景时，其与传统微服务架构下的差异体
现在计量方式、数据流处理和性能要求等方面： 
  
364 
推荐文章 | Article 
1. 计量单位的转变：在传统微服务架构中，API 网关通常按请求次数进行计费和监
控。然而，在 AI 应用中，尤其是涉及大型语言模型（如 OpenAI 的 GPT 系列）
的场景中，计费和资源消耗的关键指标转向了“token”（标记）。每个请求的
成本和资源使用量取决于输入和输出的 token 数量，而非仅仅是请求的次数。
这种转变要求网关具备对 token 使用量的精确统计和控制能力，以实现成本管
理和资源优化。 
2. 数据流处理方式的变化：AI 应用，特别是生成式模型的交互，常常采用流式
（streaming）响应方式，以提升用户体验。这意味着网关需要支持实时的数据流
处理，能够在接收到部分响应数据时立即转发给客户端，而不是等待完整响应生
成后再进行转发。这种处理方式对网关的并发处理能力和数据传输效率提出了更
高要求。 
3. 性能和稳定性的挑战：AI 模型的推理过程通常计算密集，响应时间可能较长，
且对系统资源的消耗较大。网关在处理此类请求时，必须具备高并发处理能力和
稳定性，以防止因单个请求的延迟或失败影响整体系统的性能。此外，网关还需
具备智能的流量控制和异常检测机制，以应对可能的请求激增或异常行为。 
4. 安全性和合规性的增强需求：AI 应用可能涉及敏感数据的处理和传输，网关需
要提供更强的安全控制措施，如细粒度的访问控制、数据加密、敏感信息过滤等。
同时，为满足不同行业的合规要求，网关应支持多种认证和审计机制，确保数据
处理过程的可追溯性和合规性。 
总的来说，AI 驱动的应用对 API 网关提出了新的挑战和要求，涉及计费方式的转
变、数据流处理的复杂性、系统性能的提升以及安全合规性的加强。为应对这些挑战，
API 网关需要不断演进，集成更多智能化和自动化的功能，以适应 AI 时代的需求。 
AI 网关：大模型时代的工程化中枢 
InfoQ: Kong 在 AI/LLM 网关场景中通常会遇到哪些客户需求？ 
戴冠兰：最为常见的需求是多 LLM provider 代理，当企业计划将 LLM 应用上线到
生产环境时，一定是需要有 backup 的，这时候，很自然的就是当某一个 LLM provider 
不可用的时候，能否在不做任何调整的时候，通过 Kong AI 网关继续将请求自动的转移
到其他可用的 LLM provider。 
 
365 
InfoQ 架构师2025年第一季 
其次就是基于 Token 的 ratelimit，这对于企业 LLM 应用控制成本等方面也是极其
重要的。 
当然除了这些，还有一些其他的，比如能否进行自动的 RAG，能否在 Kong AI 网关
的层面进行 cache 以节约成本，以及是否可以通过 Kong AI 网关进行一些特定内容信
息的过滤等。 
InfoQ: 有很多工程师对 LLM 时代的网关会有疑问，比如“我为什么需要一个网
关？我直接调用 API 就好了。”你如何解答这个问题？ 
戴冠兰：正如前面提到的，通过 Kong AI 网关主要完成的是对于 LLM 请求的治理，
这其中不只是包括对多个 LLM provider 的 fallback，也包括认证授权等能力。很多 LLM 
provider 其实会限制用户可创建的认证密钥的数量，而大多数情况下，我们希望可以知
道我的 token 到底消耗在了哪里，是哪个应用发起的，在什么时间，消耗了多少。 
通过 Kong AI 网关就很容易做到这些了，将 LLM provider 的认证密钥进行中心化管
理，然后为不同的用户/应用创建各自的独立的认证密钥，这样可以避免密钥泄露的风
险。借助于 Kong AI 网关强大的可观测能力，还可以了解到这些用户/应用的 Token 消
耗情况等，这样就会很方便。 
此外，集成 Kong AI 网关的另一大优势就是不需要自己额外开发重复的逻辑，将这
些基础能力都下沉到 Kong AI 网关中，开发者只需要专注于自己的业务逻辑即可。 
InfoQ: 对于使用多个 LLM 提供商的企业，会有哪些技术挑战，Kong 是如何解
决这些挑战的？ 
戴冠兰：首先是如何保障始终有可用的 LLM provider。Kong AI 网关通过实现多种
重试和负载均衡策略，在保障可用性的同时，兼顾了企业复杂场景下的多样化需求，确
保始终有可用的 LLM provider 提供服务。 
其次，不同的 LLM provider 或者是不同的 LLM 尽管大体遵循统一的 API 接口格
式，但也会有一些差异存在，通过使用 Kong AI 网关，我们在网关侧屏蔽了这些差异，
用户可以使用无差别的统一接口进行连接，身份认证，以及通过 Kong AI 网关完成可观
测性相关的需求。 
  
366 
推荐文章 | Article 
InfoQ: 典型的 LLM 网关应该具备哪些关键功能？ 这些功能在企业的 AI 工程中
能带来哪些具体优势？ 
戴冠兰：既然叫作 AI 网关， 最典型的能力自然是连接不同的 LLM provider，其次
也包含对密钥的中心化管理，分发给用户不同的虚拟密钥，以便实施不同的控制策略。 
这样的功能，可以避免密钥的泄露，同时又可以根据企业的不同策略，来进行精准
的控制。 
还有，比如说基于 Prompt 对内容进行过滤的功能。对于企业而言，避免敏感数据
泄露是很关键的，当然，另一方面是要避免企业 AI 应用返回“有害信息”，通过在 AI 
网关上进行基于 Prompt 的内容过滤，就可以有效的保护数据安全，也可以避免影响企
业形象或者避免一些安全合规问题。 
 
InfoQ: 另外，网关是否会引入额外的延迟？如何保证网关既可靠又非常快速？ 
戴冠兰：由于在整体的链路上多了一层，自然会引入一些额外的延迟。 
Kong AI 网关主要做了两方面的优化：一个是 Kong 本身就在持续的优化自身的性
能，致力于打造高性能的网关；另一方面是 Kong AI 网关引入了语义化缓存的能力，通
过 cache 来提升整体的性能，并降低 token 的消耗。 
InfoQ: 举例来说，如果一个企业使用了 M 个大模型，以及有 N 个用户，同时会
 
367 
InfoQ 架构师2025年第一季 
不同的任务指向不同的 LLM，那么这种情况下，其复杂性在哪里，Kong 如何简化其
开发和管理工作？ 
戴冠兰：正如前面所说，这里的复杂性主要就是 LLM 治理和策略如何实施了。 
Kong AI 网关提供了多种不同的配置策略，同时 Kong AI 网关也支持多样的 plugin 进行
扩展，这样就可以灵活的满足企业多样化的需求。 
InfoQ: Kong 提供了哪些机制来确保与外部 AI 服务交互的安全性和效率？ 
戴冠兰：提到安全性，首先就是密钥管理的安全性，Kong AI 网关允许进行密钥的
中心化控制，这样可以使用不同的虚拟密钥分发给不同的用户/client，一次来确保其密
钥的安全性。当然 Kong AI 网关也支持多种不同的认证策略，可以使用不同的认证策略
来完成认证。 
此外就是 Kong AI 网关提供的限流策略，无论是基于请求，还是基于 token 的，都
可以很好的包括后端实际的 LLM provider，避免因为过量的请求导致 LLM provider 的性
能或者安全问题。 
InfoQ: 如今大模型爬虫给各网站带来了很大的负荷，防范机制经常失效，那么网
关层面可以如何解决这个问题？实现原理是什么？ 
戴冠兰：对于 Kong AI 网关而言，由于我们已经在网关领域积累了很多经验，更多
的还是基于原有的经验提供多种校验和限制策略。无论是基于用户身份，请求来源，或
者是基于请求等要素，Kong AI 网关都有相关的 plugin 可以满足对应的需求。 
同时 Kong AI 网关也有基于 token 的限流限速插件，当然 Kong 也有类似 Bot De-
tection 之类的 Plugin，我们维护了一套规则集合，同时也允许用户对该规则进行扩展，
以此来防范一些爬虫的大量抓取请求。 
当然，网络的攻防本就是此消彼长的一个过程，有时候我们也需要借助一些其他的
手段才能更好的进行拦截，所以 Kong 也有很多合作方，同时 Kong 也有很好的扩展能
力，这样就可以更加灵活的来满足客户的具体需求了。 
InfoQ: AI 工程通常涉及大量 API 调用，Kong 是否支持动态限流和访问控制？
  
368 
推荐文章 | Article 
这些功能如何帮助企业优化 API 使用？ 
戴冠兰：是的，Kong AI 网关支持动态的限流限速和访问控制。比如通过使用 Kong 
AI 网关的访问控制能力，用户可以为不同的应用分配各自专有的密钥，并将限流能力附
加到这些专有密钥上，通过 Kong AI 网关提供的可观测能力，企业就可以更好的了解到
不同的应用对于 LLM token 的消耗情况，当然可观测性也不仅仅是针对 LLM 而言的，
对于一般的 API 请求， Kong 同样也提供了完备的可观测能力，让企业可以对这些 API 
的调用情况有非常直观的了解，进而进行相对应的控制和处理。 
InfoQ: Kong 如何帮助企业优化 LLM API 的调用成本和提升性能？你认为通过网
关来管理 API 调用能够为企业节省多少成本？ 
戴冠兰：Kong AI 网关主要通过保护企业免受密钥泄露，或者超量调用等方面来尽
可能的避免企业遭受损失。并且通过提供语义化缓存和可观测性来优化成本提升性能。 
此外，由于通过 Kong AI 网关屏蔽了不同 LLM provider 的差异性，使得企业在开发 
LLM 应用的过程中可以节约时间提升效率，以此来帮助企业节省成本。 
InfoQ: Kong 产品在支持 LLM/AI 应用方面还有哪些特别的设计或新能力？这些
功能的演进逻辑是什么？ 
戴冠兰：我来介绍一个我们的最新功能， AutoRAG 这个功能可以减少 LLM 幻觉，
并且可以提升开发者的体验。通过 Kong AI 网关的 AutoRAG ，企业客户可以直接将自
己的内容信息交给 Kong AI 网关处理，以此来自动的构建一个 RAG 应用，整体流程相
比于使用 Kong AI 网关之前，会简化很多，能大大提升效率。 
Kong AI Gateay 功能的整体演进逻辑，一方面来自于我们的全球客户，不同行业的
客户会有不同的需求，我们需要持续满足用户的需求。另一方面来自于我们强力的产品
和工程团队，我们可以积极的发现和了解技术趋势，并且将其转换为真正的产品功能。 
Kong 的发展和未来展望 
InfoQ：Kong 从最初专注 API 网关起步，如今在 AI 工程领域有哪些新的布局
和投入？ 
 
369 
InfoQ 架构师2025年第一季 
戴冠兰：Kong 自成立以来一直专注于 API 管理领域，AI 时代下，尽管和之前存在
一些差异，但终究还是 以 API 作为核心。随着 AI 技术的迅猛发展，我们顺势推出了 
Kong AI 网关，旨在为 AI 应用提供强大的基础设施支持。 
Kong AI 网关的核心优势： 
1. 多模型支持与无代码集成：Kong AI 网关支持多种大型语言模型（LLM），包括 
OpenAI、Anthropic、Cohere、Mistral 和 LLaMA 等。通过无代码插件，开发者可
以轻松集成这些模型，加速 AI 功能的部署。 
2. 高级提示工程与安全控制：我们提供了丰富的插件，如 AI 提示模板、提示装饰
器和提示词防火墙，帮助开发者构建更安全、可控的 AI 应用，确保提示的合规
性和一致性。 
3. 精细化资源管理：引入基于 token 的限流机制，使企业能够更精确地管理和控
制 AI 请求的资源消耗，优化成本结构。 
4. 流式响应与动态路由：支持流式数据传输，提升用户交互体验；同时，基于 
URL 的动态模型选择功能，使得在不同场景下灵活调用合适的模型成为可能。 
5. 与现有工具的无缝集成：Kong AI 网关与 OpenAI SDK 完全兼容，开发者可以无
需修改现有代码，直接接入多种 LLM，简化了迁移和集成过程。 
通过 Kong AI 网关，我们致力于为企业提供一个稳定、高效、安全的 AI 应用基础
设施，助力客户在 AI 时代实现更快的创新和增长。 
InfoQ：在你看来，Kong 在未来 AI 基础设施栈中最具战略意义的产品或能力是
什么？为什么？ 
戴冠兰：我们将与 AI 相关的核心能力高度集成在 Kong AI 网关中。无论是当前热
门的 MCP 协议，还是 Google 推出的 A2A 标准，都将在这一平台上得到全面支持和
体现。 
很难定义哪一项能力最具战略意义，一方面是因为 AI 技术迭代极快，新标准和新
需求层出不穷；另一方面，不同行业和企业的 AI 应用场景高度多样化，对能力的侧重
也有所不同。 
  
370 
推荐文章 | Article 
因此，与其强调某一个具体功能，我们更看重的是 Kong AI 网关提供的可扩展性、
开放性与演进能力 —— 这是在一个高速演变、标准未定的领域中，真正具备长期战略
价值的能力。 
InfoQ：Kong 网关在中国市场的应用情况如何？有没有一些本地化调整？ 
戴冠兰：Kong 在中国市场的应用呈现出强劲的增长势头，尤其是在开源社区的推
动下，已成为众多企业的技术基石。根据 Kong 官方数据，其全球开源社区用户超过 
16 万人，覆盖 46 个国家的 80 个用户组。在中国，Kong 社区的活跃度位居全球第二，
显示出其在本地开发者中的广泛认可。 
Kong 与本地云服务提供商和技术合作伙伴建立了紧密的合作关系，共同推动产品
在中国市场的应用和发展。针对中国市场的特殊需求，在出海和国际公司在国内落地的
场景，Kong 持续优化产品功能，增强对这些场景的支持。 
InfoQ：如何看待 AI 网关的未来发展，有哪些计划来增强其在 AI 时代的竞争力？ 
戴冠兰：AI 网关正迅速成为大模型（LLM）和生成式 AI 系统中的关键基础设施。
它不仅统一了流量入口，还在安全、合规、性能和可观察性方面提供了核心能力。Kong 
正在积极推动这一演进，致力于打造一个高性能、可扩展且面向未来的 AI 网关平台。 
增强竞争力的计划： 
• 持续扩展模型支持：Kong 将根据用户需求，持续增加对更多 LLM 的支持，包
括最新的 Claude 3.7 等，确保平台的前瞻性和兼容性。 
• 更多语义智能驱动的功能：在 3.8 版本中，Kong 引入了语义缓存（Semantic 
Caching）和语义路由（Semantic Routing）等功能。这些功能通过理解用户请求
的语义含义，实现更高效的响应和更智能的模型选择，提升了 AI 应用的性能和
用户体验 
• 优化开发者体验：推出如 Insomnia AI Runner 等工具，简化 AI 应用的开发、测
试和部署流程，降低技术门槛，提升开发效率。 
• 强化生态系统建设：通过开源社区的推动和与本地合作伙伴的协作，Kong 致力
于构建一个开放、协同的 AI 网关生态系统，促进技术创新和应用落地。 
 
371 
InfoQ 架构师2025年第一季 
“AI 六小虎”两年混战，新的较量开始 
 
智谱被曝启动上市备案，但六小虎胜者未定。 
“AI 六小虎”是过去两年国内大模型时代的一个标志，指的是当年最早完成 10 
亿+美元融资，且均拥有自研千亿参数级大模型，在国际基准测试中与 GPT-4、Llama 等
对标的大模型创业公司。 
这是当时那个阶段中国大模型的代表，代表了一个时代的认知。但如今，随着六家
公司分化出各自不同的道路，这个符号背后代表的大模型发展也有了不同的含义。 
 
作者 褚杏娟 
  
372 
推荐文章 | Article 
两年时间基本完成第一轮较量 
2023 年上半年，百川智能、阶跃星辰、零一万物和月之暗面成立，DeepSeek 也是
这一年成立。智谱和 MiniMax 要更早些，分别是在 2019 年和 2021 年。 
在过去两年多时间里，大模型公司主要围绕着模型层、产品层和营销层三个方面展
开较量。 
总体看来，MiniMax、月之暗面前期在技术上的对外分享并不多，反而是在产品上
更有优势，比如月之暗面巨额投放的 kimi、MiniMax 主打出海的 AI 虚拟人物聊天软件 
Talkie 等，其应用的知名度高于大模型本身。而百川、阶跃星辰、零一万物和智谱入局
后都先将精力放在了模型研发上，大模型的知名度高于后推出的应用。 
过去两年间，对国内大模型公司基座模型研发影响最大的就是 OpenAI。从 1 亿多
参数的 GPT-1 到 1.8 万亿参数的 GPT-4，模型参数成为早期大模型创企的必争指标。
在去年上半年，AI 六小虎大都迈入了千亿参数模型行列，但之后基座模型的参数规模也
基本停留在了这个阶段。 
• 百川最早在 23 年 6 月发布了中英文语言模型 Baichuan-7B，24 年 1 月发布
了超千亿参数的大语言模型 Baichuan 3，四个月后发布 Baichuan4。 
• 阶跃星辰在成立一年后的 24 年 3 月，首发了千亿参数语言大模型 Step-1、
Step-1V 千亿参数多模态大模型和 Step-2 万亿参数 MoE 语言大模型预览版。
当年 7 月，又发布了 Step-2 正式版、Step-1.5V 多模态大模型和 Step-1X 图像
生成大模型。 
• 零一万物在当年 11 月开源发布首款预训练大模型 Yi-34B，24 年 5 月发布千
亿参数闭源大模型 Yi - Large。 
• MiniMax 的 ABAB 大模型在 2023 年 8 月通过备案，向公众开放。去年 4 月，
ABAB 6.5 万亿参数的 MoE 模型发布，支持 245k 上下文窗口。 
• 智谱 2021 年开源百亿大模型 GLM-10B，2022 年 8 月就发布了千亿参数大模
型 GLM-130B，2024 年 1 月迭代到最新的 GLM-4。 
• 这一时期，月之暗面并未公布基座模型的参数信息，技术上是靠长上下文出圈。 
第一轮关于参数的争夺基本落幕。但当前，基座模型的参数量还远远没有达到瓶颈
 
373 
InfoQ 架构师2025年第一季 
和人类顺利使用大模型的需求目标，不再卷参数规模反映出了大模型一直以来都面临的
困境。 
正如白鲸开源 CEO 郭炜所说，大模型公司竞争的关键要素其实一直没有变化，模
型参数规模还是重要的衡量指标，只不过中国原创大模型都遇到了“三不够”的挑战：
钱不够、卡不够、数据不够，这种情况下，大模型参数很难提上去。 
基座模型最核心的问题在于需要持续投入高额算力和密集的高端人才，处于追赶阶
段的大模型公司需要投入更多。另外，基座模型的盈利周期较长，短期内难以拥有自我
造血能力，这就要求企业要么持续大规模融资，要么自身具备足够雄厚的现金流来支撑
长期竞争。 
但对于初期还在快速向前奔跑的公司来说，降本不是一件重要的事情，占领市场更
重要。郭炜认为，等发现“三不够”的时候再做降本这件事也来得及，但就要看公司的
战略决心和战略眼光了。 
在“三不够”的情况下，AI 六小虎早已分化赛道，这并非完全因为 DeepSeek，更
多是在资源不足下，有的企业开始转而求其次，在新大模型下蒸馏和工程化创新、在细
分领域深耕领域大模型、在全球化领域想办法做应用挣钱，这些是无奈的选择。 
比如，百川现已转向垂直模型。早在去年 2 月，百川发布了医疗垂域通用大模型 
baichuan2-Turbo，同年 12 月推出全链路领域增强金融大模型 Baichuan4 - Finance。今
年 3 月消息称，百川再收缩和裁撤金融业务、all in 医疗，此外也暂停了预训练。 
零一万物在去年 5 月就放弃了原定的万亿参数 Yi-X-Large 模型训练计划，转而训
练更轻量化、更具商业落地前景的 MoE（混合专家）模型 Yi-Lightning。2025 年更是与
阿里云合作，将耗费成本和精力更大的超大模型交给阿里训练，并明确表示不会再做万
亿以上超大参数模型。 
张鹏则在近期表示智谱依然还在做预训练模型，并非只训小模型。去年底，阶跃星
辰称自己坚持预训练、继续冲击 AGI。MiniMax、月之暗面目前也未有停止预训练的消
息。可见，基础大模型的参数之争或许不再激烈，但整体竞争还远远没到得出结果的时
候。 
  
374 
推荐文章 | Article 
 
DeepSeek 催化各方迅速决策 
推理是大模型烧钱背景下做出的一个重要路径选择。OpenAI 在 2024 年 9 月发布
的推理模型 GPT-o1 成为大模型竞争的一个分界点，而后来者 DeepSeek 无疑成为这次
游戏的最大赢家之一。 
有投资人指出，DeepSeek 的出圈并不是靠模型能力碾压 OpenAI 或 Claude、达到
三五倍的用户体验优势，而是通过一种极具中国特色的“制造业式”成本控制——在各
个维度性能差距仅 5%-10% 的情况下，将推理成本压缩到了 1/30 至 1/50。 
这就好比在软件商店里用极致性价比实现了突围，这种出圈方式反过来也印证了当
前行业的竞争逻辑。DeepSeek 探索出的这条路径目前给其他几家的技术路线带来了挑
战。 
DeepSeek 的爆火和开源无疑让投资人和大模型企业纷纷紧张，但他们最终得出的
结论是：目前还远未到能对整个行业格局下定论的时候，整个行业仍处于你追我赶的发
展阶段，并非终局。 
而 DeepSeek 最重要的行业影响之一在于帮助整个生态的参与者快速找准了自己的
战略定位。如果说之前大家还在犹豫是做大模型、应用开发还是深耕垂直领域的话，
DeepSeek 则让各方迅速认清了适合自己的发展方向。 
• 百川智能战略收缩，聚焦在了 B 端医疗领域深耕，比如与北京儿童医院合作推
进医疗大模型，逐渐深化技术场景化能力。但目前基座模型迭代速度放缓，其 C 
端应用发展不及预期，近期人才流失也较为严重。 
• 阶跃星辰目前有万亿级模型和多模态技术储备，长期以来较为低调，市场声量、
 
375 
InfoQ 架构师2025年第一季 
用户认知度可能不及其他五家。 
• 零一万物刻意控制模型研发成本，聚焦在轻量级模型上。商业化路径确定 To B 
市场，通过性价比和本地化服务与大厂竞争，此外海外用户付费意愿强，单款产
品年收入过亿元。但其灵活调整的战略也带来了人才流动和业务重心频繁转变的
挑战。 
• Minimax 作为国内首家多模态大模型创业公司，有一定多模态技术积累，而且国
内外产品双线布局，取得了可观商业化收入，2024 年收入或达 7000 万美元，
其中多数来自 Talkie。但多模态领域竞争激烈，海外产品也会面临合规等风险。 
• 智谱还在持续迭代基础模型和多模态模型，并技术开源。商业化路径也较为清晰，
其在 B 端和 G 端市场表现突出。但 B 端服务上有被质疑“只会做定制化项
目”，可能限制规模化发展，C 端应用开发和流量不够突出。此外，智谱目前已
在北京证监局办理辅导备案，由中国国际金融股份有限公司担任辅导机构，为其
上市进程做准备。 
• 月之暗面有很突出的 C 端产品 Kimi，但前期过于依赖大厂流量投放，用户黏性
和可持续性面临挑战。此外，模型信息透明度较低，最初建立的长文本优势被快
速打破，需建立新的技术突破，C 端也面临大厂挤压风险。 
此外，AI 六小虎的分化，也让大厂迎来了赶超的最佳时机，如阿里的 Qwen、字节
的豆包等大模型，腾讯混元也在后续发力，吸纳了大量 DeepSeek 红利和流量。 
郭炜分析称，AI 六小虎的“三不够”，正是大厂的“三够”，与 C 端 App 短期
烧钱就能烧出结果不同，大模型是一个长期的“全面战争”，大厂“三够”情况下更能
坚持下去。 
“大模型没有技术壁垒” 
对于是否还要投入基础模型的问题，答案其实无外乎坚持和转向。 
现在，大模型公司面临的选择基本就是：要么转向投入较低的方向、以维持更长时
间实现盈利，要么继续争取更多资金去摘取“皇冠上的明珠”，但这要看市场是否还愿
意支持这个还要持续高投入三、五年之久的梦想。至于最终登顶的是谁，取决于其选择
的发展路径和关键环节的把握，而答案可能需要等两到三年的时间才能分晓。 
  
376 
推荐文章 | Article 
有一部分人并不太看好继续坚持基座模型。某大厂高管认为，如果做不到 
DeepSeek 的水平，可能就没必要投入基座大模型的研发了。现在这个赛道的门槛已经
高到离谱——光是训练集群就得从 1 万张显卡起步，绝大多数公司根本承担不起这种
成本。DeepSeek 团队也纯粹是因为老板资金雄厚。其认为，现阶段最现实的路径还是
等他们即将开源的新版本。只要完整的技术方案公开，行业跟进的成本会大幅降低。 
在该高管看来，除非出现革命性的技术路线突破，比如多模态领域找到新方向才值
得重点关注，因为多模态技术能整合图像、语音等多媒体数据，理论上具备近乎无限的
数据扩展空间。 
但是，基座模型的战略价值依然显著，它不仅为上层应用提供技术底座，还对整个 
AI 生态发展有不可替代的推动作用。 
如果要想在大模型上继续突破，有业内人士认为必须在两个维度有所突破：第一是
模型能力必须足够惊艳，但这一点很难：第一波从 60% 到 80% 准确率容易，但现在
从 95% 到 97% 会异常艰难；第二是成本控制，能否用更创新的架构实现比 DeepSeek 
更低的推理成本，然后出圈，这也是很好的一条路径。但如果这两点都做不到，那就需
要认真考虑战略转型：是拥抱开源生态做 ToB 服务，还是彻底转向产品化做 ToC 应用。 
值得注意的是，郭炜指出，大模型没有技术壁垒，或者说，在商业世界里技术本身
就不是壁垒。“虽然有些绝对，但是创业之后我深刻理解到：超强的技术算法优势背后
是人才的竞争，是战略的先知先决，是困难时刻战略方向的坚持，是大量资本的投入，
这都与技术无关。所以，模型技术能力在短期竞争内有决定性作用，但是在中长线竞争
当中绝对不是最重要的壁垒。” 
但目前大模型公司基本已经形成了开源的共识。“拥抱开源是 AI 六小虎的唯一出
路。”郭炜说道。 
“大模型周期太长、投入太大了，一家公司很难融到那么多钱、买到那么多卡、找
到那么多数据，只有充分利用开源的方式建立起市场的认知标准，全民一起共建才有胜
利的希望。否则，DeepSeek 及其开源大模型相关生态会碾压过去 AI 六小虎的所有成
就。那么，AI 六小虎也只能是“AI 六小猫”，去做细分领域模型和应用了。”郭炜分
析称。 
 
377 
InfoQ 架构师2025年第一季 
一直以来，国内外的大模型公司都面临着闭源与开源路线的选择。经过两年以来的
开闭源之争，天平已经明显倾向了开源，之前闭源的 OpenAI、百度等也开始拥抱开源。
而在被曝出开启上市辅导的同一天，智谱又一口气上线并开源了三大类最新的 GLM 模
型。 
商业化，重要吗？ 
另外，最近可以看到大模型企业开始纷纷“秀客户”，来证明自己的商业落地能力。 
郭炜表示，商业化对 AI 六小虎不是那么重要，反而收缩战线，形成单点突破（比
如 DeepSeek）比全面出击商业化重要得多。 
关于这一点，王小川在百川两周年的全员信中也提到了过早商业化的问题，基础模
型、垂直模型、C 端应用到过早商业化，全面布局的结果就是极大增加了百川智能组织
的复杂度。 
而有的投资人则认为，大模型企业必须紧跟场景，发挥数据优势、完善商业闭环。
“无论世界怎么变化，商业的本质始终存在。”核心打法是要做出差异化。 
实际上，对于一直在寻求融资的 AI 六小虎来说，不商业化似乎也是不可能的。与 
DeepSeek 不同，AI 六小虎融到的钱总有一天会花完，他们必须要自我造血、创造现金
流。 
普遍来看，业内人士更看好 To B 领域，因为这项技术能够大幅提高效率、降低成
本，并减少人力投入，因此开始产生一些收入，尤其 DeepSeek 极大减轻了市场教育压
力，很多企业是主动部署，而非被动推销。但在 To C 领域，目前还没有出现爆款应用，
另外紧贴大模型能力开发的应用，一旦基础模型升级，很多应用可能就要彻底改变。 
关键在于什么样的应用场景能让用户愿意支付如此高昂的成本使用 AI 来解决问题。
研究领域相对特定且拥有高质量数据集，这个场景下，用户可能愿意为了发表论文每个
月支付 200 美金。但在更发散、更泛化的环境里，情况就完全不同了。 
 
 
  
378 
推荐文章 | Article 
结束语 
在最终没有跑出结果之前很难谈优劣，创业者永远都是和时间赛跑，太早就是“先
烈”，太晚汤都喝不到。合适的时机、合适的场景做出合适的产品，是创业者最难的决
策，也是最有意思的挑战。 
在郭炜看来，AI 六小虎是大模型领域的先驱，也是这个市场最好的“教育者”，如
果没有他们，大多数技术开发者都无法接触到廉价的国产大模型，也就不会有 
DeepSeek 的爆点。然而，最终先驱是不是先烈，还是看“三不够”场景下，如何在大
模型领域形成自己的生态和闭环。 
“不要妄谈任何一个这个时代的大模型创业者，所有人都是勇士。”郭炜说道。 
 

