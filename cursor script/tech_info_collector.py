#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ€æœ¯ä¿¡æ¯æ™ºèƒ½æ”¶é›†å™¨
æ”¯æŒGitHubè¶‹åŠ¿ã€æŠ€æœ¯æ–°é—»ã€ç¤¾åŒºè®¨è®ºç­‰å¤šæºä¿¡æ¯æ”¶é›†
"""

import requests
import json
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import yaml
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TechInfo:
    """æŠ€æœ¯ä¿¡æ¯æ•°æ®ç»“æ„"""
    title: str
    url: str
    source: str
    category: str
    timestamp: datetime
    description: str = ""
    tags: List[str] = None
    metrics: Dict = None

class TechInfoCollector:
    """æŠ€æœ¯ä¿¡æ¯æ”¶é›†å™¨"""
    
    def __init__(self, config_path: str = "config/workflow_config.yaml"):
        self.config = self._load_config(config_path)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'TechInfoCollector/1.0 (https://github.com/your-repo)'
        })
    
    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            # é»˜è®¤é…ç½®
            return {
                'information_sources': {
                    'primary': ['github_trending', 'hackernews'],
                    'secondary': ['infoq_cn'],
                    'custom': []
                },
                'analysis_focus': ['architecture_design', 'performance_optimization'],
                'output_preferences': {
                    'language': 'zh-CN',
                    'style': 'deep_analysis'
                }
            }
    
    def collect_github_trending(self, timeframe: str = "weekly", language: str = None) -> List[TechInfo]:
        """æ”¶é›†GitHubè¶‹åŠ¿é¡¹ç›®"""
        logger.info(f"æ”¶é›†GitHubè¶‹åŠ¿é¡¹ç›® - æ—¶é—´èŒƒå›´: {timeframe}")
        
        # GitHub Trending API (éå®˜æ–¹)
        url = "https://api.github.com/search/repositories"
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        if timeframe == "daily":
            since = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        elif timeframe == "weekly":
            since = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        else:
            since = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        
        params = {
            'q': f'created:>{since}',
            'sort': 'stars',
            'order': 'desc',
            'per_page': 10
        }
        
        if language:
            params['q'] += f' language:{language}'
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            trending_repos = []
            for item in data.get('items', []):
                tech_info = TechInfo(
                    title=item['full_name'],
                    url=item['html_url'],
                    source='github_trending',
                    category='open_source_project',
                    timestamp=datetime.now(),
                    description=item.get('description', ''),
                    tags=item.get('topics', []),
                    metrics={
                        'stars': item['stargazers_count'],
                        'forks': item['forks_count'],
                        'language': item.get('language', 'Unknown')
                    }
                )
                trending_repos.append(tech_info)
            
            logger.info(f"æˆåŠŸæ”¶é›†åˆ° {len(trending_repos)} ä¸ªè¶‹åŠ¿é¡¹ç›®")
            return trending_repos
            
        except Exception as e:
            logger.error(f"æ”¶é›†GitHubè¶‹åŠ¿æ—¶å‡ºé”™: {e}")
            return []
    
    def collect_hackernews_posts(self, max_posts: int = 20) -> List[TechInfo]:
        """æ”¶é›†Hacker NewsæŠ€æœ¯ç›¸å…³å¸–å­"""
        logger.info("æ”¶é›†Hacker NewsæŠ€æœ¯å¸–å­")
        
        try:
            # è·å–çƒ­é—¨æ•…äº‹ID
            top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = self.session.get(top_stories_url)
            response.raise_for_status()
            story_ids = response.json()[:max_posts]
            
            tech_posts = []
            for story_id in story_ids:
                # è·å–å…·ä½“æ•…äº‹ä¿¡æ¯
                story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                story_response = self.session.get(story_url)
                if story_response.status_code == 200:
                    story_data = story_response.json()
                    
                    # è¿‡æ»¤æŠ€æœ¯ç›¸å…³å†…å®¹
                    if self._is_tech_related(story_data.get('title', '')):
                        tech_info = TechInfo(
                            title=story_data.get('title', ''),
                            url=story_data.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            source='hackernews',
                            category='tech_discussion',
                            timestamp=datetime.fromtimestamp(story_data.get('time', 0)),
                            description=story_data.get('text', ''),
                            metrics={
                                'score': story_data.get('score', 0),
                                'comments': story_data.get('descendants', 0)
                            }
                        )
                        tech_posts.append(tech_info)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(0.1)
            
            logger.info(f"æˆåŠŸæ”¶é›†åˆ° {len(tech_posts)} ä¸ªHNæŠ€æœ¯å¸–å­")
            return tech_posts
            
        except Exception as e:
            logger.error(f"æ”¶é›†Hacker Newså¸–å­æ—¶å‡ºé”™: {e}")
            return []
    
    def _is_tech_related(self, title: str) -> bool:
        """åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä¸æŠ€æœ¯ç›¸å…³"""
        tech_keywords = [
            'python', 'javascript', 'rust', 'go', 'java', 'typescript',
            'docker', 'kubernetes', 'redis', 'postgresql', 'mongodb',
            'react', 'vue', 'angular', 'nodejs', 'api', 'database',
            'ai', 'ml', 'machine learning', 'deep learning', 'llm',
            'cloud', 'aws', 'azure', 'gcp', 'microservices',
            'open source', 'github', 'framework', 'library',
            'performance', 'optimization', 'security', 'devops'
        ]
        
        title_lower = title.lower()
        return any(keyword in title_lower for keyword in tech_keywords)
    
    def collect_all_sources(self) -> Dict[str, List[TechInfo]]:
        """æ”¶é›†æ‰€æœ‰é…ç½®çš„ä¿¡æ¯æº"""
        all_info = {}
        
        # æ”¶é›†ä¸€çº§ä¿¡æ¯æº
        for source in self.config['information_sources']['primary']:
            if source == 'github_trending':
                all_info['github_trending'] = self.collect_github_trending()
            elif source == 'hackernews':
                all_info['hackernews'] = self.collect_hackernews_posts()
        
        return all_info
    
    def save_collected_data(self, data: Dict[str, List[TechInfo]], output_dir: str = "data"):
        """ä¿å­˜æ”¶é›†çš„æ•°æ®"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_data = {}
        for source, info_list in data.items():
            serializable_data[source] = []
            for info in info_list:
                info_dict = {
                    'title': info.title,
                    'url': info.url,
                    'source': info.source,
                    'category': info.category,
                    'timestamp': info.timestamp.isoformat(),
                    'description': info.description,
                    'tags': info.tags or [],
                    'metrics': info.metrics or {}
                }
                serializable_data[source].append(info_dict)
        
        output_file = os.path.join(output_dir, f"tech_info_{timestamp}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        return output_file

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    collector = TechInfoCollector()
    
    print("ğŸš€ å¼€å§‹æ”¶é›†æŠ€æœ¯ä¿¡æ¯...")
    collected_data = collector.collect_all_sources()
    
    print("\nğŸ“Š æ”¶é›†ç»“æœç»Ÿè®¡:")
    for source, info_list in collected_data.items():
        print(f"  {source}: {len(info_list)} æ¡ä¿¡æ¯")
    
    # ä¿å­˜æ•°æ®
    output_file = collector.save_collected_data(collected_data)
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_file}")