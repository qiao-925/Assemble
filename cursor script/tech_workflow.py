#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ€æœ¯ä¿¡æ¯æ™ºèƒ½æ„å»ºå·¥ä½œæµä¸»ç¨‹åº
æ•´åˆä¿¡æ¯æ”¶é›†ã€æ™ºèƒ½åˆ†æå’Œå†…å®¹ç”ŸæˆåŠŸèƒ½
"""

import argparse
import sys
import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from tech_info_collector import TechInfoCollector, TechInfo
from tech_analyzer import TechAnalyzer, TechAnalysis

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TechWorkflow:
    """æŠ€æœ¯ä¿¡æ¯å·¥ä½œæµä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config_path: str = "config/workflow_config.yaml"):
        self.collector = TechInfoCollector(config_path)
        self.analyzer = TechAnalyzer()
        self.config_path = config_path
        
        # ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = ["data", "output", "cache", "config"]
        for dir_name in directories:
            Path(dir_name).mkdir(exist_ok=True)
    
    def run_daily_digest(self) -> str:
        """æ‰§è¡Œæ¯æ—¥æŠ€æœ¯ä¿¡æ¯æ‘˜è¦"""
        logger.info("ğŸŒ… å¼€å§‹ç”Ÿæˆæ¯æ—¥æŠ€æœ¯æ‘˜è¦")
        
        # æ”¶é›†ä¿¡æ¯
        collected_data = self.collector.collect_all_sources()
        
        # ä¿å­˜åŸå§‹æ•°æ®
        data_file = self.collector.save_collected_data(collected_data)
        
        # ç”Ÿæˆè¶‹åŠ¿åˆ†æ
        trend_analysis = self.analyzer.generate_trend_report(collected_data)
        
        # ä¿å­˜åˆ†æç»“æœ
        output_file = self.analyzer.save_analysis(trend_analysis)
        
        logger.info(f"âœ… æ¯æ—¥æ‘˜è¦å·²å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_file}")
        return output_file
    
    def analyze_project(self, repo_url: str) -> str:
        """åˆ†ææŒ‡å®šçš„GitHubé¡¹ç›®"""
        logger.info(f"ğŸ” å¼€å§‹åˆ†æé¡¹ç›®: {repo_url}")
        
        # è¿›è¡Œé¡¹ç›®åˆ†æ
        analysis = self.analyzer.analyze_github_project(repo_url)
        
        if analysis:
            # ä¿å­˜åˆ†æç»“æœ
            output_file = self.analyzer.save_analysis(analysis)
            logger.info(f"âœ… é¡¹ç›®åˆ†æå·²å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_file}")
            return output_file
        else:
            logger.error("âŒ é¡¹ç›®åˆ†æå¤±è´¥")
            return None
    
    def compare_projects(self, project_urls: list) -> str:
        """å¯¹æ¯”å¤šä¸ªé¡¹ç›®"""
        logger.info(f"âš–ï¸ å¼€å§‹å¯¹æ¯” {len(project_urls)} ä¸ªé¡¹ç›®")
        
        # åˆ†ææ¯ä¸ªé¡¹ç›®
        analyses = []
        for url in project_urls:
            analysis = self.analyzer.analyze_github_project(url)
            if analysis:
                analyses.append(analysis)
        
        if len(analyses) < 2:
            logger.error("âŒ è‡³å°‘éœ€è¦2ä¸ªæœ‰æ•ˆé¡¹ç›®è¿›è¡Œå¯¹æ¯”")
            return None
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison_content = self._generate_comparison_report(analyses)
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"output/competitor_comparison_{timestamp}.md"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(comparison_content)
        
        logger.info(f"âœ… é¡¹ç›®å¯¹æ¯”å·²å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_file}")
        return output_file
    
    def _generate_comparison_report(self, analyses: List[TechAnalysis]) -> str:
        """ç”Ÿæˆé¡¹ç›®å¯¹æ¯”æŠ¥å‘Š"""
        content_parts = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        project_names = [analysis.title.replace(' æŠ€æœ¯æ·±åº¦è§£æ', '') for analysis in analyses]
        title = f"# âš–ï¸ {' vs '.join(project_names)} ç«å“æŠ€æœ¯å¯¹æ¯”åˆ†æ"
        content_parts.append(title)
        
        # å¯¹æ¯”æ¦‚è§ˆ
        overview = f"""
## ğŸ¯ å¯¹æ¯”æ¦‚è§ˆ

æœ¬æŠ¥å‘Šå¯¹æ¯”åˆ†æäº† {len(analyses)} ä¸ªç›¸å…³æŠ€æœ¯é¡¹ç›®ï¼Œä»æŠ€æœ¯æ¶æ„ã€æ€§èƒ½è¡¨ç°ã€ç”Ÿæ€ç³»ç»Ÿç­‰å¤šä¸ªç»´åº¦è¿›è¡Œæ·±åº¦å¯¹æ¯”ã€‚

### ğŸ“Š åŸºæœ¬ä¿¡æ¯å¯¹æ¯”

| é¡¹ç›® | Stars | Forks | ä¸»è¦è¯­è¨€ | å¼€æºåè®® |
|------|-------|-------|----------|----------|
"""
        
        # æ·»åŠ è¡¨æ ¼æ•°æ®ï¼ˆè¿™é‡Œéœ€è¦ä»åˆ†æä¸­æå–ï¼‰
        for analysis in analyses:
            # æå–åŸºæœ¬ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦ä»åˆ†æå†…å®¹ä¸­è§£æï¼‰
            overview += f"| {analysis.title.replace(' æŠ€æœ¯æ·±åº¦è§£æ', '')} | - | - | - | - |\n"
        
        content_parts.append(overview)
        
        # è¯¦ç»†å¯¹æ¯”
        details = """
## ğŸ”§ æŠ€æœ¯æ¶æ„å¯¹æ¯”

### è®¾è®¡å“²å­¦å·®å¼‚
"""
        
        for i, analysis in enumerate(analyses, 1):
            details += f"\n#### {i}. {analysis.title.replace(' æŠ€æœ¯æ·±åº¦è§£æ', '')}\n"
            details += f"ä¸»è¦æ´å¯Ÿï¼š{analysis.insights[0] if analysis.insights else 'å¾…æ·±å…¥åˆ†æ'}\n"
        
        content_parts.append(details)
        
        # ç»¼åˆè¯„ä»·
        conclusion = f"""
## ğŸ† ç»¼åˆè¯„ä»·

åŸºäºæŠ€æœ¯æ¶æ„ã€ç”Ÿæ€å¥åº·åº¦ã€ç¤¾åŒºæ´»è·ƒåº¦ç­‰ç»´åº¦çš„ç»¼åˆåˆ†æï¼š

*ï¼ˆå…·ä½“è¯„ä»·éœ€è¦åŸºäºè¯¦ç»†çš„æŠ€æœ¯å¯¹æ¯”æ•°æ®è¿›è¡Œç”Ÿæˆï¼‰*

---
*ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        content_parts.append(conclusion)
        
        return '\n'.join(content_parts)
    
    def run_trending_analysis(self, topic: str = None) -> str:
        """è¿è¡Œè¶‹åŠ¿åˆ†æ"""
        logger.info(f"ğŸ“ˆ å¼€å§‹è¶‹åŠ¿åˆ†æ - ä¸»é¢˜: {topic or 'å…¨é¢†åŸŸ'}")
        
        # æ”¶é›†æ•°æ®
        collected_data = self.collector.collect_all_sources()
        
        # å¦‚æœæŒ‡å®šäº†ä¸»é¢˜ï¼Œè¿‡æ»¤ç›¸å…³å†…å®¹
        if topic:
            collected_data = self._filter_by_topic(collected_data, topic)
        
        # ç”Ÿæˆè¶‹åŠ¿æŠ¥å‘Š
        trend_analysis = self.analyzer.generate_trend_report(collected_data)
        
        # ä¿å­˜ç»“æœ
        output_file = self.analyzer.save_analysis(trend_analysis)
        
        logger.info(f"âœ… è¶‹åŠ¿åˆ†æå·²å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_file}")
        return output_file
    
    def _filter_by_topic(self, data: Dict, topic: str) -> Dict:
        """æ ¹æ®ä¸»é¢˜è¿‡æ»¤æ•°æ®"""
        filtered_data = {}
        topic_lower = topic.lower()
        
        for source, info_list in data.items():
            filtered_list = []
            for info in info_list:
                # æ£€æŸ¥æ ‡é¢˜ã€æè¿°å’Œæ ‡ç­¾æ˜¯å¦åŒ…å«ä¸»é¢˜
                if (topic_lower in info['title'].lower() or 
                    topic_lower in info.get('description', '').lower() or
                    any(topic_lower in tag.lower() for tag in info.get('tags', []))):
                    filtered_list.append(info)
            
            if filtered_list:
                filtered_data[source] = filtered_list
        
        return filtered_data

def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æŠ€æœ¯ä¿¡æ¯æ™ºèƒ½æ„å»ºå·¥ä½œæµ')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ¯æ—¥æ‘˜è¦å‘½ä»¤
    daily_parser = subparsers.add_parser('daily', help='ç”Ÿæˆæ¯æ—¥æŠ€æœ¯æ‘˜è¦')
    
    # é¡¹ç›®åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze-project', help='åˆ†æGitHubé¡¹ç›®')
    analyze_parser.add_argument('--repo', required=True, help='GitHubä»“åº“URLæˆ–owner/repoæ ¼å¼')
    
    # è¶‹åŠ¿åˆ†æå‘½ä»¤
    trend_parser = subparsers.add_parser('trend-report', help='ç”ŸæˆæŠ€æœ¯è¶‹åŠ¿æŠ¥å‘Š')
    trend_parser.add_argument('--topic', help='æŒ‡å®šåˆ†æä¸»é¢˜')
    
    # é¡¹ç›®å¯¹æ¯”å‘½ä»¤
    compare_parser = subparsers.add_parser('compare', help='å¯¹æ¯”å¤šä¸ªé¡¹ç›®')
    compare_parser.add_argument('--projects', required=True, help='é¡¹ç›®åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”')
    
    # é…ç½®å‘½ä»¤
    config_parser = subparsers.add_parser('config', help='æ˜¾ç¤ºæˆ–ä¿®æ”¹é…ç½®')
    config_parser.add_argument('--show', action='store_true', help='æ˜¾ç¤ºå½“å‰é…ç½®')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # åˆ›å»ºå·¥ä½œæµå®ä¾‹
    workflow = TechWorkflow()
    
    try:
        if args.command == 'daily':
            output_file = workflow.run_daily_digest()
            print(f"ğŸ‰ æ¯æ—¥æŠ€æœ¯æ‘˜è¦å·²ç”Ÿæˆ: {output_file}")
            
        elif args.command == 'analyze-project':
            repo_url = args.repo
            # å¦‚æœä¸æ˜¯å®Œæ•´URLï¼Œæ„é€ GitHub URL
            if not repo_url.startswith('http'):
                repo_url = f"https://github.com/{repo_url}"
            
            output_file = workflow.analyze_project(repo_url)
            if output_file:
                print(f"ğŸ‰ é¡¹ç›®åˆ†æå·²å®Œæˆ: {output_file}")
            else:
                print("âŒ é¡¹ç›®åˆ†æå¤±è´¥")
                
        elif args.command == 'trend-report':
            output_file = workflow.run_trending_analysis(args.topic)
            print(f"ğŸ‰ è¶‹åŠ¿æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            
        elif args.command == 'compare':
            project_list = [p.strip() for p in args.projects.split(',')]
            # æ„é€ å®Œæ•´URL
            project_urls = []
            for project in project_list:
                if not project.startswith('http'):
                    project_urls.append(f"https://github.com/{project}")
                else:
                    project_urls.append(project)
            
            output_file = workflow.compare_projects(project_urls)
            if output_file:
                print(f"ğŸ‰ é¡¹ç›®å¯¹æ¯”å·²å®Œæˆ: {output_file}")
            else:
                print("âŒ é¡¹ç›®å¯¹æ¯”å¤±è´¥")
                
        elif args.command == 'config':
            if args.show:
                with open('config/workflow_config.yaml', 'r', encoding='utf-8') as f:
                    print(f.read())
                    
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‘½ä»¤æ—¶å‡ºé”™: {e}")
        print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()